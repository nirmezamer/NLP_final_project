{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL3sH1xzTDzL",
        "outputId": "da99e59d-5dcc-4c0a-ecb3-9c08e608d89b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "\n",
        "data_jasons = {\"reddit\": \"reddit_data_set.json\", \"wikipedia\": \"wikipedia_summaries.json\", \"news\": \"newspapers_data.json\"}\n",
        "\n",
        "is_train = True\n",
        "load_model = False\n",
        "load_optimizer = False\n",
        "model_path = \"/content/bert_classifier.pth\"\n",
        "optimizer_path = \"/content/optimizer.pth\"\n",
        "epoch_path = \"/content/epoch.pth\"\n",
        "if load_model:\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "model.to(device)\n",
        "def tokenize_data(texts, labels, max_length=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "def trainer(epochs_num, model, criterion, device, optimizer, train_dataloader,val_dataloader ):\n",
        "    print(epochs_num)\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    index = 0\n",
        "    for epoch in tqdm(range(epochs_num)):\n",
        "        batch_number = 0\n",
        "        model.train()\n",
        "        train_accuracy = 0\n",
        "        for batch in train_dataloader:\n",
        "            batch_number += 1\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                    'attention_mask': batch[1],\n",
        "                    'labels': batch[2]}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            predictions = [1 if i > 0.5 else 0 for i in logits]\n",
        "            N = len(predictions)\n",
        "            for i in range(N):\n",
        "                if predictions[i] == inputs['labels'][i]:\n",
        "                    train_accuracy += 1\n",
        "            loss = criterion(logits.view(-1), inputs['labels'].view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        train_accuracies.append(train_accuracy / len(train_dataloader.dataset))\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_accuracy = 0\n",
        "        for i, batch in enumerate(val_dataloader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                    'attention_mask': batch[1],\n",
        "                    'labels': batch[2]}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            logits = outputs.logits\n",
        "            predictions = [1 if i > 0.5 else 0 for i in logits]\n",
        "            N = len(predictions)\n",
        "            for i in range(N):\n",
        "                if predictions[i] == inputs['labels'][i]:\n",
        "                    val_accuracy += 1\n",
        "\n",
        "        val_accuracies.append(val_accuracy / len(val_dataloader.dataset))\n",
        "        print(f\"Epoch {epoch+1}/{epochs_num} - Validation Accuracy: { val_accuracies[index]:.4f}, Train Accuracy: {train_accuracies[index]:.4f}\")\n",
        "        index+=1\n",
        "        torch.save(model.state_dict(), 'bert_classifier.pth')\n",
        "        torch.save(optimizer.state_dict(), 'optimizer.pth')\n",
        "        torch.save({ epoch: i }, 'epoch.pth')\n",
        "    return train_accuracies, val_accuracies\n",
        "\n",
        "def evaluate(model, criterion, device, dataloader):\n",
        "    model.eval()\n",
        "    accuracy = 0\n",
        "    predictions = []\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs = {'input_ids': batch[0],\n",
        "                'attention_mask': batch[1],\n",
        "                'labels': batch[2]}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        batch_predictions = [1 if i > 0.5 else 0 for i in logits]\n",
        "        predictions.extend(batch_predictions)\n",
        "\n",
        "        N = len(batch_predictions)\n",
        "        for i in range(N):\n",
        "            if batch_predictions[i] == inputs['labels'][i]:\n",
        "                accuracy += 1\n",
        "\n",
        "    accuracy /= len(dataloader.dataset)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "data = read_data(data_jasons)\n",
        "# for key in data_jasons.keys():\n",
        "#     data_path = data_jasons[key]\n",
        "#     with open(data_path, 'r') as file:\n",
        "#         json_data = json.load(file)\n",
        "#         if key == \"reddit\":\n",
        "#             for item in json_data.values():\n",
        "#                 data.append((item['human_comment'], 0))\n",
        "#                 data.append((item['AI_comment'], 1))\n",
        "#         else:\n",
        "#             for item in json_data.values():\n",
        "#                 data.append((item['human_comment'], 0))\n",
        "#                 data.append((item['AI_comment'], 1))\n",
        "\n",
        "\n",
        "print(\"data size:\", len(data), \"\\n\")\n",
        "train_data, test_data = train_test_split(data, test_size=0.2)\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "val_texts = []\n",
        "val_labels = []\n",
        "for item in train_data:\n",
        "    train_texts.append(item[0])\n",
        "    train_labels.append(item[1])\n",
        "for item in test_data:\n",
        "    val_texts.append(item[0])\n",
        "    val_labels.append(item[1])\n",
        "\n",
        "\n",
        "\n",
        "train_inputs, train_masks, train_labels = tokenize_data(train_texts, train_labels) #TODO add the data\n",
        "val_inputs, val_masks, val_labels = tokenize_data(val_texts, val_labels) # TODO add the data\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "epochs_num = 20\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "if load_optimizer:\n",
        "    optimizer.load_state_dict(optimizer_path)\n",
        "    epochs_num -= torch.load(epoch_path)\n",
        "\n",
        "if is_train:\n",
        "    train_accuracies, val_accuraccies = trainer(epochs_num, model, criterion ,device, optimizer, train_dataloader, val_dataloader)\n",
        "    plt.plot(train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(val_accuraccies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.savefig('/content/accuracy.png')\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    predctions, accuracy = evaluate(model, criterion, device, val_dataloader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "87Zb5vtqTLpT",
        "outputId": "970e7fe9-384e-4145-ea0d-61f52b7340f7"
      },
      "outputs": [],
      "source": [
        "# prompt: i want to  get acces in my code to a local file\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeiHy3xZUAeK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def read_data(data_key_path):\n",
        "    data = []\n",
        "    for key in data_key_path.keys():\n",
        "        data_path = data_key_path[key]\n",
        "        if key == \"reddit\":\n",
        "            with open('/content/' + data_path, 'r') as file:\n",
        "                json_data = json.load(file)\n",
        "                for item in json_data.values():\n",
        "                    data.append((item['human_comment'], 0))\n",
        "                    data.append((item['AI_comment'], 1))\n",
        "        elif key == \"wikipedia\":\n",
        "            with open('/content/' + data_path, 'r') as file:\n",
        "                json_data = json.load(file)\n",
        "                for item in json_data.values():\n",
        "                    data.append((item['Summary'], item['Is AI generated']))\n",
        "        else:\n",
        "            with open('/content/' + data_path, 'r', encoding='utf-8') as file:\n",
        "                json_data = json.load(file)\n",
        "                for key in json_data.keys():\n",
        "                    if key == \"human\":\n",
        "                        for item in json_data[key]:\n",
        "                            data.append((item, 0))\n",
        "                    else:\n",
        "                        for item in json_data[key]:\n",
        "                            data.append((item, 1))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9CT7Pq1BUBZz",
        "outputId": "bc6b0874-8c0f-4c2f-df9c-f43c91b90095"
      },
      "outputs": [],
      "source": [
        "# files.download('bert_classifier.pth')\n",
        "files.download('optimizer.pth')\n",
        "# files.download('epoch.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
