{
    "How often should promotional posts be allowed?": {
        "title": "How often should promotional posts be allowed?",
        "score": 13,
        "url": "https://www.reddit.com/r/dataengineering/comments/1dnemu2/how_often_should_promotional_posts_be_allowed/",
        "content": "Context: We allow self promotion but limit the frequency to once every 2 weeks. This poll is on whether or not we should update the frequency it is allowed.\n\n[View Poll](https://www.reddit.com/poll/1dnemu2)",
        "num_comments": 14,
        "comments": [
            "Given the complexities and level of astroturfing, I'd rather there was zero self promotion, market elsewhere.",
            "It may help to better define self promotion?  For example, if a person works for company X and a conversation has begun organically involving their product, it should be fine for them to pop in to clarify or provide information.  Everyone works for someone.",
            "Is every 4 weeks all that different to once a month?",
            "Can we force these posts to have a specific flair so we can know it\u2019s a self promotion?",
            "TY for the subtle reminder (for all of us) to go read rule #3 details again.  :)",
            "Force posts to have a self-promotion flair.",
            "I understand the logic but you might just split the votes of a similar view to a point that it is hard to interpret (don't let r/datascience see this poll)",
            "I am fine with any frequency as long as they are upfront about it.\n\nMost of these fuckers act in a very shady way which has exactly the opposite outcome they are trying to achieve. \n\nI got sucked into getting on call with Firebolt marketing folks and it was giant waste of time.  Has to be one of the shadiest data companies out there.",
            "We should point folks to the specific rule(s) number they are violating to have them clean up their act; or just go away.  You're thinking of #8, of course, but I see #1 as the biggest problem on this subreddit.  Just my $0.01's worth.  And yes, my profile name is actually a violation of #7; haha.",
            "Every 4 weeks means you can post every four weeks. If you posted Jun 24, you can post again July 22.\n\n\nOnce a month means you can post once each month. If you posted Jun 24, you could post again July 1.\n\n\nThey are effectively the same number of posts with different rules for enforcement. Every four weeks also gives one more post per year.\n\n\nI'm half joking."
        ]
    },
    "Quarterly Salary Discussion - Jun 2024": {
        "title": "Quarterly Salary Discussion - Jun 2024",
        "score": 19,
        "url": "https://www.reddit.com/r/dataengineering/comments/1d5q7db/quarterly_salary_discussion_jun_2024/",
        "content": "https://preview.redd.it/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd\n\nThis is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.\n\n# [Submit your salary here](https://tally.so/r/nraYkN)\n\nYou can view and analyze all of the data on our [DE salary page](https://dataengineering.wiki/Community/Salaries) and get involved with this open-source project [here](https://github.com/data-engineering-community/data-engineering-salaries).\n\n&#x200B;\n\nIf you'd like to share publicly as well you can comment on this thread using the template below but it will not be reflected in the dataset:\n\n1. Current title\n2. Years of experience (YOE)\n3. Location\n4. Base salary & currency (dollars, euro, pesos, etc.)\n5. Bonuses/Equity (optional)\n6. Industry (optional)\n7. Tech stack (optional)",
        "num_comments": 23,
        "comments": [
            "any value in sharing EU salaries?",
            "Forgot New Zealand in you countries list.",
            "Is the link to the DE salary page working for anyone else? I get an error any time I try to use it in any browser.",
            "Seems median pay/YOE is about 130-140k and 4 YOE. I have a little over 4 YOE and am in the final stages with a company that posted a range of 100-120k for mid level full remote. From my understanding the market is kind of tough right now so would a lot of these salaries be from before salaries went down? I hope to ask for 120k if they start lower but don\u2019t want to overplay negotiations if that is a fair range.\n\nAt 2YOE I made 125k but that was peak 2021. Most recent job was super low pay as I worked abroad for 2 years.",
            "This is solid in Ohio with 2YOE. Keep it up!",
            "just like on the maps",
            "People are just entering into the database linked above",
            "Would you mind making an issue on GitHub with the details of the error?",
            "I am also DM Consultant,  work is same except migration end point always is SAP.",
            "It\u2019s Columbus, OH\n\nI wish I asked for more, but I\u2019m also happy with what I have. Comparison is the thief of joy"
        ]
    },
    "Data engineering projects: Airflow, Spark, dbt, Docker, Terraform (IAC), Github actions (CI/CD), Flink, DuckDB & more runnable on GitHub codespaces ": {
        "title": "Data engineering projects: Airflow, Spark, dbt, Docker, Terraform (IAC), Github actions (CI/CD), Flink, DuckDB & more runnable on GitHub codespaces ",
        "score": 9,
        "url": "https://www.reddit.com/r/dataengineering/comments/1dr9p03/data_engineering_projects_airflow_spark_dbt/",
        "content": "Hello everyone,\n\nSome of my previous posts on data projects, such as [this](https://www.reddit.com/r/dataengineering/comments/ygieh8/data_engineering_projects_with_template_airflow/) and [this](https://www.reddit.com/r/dataengineering/comments/nto0nd/data_engineering_project_for_beginners_v2/), have been well-received by the community in this subreddit.\n\nMany readers reached out about the difficulty of setting up and using different tools (for practice). With this in mind, I put together a list of 10 projects that can be setup with one command (**`make up`**) and covering:\n\n1. [Batch](https://www.startdataengineering.com/post/data-engineering-projects/#32-batch-pipelines)\n2. [Stream](https://www.startdataengineering.com/post/data-engineering-projects/#33-stream-pipelines)\n3. [Event-Driven](https://www.startdataengineering.com/post/data-engineering-projects/#34-event-driven-pipelines)\n4. [RAG](https://www.startdataengineering.com/post/data-engineering-projects/#35-llm-rag-pipelines)\n\nThat uses best practices and helps you use them as a template to build your own. They are fully runnable on GitHub Codespaces(instructions are on the posts). I also use industry-standard tools. \n\n1. local development: Docker & Docker compose\n2. IAC: Terraform\n3. CI/CD: Github Actions\n4. Testing: Pytest\n5. Formatting: isort & black\n6. Lint check: flake8\n7. Type check: mypy\n\nThis helps you get started with building your project with the tools you want; any feedback is appreciated.\n\n Tl; DR: Data infra is complex; use this list of projects and use them as a base for your portfolio data projects \n\n> Blog https://www.startdataengineering.com/post/data-engineering-projects/",
        "num_comments": 0,
        "comments": []
    },
    "Why does every data engineering job require 3-5+ years experience": {
        "title": "Why does every data engineering job require 3-5+ years experience",
        "score": 107,
        "url": "https://www.reddit.com/r/dataengineering/comments/1dquupe/why_does_every_data_engineering_job_require_35/",
        "content": "Questions:\n\nWhy do most of the data engineering jobs require 3-5 years experience? Is there something qualitative DE jobs are looking for nowadays that can\u2019t be gained through \u201chours in\u201d building data architecture?\n\nWhat is the current overview of the DE job market? Is it exceptionally dry right now? Are there recruiting cycles? Is there a surplus of data engineers?\n\nDo you have personal experience with applying for DE jobs just slightly under minimum required YOE (but you make up for it in other aspects such as side projects, unique perspective, etc)\n\nHere is some context to the questions above:\nI have recently been applying to data engineering jobs and have had miserably low success. I have 2 years traditional work experience but due to my personal projects and startup I\u2019m building I really am competitive for 3-5 year experience jobs. Just based on hours worked compared to 40 hour weeks x 3 years. I come from a top 20 US college & top 10 US asset manager. Ive got a ton of hands on experience in really \u201chot\u201d data engineering tools since I\u2019ve had to build most things from scratch, which I believe to be a significantly more valuable learning experience than maintaining a pre-built enterprise system. My current portfolio demonstrates experience in Kubernetes, Airflow, Azure, SQL&Mongo, DBT, and flask but I feel like I\u2019m missing something key which is why I\u2019m getting so many rejections. Please provide advice or resources on a young less-experienced data engineer. I really love this stuff but can\u2019t get anyone to give me an opportunity.",
        "num_comments": 90,
        "comments": [
            "Because then they regret hiring people like me with only 3 days of SQL experience before getting the job.",
            "Junior positions are disappearing across all engineering spaces, not just DE.",
            "I once was recruiting for interns at a startup and I sat down with the Data Director asking about what he is looking for in interns.  Fully knowing that we were talking about interns, he said seriously, that he looked for 2 years of experience...",
            "Managers are too busy to train juniors which is BS. I make my seniors train the mids, and mids train the juniors. Having people below you gives you invaluable experience.",
            "Maybe some industry regret - hopping between subreddits around this area I've seen far too many posts about things like people wanting to apply for data science and data engineering jobs because they know excel / people having data science and data engineering jobs and feeling lost because they didn't really have any solid coursework or training beyond excel type crap, people talking about how their Fortune 500 only uses drag-and-drop ETL tools and so on, it makes me think the job market got flooded/is being flooded by a lot of people who don't really know what they are doing, and companies are starting to realize they need to do better.",
            "This is BS. I worked in this field for almost 3 decades, and it's totally possible to train people for DE starting from junior. There are plenty of simple pipelines for them to get staryed, just don't throw them them most complex mess you have to load on day 1.\n\nI write it down to lazyness, greed and a mindset of everyone for themselves that will eventually lead to failure. If you have at least one senior person, you can hire a junior. If you lose them after you train6them, it's because you don't want to recognize their growth.",
            "I would really push back on the idea that personal projects are a substitute for enterprise experience.  I just really don't see how those two are even close to the same.  Most of the time common tools found at the enterprise level are cost prohibitive to the individual user, and most personal projects I've seen or had people describe had little to none of the typical DevOps or governance best practices applied as part of the development.\n\nThat's not even to mention that most folks tunnel on the technical skills when really it's the soft skills that actually make you successful.  Either partnering with stakeholders in a way that allows you to deliver what they need and leave them thinking positively towards you or communicate advanced technical concepts to leadership and influence them.",
            "Because most of the errors you can make won\u2019t spit out an error message. The stuff is too important to trust a novice on\u00a0",
            "Because the nominal wage difference between 1-2 yrs vs 3-5yrs ain't worth the savings.",
            "Because being greedy and lazy is easy. \n\nWhy bother training someone when you can let other STUPID IDIOTS train them for you?"
        ]
    },
    "Let's talk about deployments and development ": {
        "title": "Let's talk about deployments and development ",
        "score": 9,
        "url": "https://www.reddit.com/r/dataengineering/comments/1dr6z6r/lets_talk_about_deployments_and_development/",
        "content": "\nHey Devs\nI started my career a year ago, and I got added to a really important project. Now, I feel I do fairly good with things like development in my local machine but when it comes to deployment I feel like that there is an endless loop of getting service accounts access and getting into discussions with infrastructure and their counters to why we shouldn't use an infra. I feel pretty overwhelmed with all this, I feel before getting a story there should at least be someone guiding me with stuff like this I won't magically know the environments for clients and other access stuff and my manager also has very less time for me and he is same indecisive with stuff like this. I expressed my frustrations that these clients env and accesses are tough for me to decide even if I decide something I want him to validate first as it's overwhelming.\nI feel like a good developer but incompetent with architecture and deciding how things work with clients.\nDoes everyone feel like this initially or I am super slow?",
        "num_comments": 11,
        "comments": [
            "This honestly sounds like a problem related to where you work - deployment should be automated and governed though code, and should be a standard pattern. You should know where you are deploying and how before you start work; I\u2019d expect this to be discussed in refinement, or however you distribute work. \n\nAlso, at one year exp you\u2019re still learning - infra is a complex beast and a different set of skills to dev work. It\u2019s absolutely something you should learn to do, but a year in don\u2019t beat yourself up about not knowing it all yet.",
            "Our apps are automatically build and released when we push code  to to certain branches (or via PRs). This all can be done with yaml pipelines configs. And all our credentials are in an Azure key vault and loaded into the apps  when functions apps are called or websites are loaded.  \nShould all be done automatically.",
            "I use CDK for literally everything. This sounds like process problem where you work, becaus \u00f1e in many many cases, as a DE, you own your infra.\n\nFor example, my main compute is a combination of Glue, Lambda and Step Functions. All my ETL code is Python. 100% of my tables are deployed with CDK. So what I do is, I have 2 code repos: one for CDK and one for Python. The CDK repo creates the infrastructure including the tables and a code pipeline connected to both repos. If a change happens to any package, the pipeline is triggered and deploys the changes. Unless there\u2019s an infra change, the CDK code doesn\u2019t get touched a lot.",
            "I am working on a new platform where we have a cataloging tool as a single tenant and data platform as multi tenant. This keeps running into performance issues with huge data volumes. So our infra keeps changing frequently in terms of ingestion sources and etl deployments",
            "I mostly build etls where sources have to be accessed differently as per client environment, we are building a new platform so we also run into discussions with deployment of pipelines. So even though etl is ok but the mess of account access is rough",
            "Most cataloging tools are bad, but from experience they don\u2019t really put much load on a DB - it\u2019s lots of tiny queries, often hitting metadata (eg GET DLL in snowflake). Is it set up in a wierd way?\n\nAlso, one instance of a catalog with multiple instance to monitor is normal - shouldn\u2019t cause issues?",
            "Not deployment of pipeline, but pipeline for deployment.\n\nA deployment pipeline is a set of tasks (sometimes scripts) that configure an environment (cloud, VM, etc.) and deploy code (for example a data pipeline).",
            "So when we do metadata ingestions the metadata is consumed via Kafka and this Kafka gets overloaded when we have these ingestions. If we do async calls then the platform becomes unresponsive. If we fetch a lot of entities the platform gets down. So this is killing us. So we keep changing ingestion flows. Also we have a lot of push based ingestion mechanisms that I extend",
            "Yeah so apart from development/ deployment i am the one who has to figure out the entire architecture and how it will work. I wish it was straight up like a standard repo that has automated deployment but no I am out in the wild",
            "Sounds like DataHub. How are you deploying DataHub? Have you tried increasing resources to Kafka?"
        ]
    }
}