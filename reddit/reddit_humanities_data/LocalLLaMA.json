{
    "What do you guys think about this wild opinion": {
        "title": "What do you guys think about this wild opinion",
        "score": 98,
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1dr1di9/what_do_you_guys_think_about_this_wild_opinion/",
        "content": "I am currently doing a certification on Gen AI and the instructor said this -\n\"All text generated from a LLM is hallucinated. The generation just happens to be correct most of the time.\"",
        "num_comments": 139,
        "comments": [
            "Hes just saying that the mechanism for producing all the text is the same. Theres no time when something flips or goes wrong and it goes into hallucination mode, its all the same thing. Definitely worth remembering. Its kindof as obvious as water is wet  Your instructor phrased it provocatively enough to get you thinking ",
            "they are just playing with semantics. It's true that all of the text generated is not based on reality, but an approximation of reality which makes sense, but it's a semantic difference not some deep insight.",
            "certification on gen ai is a fake scam, sorry dude v_v",
            "Ehhh, semantics and anthropomorphising. But why even call it a hallucination, just say its the statistical probabilities of what comes next. Sometimes the probability is closest to fact sometimes its not.",
            "anyone can play semantics games",
            "Some say most of your phenomenology is a construction of your mind, a type of hallucination that just so happens to tend to have sufficiently accurate handshakes between your top-down modeling and bottom-up sense-data from the \"external world\". It's as though we too hallucinate all the time, but just so happen to be correct \"enough\" of the time. I don't have a problem with some form of naive realism though.",
            "LLMs just don't have any means of knowing what they don't know. So therefore the statement of your instructor is valid.",
            "I think the idea is that if you don't have a neural network, only a large text corpus and a \"magic\" formula to find the next token probability from the corpus, you will still get exactly what you can do with an LLM.",
            "One should not tie to the phrases like hallucinations, it just means the text generation is not matching with co text or reality. People will coin different terms for it but meaning doesn't change",
            "Yes. It's true~ This is very close to the LLM \"intelligence\"~"
        ]
    },
    "Drummer\u2019s Llama 3Some V2 is an absolute killer for RP and story telling.": {
        "title": "Drummer\u2019s Llama 3Some V2 is an absolute killer for RP and story telling.",
        "score": 27,
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1dr5bce/drummers_llama_3some_v2_is_an_absolute_killer_for/",
        "content": "I\u2019ve been playing and testing it extensively since it released, and it blows every model I tested (hundreds of models between 7B 8Q - 70B IQ2XS) out of the water.\n\nIf you like role playing - story telling, and haven\u2019t yet tried it, I suggest you get a part of the 3 some\u2026 ASAP!\n\nYou\u2019re going to have a BLAST \ud83d\udca5 ",
        "num_comments": 22,
        "comments": [
            "It's great when someone says \"you should try this model\" and then doesn't properly name the model OR give a link to it.\n\nEven just saying TheDrummer/Llama-3SOME-8B-v1-GGUF would have made it just that much easier to find.",
            "In my testing, it doesn't follow character traits particularily well and ends up in my bottom half of 7-8b models.",
            "What is your sillytavern configs?",
            "I used to think his models were good but I've come to the conclusion his datasets are weak and the models are worse than competing RP models like L3-Aethora or Merged-RP-Stew",
            "As someone completely new to the game, is there a detailed set of instructions on how to get this actually running? I've never installed a local llm before so square 1.",
            "I haven't found anything that works better than psyfighter2/tiefighter2 13b and I've been looking for a while!",
            "have you tried with the llama 3 instruct for the preset on silly tavern? I've been having decent results with that and the drummer's 3some model. better than moistral imho",
            "So which models are better in your opinion?",
            "Yeah, I don't know how people seem to rate these (3some, moistral) so high. I've tested over hundred models for (e)rp purposes at this point and while these models generally rank above the models that aren't specifically finetuned for (e)rp, they are at like bottom 25% out of the (e)rp models.",
            "It's actually almost exactly like the meta instruct when talking to it normally, in general it has fewer refusals and more nsfw variety. It's similar to an abliterated version but with what seems to be no apparent intelligence loss. It won't do nsfw stuff at all unless the prompt makes sense for it, sort of like a normal person who can switch from being normal and lewd when appropriate. Honestly kind of impressive how well it works for both.\n\nIt's been practically a daily driver for me for all things RP and chat for the past week."
        ]
    },
    "Where are we with Gemma 2 with llama.cpp?": {
        "title": "Where are we with Gemma 2 with llama.cpp?",
        "score": 9,
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1dr9h3t/where_are_we_with_gemma_2_with_llamacpp/",
        "content": "Hi, I understood that a llamacpp update corrected a part of the problems but there are still issues. Could you confirm there is no actual GGUF properly working?\n\n\nTried the HF transformers which seemed to work.\n\nThx!",
        "num_comments": 6,
        "comments": [
            "it weirds me out that we did not streamline this.",
            "Thank you so much for this clear answer about the actual issues!!! So it can take some time before getting a fully working version ^^",
            "Yeah, give it a week and it'll be all sorted.",
            "> Logic Soft-Capping, which Gemma-2 was trained with but which was initially not implemented in Transformers due to it conflicting with flash attention, was far more important than Google had believed it to be. Especially for the larger model.\n\nHeh, I saw this in the paper and it kinda raised my eyebrows. \"Just leave it out, it'll be fine,\" though I did take their word for it.\n\nDo Google engineers not *test* this stuff for a release? I get that OSS Gemma is kind of a gift/side project, but still, it's starting to be a pattern.",
            "Merged last night\n\n[https://github.com/ggerganov/llama.cpp/pull/8156](https://github.com/ggerganov/llama.cpp/pull/8156)",
            "Gemma had two major issues at launch which we know of so far.\n\nThe first was an incorrect tokenizer, which was fixed relatively quickly though a lot of GGUFs were made before that.\n\nThe second issue which was discovered much later was that Logic Soft-Capping, which Gemma-2 was trained with but which was initially not implemented in Transformers due to it conflicting with flash attention, was far more important than Google had believed it to be. Especially for the larger model.\n\nThe first issue (broken tokenizer) has been fixed for a while, and fixed GGUF has been uploaded to [Bartowski's Account](https://huggingface.co/bartowski). But the second issue has not been fixed in llama.cpp yet. There is a [PR ](https://github.com/ggerganov/llama.cpp/pull/8197)but it has not been merged, though it likely will be very soon based on the recent approvals.\n\nIt was first believed that GGUFs would have to be remade after the PR got merged, but a default value was added for the soft-capping which means that old GGUFs will work as soon as the PR is merged.\n\nSo to summarize, if you download a GGUF from bartowski right now it will work as soon as the PR is merged, but before then you will experience degraded performance. Especially on the 27b model, which is entirely broken at certain tasks at the moment.\n\nIt's entirely possible that there are issues beyond just these two. It's not rare for various bugs to rear their heads when a new architecture emerges after all. And I have seen some say that they are experiencing issues even after the fixes. Like [this](https://github.com/ggerganov/llama.cpp/issues/8183#:~:text=start_of_turn%3Emodel%5Cn%22-,EDIT,-After%20%238197%20the) post.\n\nIt's also worth noting that since llama.cpp does not support sliding window attention at the moment it will likely perform pretty poorly with context sizes larger than 4K. There is an [issue](https://github.com/ggerganov/llama.cpp/issues/3377) open for sliding window attention but it has not really been worked on so far since few models actually use it."
        ]
    },
    "I feel like the biggest idiot": {
        "title": "I feel like the biggest idiot",
        "score": 30,
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1dr34ps/i_feel_like_the_biggest_idiot/",
        "content": "I\u2019ve been screwing around for a couple of months getting my local development server up and running and I have had so many issues with Nvidia drivers it\u2019s crazy. I\u2019m primarily using Ubuntu because so many of the tutorials or installation examples use Ubuntu. I\u2019m currently on Mint and it\u2019s been ok so far but has glitched more than once and I had to fix it. I finally just started manually reviewing all updates. I even had a cups update screw up my video drivers by doing something to the kernel. I don\u2019t get it.\n\nI am so dumb. I just found out you can enable cuda on wsl. Yep. Pure moron over here.",
        "num_comments": 45,
        "comments": [
            "WSL was not viable for my team either, we all moved to linux native or macOS.",
            "I'm not sure I understand what you're saying here, but if you're saying you're thinking about moving from native Ubuntu to Ubuntu under WSL, there's a major performance difference in at least one place. I started on WSL and moved to Ubuntu native and my time to load a 70B 4 bit K\\_M GGUF quant went down from twenty minutes (!) to about 90 seconds.",
            "Use linux and conda venv.",
            "linux native > windows > wsl2",
            "Cant you just containerise your setup with docker /podman?",
            "Also if on ubuntu: `sudo systemctl stop unattended-upgrades` and then `sudo apt remove unattended-upgrades` It's the equivalent of microsoft restarting your machine randomly for windows update and whoever proposed it ought to be taken outside and beaten with the stupid stick. Locks up apt for extremely long periods and I'm pretty sure it broke my graphics drivers on more than two occasions.",
            "Thanks, this is helpful indeed.",
            "Yeah I work at one of the big companies. We deal with cuda driver issues alllll day and night",
            "Thanks. Thats crazy. I have 3 4090s on a threadripper 7960x with 128gb ram. Loading a larger model takes me like 15 seconds most of the time natively. Ive just been going nuts with shit breaking due to updates but I dont want to risk missing security updates. I just know windows better.\n\nIm not planning on nuking it but I might try running it on KVM.",
            "I prefer the native python env. I had better results as sometimes conda would mix up which python/pip and to where is installing things"
        ]
    },
    "Quick Python tool/script to convert zips + download / convert GitHub repos to llm ready (RAG etc) .md files": {
        "title": "Quick Python tool/script to convert zips + download / convert GitHub repos to llm ready (RAG etc) .md files",
        "score": 60,
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1dqxzmw/quick_python_toolscript_to_convert_zips_download/",
        "content": "# ~auto~md~\n\nmini python based tool designed to convert various types of files and GitHub repositories into LLM-ready Markdown documents\n\n\n## Features\n\n- **Supports Multiple File Types**: Processes `.txt`, `.md`, `.html`, `.css`, `.py`, `.js`, `.yaml`, `.yml`, `.json`, `.xml`, `.csv`, `.rst`, `.ini`, `.cfg`, `.log`, `.conf` files\n- **Handles Zip Files**: Automatically extracts and processes nested zip files\n- **GitHub Integration**: Clones and processes GitHub repositories\n- **Optimized Markdown**: Generates Markdown file with metadata, a table of contents, and consistent heading styles\n\n\n[GitHub Repo Link](https://github.com/tegridydev/auto-md)",
        "num_comments": 15,
        "comments": [
            "Be sure to add a license.",
            "Update: Didn't expect so many people to see this haha! Just pushed an updated version ",
            "Omg I was just thinking about about something like this today, super cool looking thanks for sharing!",
            "Seems like the zip file needs to be inside the .git directory to be processed. You might want to add that to the instructions.",
            "You don't need to blacklist files, you're already whitelisting my friend.  If you took out the 'skipped extensions' the behavior would be identical.",
            "Cheers for the source I'll try it out! \n\nI was having dumb issues with lists for some reason so this info helps x",
            "I was trying to do something like this a few days ago, but it wasn't working because I was trying to extract it as plain text. Let's see how this method turns out.",
            "Should just be able to click browse and grab a zip file and process it that way (unless I bricked something in last update which is a high probability )",
            "Claude 3.5 isn't great at writing the most optimized code. Also pretty bad at making things into classes unless you ask for it.",
            "it seems the issue was resolved only after placing the file in the bin directory. Initially, I tried to process the zip file directly as plain text, which led to problems. Adding this detail to the instructions might prevent others from facing the same issue. Perhaps there's a need to ensure files are in specific directories for successful processing. Any additional insights or updates to the script that could streamline this would be greatly appreciated."
        ]
    }
}