{
    "Calling all new AWS users: read this first!": {
        "title": "Calling all new AWS users: read this first!",
        "score": 105,
        "url": "https://www.reddit.com/r/aws/comments/16ep32j/calling_all_new_aws_users_read_this_first/",
        "content": "Hello and welcome to the `/r/AWS` subreddit! We are here to support those that are new to Amazon Web Services (`AWS`) along with those that continue to maintain and deploy on the *AWS Cloud*! An important consideration of utilizing the *AWS Cloud* is controlling operational expense (costs) when maintaining your AWS resources and services utilized.\n\nWe've curated a set of documentation, articles and posts that help to understand costs along with controlling them accordingly. See below for recommended reading based on your `AWS` journey:\n\n## If you're new to AWS and want to ensure you're utilizing the free tier..\n\n* [What is the AWS Free Tier, and how do I use it?](https://aws.amazon.com/premiumsupport/knowledge-center/what-is-free-tier/)\n* [How do I make sure I don't incur charges when I'm using the AWS Free Tier?](https://aws.amazon.com/premiumsupport/knowledge-center/free-tier-charges/)\n* [A Beginner\u2019s Guide to AWS Cost Management](https://aws.amazon.com/blogs/aws-cloud-financial-management/beginners-guide-to-aws-cost-management/)\n* [Using the AWS Free Tier](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-free-tier.html)\n\n## If you're a regular user (think: developer / engineer / architect) and want to ensure costs are controlled and reduce/eliminate operational expense surprises..\n\n* [AWS Well-Architected Framework: Cost Optimization Pillar](https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/welcome.html)\n* [AWS Cost Optimization Best Practices](https://aws.amazon.com/aws-cost-management/aws-cost-optimization/)\n* [How to manage cost overruns in your AWS multi-account environment pt1](https://aws.amazon.com/blogs/mt/manage-cost-overruns-part-1/)\n* [How to manage cost overruns in your AWS multi-account environment pt2](https://aws.amazon.com/blogs/mt/manage-cost-overruns-part-2/)\n\n## Enable multi-factor authentication whenever possible!\n\n* [Enabling a virtual multi-factor authentication (MFA) device (console)](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_virtual.html)\n* [Different forms of MFA](https://aws.amazon.com/iam/features/mfa/)\n* [Guided tour on how to add MFA to your AWS IAM users](https://pages.awscloud.com/how-to-enable-multi-factor-authentication-for-aws-account.html?nc1=f_ls)\n* [Adding multiple MFA devices to IAM users](https://aws.amazon.com/blogs/security/you-can-now-assign-multiple-mfa-devices-in-iam/)\n\n## [Continued reading material, straight from the /r/AWS community..](https://www.reddit.com/r/aws/search/?q=free%20tier%20AND%20bill%20AND%20costs%20AND%20cost%20AND%20billing&restrict_sr=1&sr_nsfw=&sort=relevance&t=all)\n\nPlease note, this is a living thread and we'll do our best to continue to update it with new resources/blog posts/material to help support the community.\n\nThank you!\n\n**Your** `/r/AWS` **Moderation Team**\n\n    changelog\n    09.09.2023_v1.3 - Readded post\n    12.31.2022_v1.2 - Added MFA entry and bumped back to the top.\n    07.12.2022_v1.1 - Revision includes post about MFA, thanks to a /u/fjleon for the reminder!\n    06.28.2022_v1.0 - Initial draft and stickied post",
        "num_comments": 0,
        "comments": []
    },
    "Streaming Architcture Feedback!!": {
        "title": "Streaming Architcture Feedback!!",
        "score": 2,
        "url": "https://www.reddit.com/r/aws/comments/1dlpu0d/streaming_architcture_feedback/",
        "content": "Hey All,\n\ni have been tasked with creating a streaming architecture at my workplace and was looking for some feedback on my plans on how i should be implementing it.\n\nBackground:\n\n* Currently we have a bunch of DMS tasks continuously running and shoving data from our Aws prod postgres database into a redshift cluster which is then accessed by everyone(BI, CS, ETC). Since this is a startup no one had bothered to partition or set up sort/dist keys on the warehouse. Querying any of the old data causes 40min query wait time.\n* We have an application endpoints within our main product which sends out finalized transactions. These transactions are currently being dumped into an s3 bucket which is then copied over into a redshift cluster via an automated job which is continuously running. This stream account of majority of our company data.\n* in total we have 70TB of data for the past 3 years. Our company has grown massively in recent times.\n* The stream of finalized transaction has grown from 1.5k request per second to spikes of 8k request per second in a span of 2 years, with an avg of around 5k.\n* In a single day about 60GB of  parquet data with snappy compressed is generated, about 600-650 GB uncompressed.\n\nI have finally got management to recognize the need to change our data engineering practises. The first step we have decided upon is to switch over the application end point stream to a better solution.\n\nSolution:\n\n* I have come with a semi real time streaming architecture using kinesis, glue and s3.\n* I have asked the dev team to start sending me the stream data to a kinesis endpoint.\n* i would be placing all the transactions from the kinesis endpoint into an s3 bucket for staging.\n* We have continuous glue job taking this data through the 3 tier medallion layers at the same time applying SCD type 2 changes in a micro batch process\n* We run glue crawlers periodically to keep our glue data catalog updated.\n* i\u2019ll surface the data to our analyst team via redshift spectrum.\n* In the future i would also look into creating a fully streaming architecture, which continiously process these records but for now i am looking at a batch job running every 30-40min hopefully.\n\nMy main question is if this architecture is doable for current and foreseeable growth(10k RPS).\n\nJust want to understand if i am doing something wrong or if something can be done better.\n\nplease feel free to poke holes in my solution and recommend any tools you would think would help.",
        "num_comments": 3,
        "comments": [
            "So what are you trying to solve?",
            "Your proposed solution for a semi-real-time streaming architecture using Kinesis, Glue, and S3 is a solid approach and should be scalable for your current and foreseeable data growth. Here are some considerations and potential improvements to ensure robustness and efficiency:\n\n# Architecture Validation\n\n1. **Kinesis Data Streams:**\n   * **Sharding:** Ensure that you have sufficient shards in your Kinesis stream to handle the peak load (8k RPS). Monitor the shard usage and scale accordingly.\n   * **Data Retention:** Configure the retention period based on your processing needs.\n2. **S3 for Staging:**\n   * **Partitioning:** Use a sensible partitioning scheme (e.g., by date and hour) to manage the data efficiently in S3. This will help with both storage management and query performance.\n   * **Lifecycle Policies:** Implement S3 lifecycle policies to transition older data to cheaper storage classes or delete them if they are no longer needed.\n3. **AWS Glue:**\n   * **Job Optimization:** Optimize Glue jobs for performance by tuning DPU (Data Processing Units) allocation and job parallelism.\n   * **Schema Evolution:** Make sure your Glue jobs can handle schema evolution gracefully since your data schema might change over time.\n   * **Error Handling:** Implement robust error handling and logging mechanisms in your Glue jobs to manage and debug job failures effectively.\n4. **Redshift Spectrum:**\n   * **Partitioning and File Formats:** Ensure the data in S3 is partitioned and in columnar formats like Parquet for efficient querying.\n   * **Data Catalog:** Keep the Glue Data Catalog updated with crawlers to maintain an up-to-date schema for Redshift Spectrum queries.\n5. **Monitoring and Scaling:**\n   * **Monitoring:** Use CloudWatch to monitor the performance and health of your Kinesis streams, Glue jobs, and Redshift queries.\n   * **Scaling:** Design the architecture to scale automatically. For example, Kinesis can automatically add shards based on the incoming data volume, and Glue can be set up with job bookmarks to manage state between runs.\n\n# By addressing these considerations and potential improvements, your streaming architecture should be robust, scalable, and efficient enough to handle your current and future data needs.\n\nAs well I also recommend using an IaC like AWS CDK for resource management. Reach out if you need further help",
            "This! What KPIs are you trying to hit and how much money do you have to spend?"
        ]
    },
    "What\u2019s your cloud workflow like??": {
        "title": "What\u2019s your cloud workflow like??",
        "score": 7,
        "url": "https://www.reddit.com/r/aws/comments/1dlkqbm/whats_your_cloud_workflow_like/",
        "content": "Hello! I was chatting with a friend and I\u2019ve come to realize that their workflow as a developer is significantly different from mine. To be fair\u2026 I\u2019m not a cloud developer\u2026 but that makes me wonder: what\u2019s your workflow like? I\u2019m curious to learn more about what being a cloud developer is like - the good, the bad, etc etc. \n\nThanks everybody! :)",
        "num_comments": 20,
        "comments": [
            "Honestly, my workflow is 90% clicking 'apply' on Terraform, 10% crying about costs",
            "Mostly wish I had a monorepo instead of 17 different repos to wrangle for a very small and fairly cohesive product.",
            "I've been the sole person to setup our entire cloud infrastructure and deployment pipelines at my small company. Getting that all done was alot of work, but at this point, code deployments are 100% automated.\n\nYou merge a PR, code gets deployed to beta in <5min and a qa testing ticket gets generated. You create a github release and code gets deployed to prod in <5min. It's extremely easy for devs and qa to push and test code.\n\nApplying terraform IAC changes is still a manual process, but it's not that frequent, all things considered. Less and less as the products mature, and it's easier to control access by not having it as part of github pipelines.",
            "Talk to the customer or stakeholder, implement what they ask for, ci pipeline deploys it to dev and QA environments, product owner and/or stakeholder confirms intended behaviour, pipeline rolls it to production. \n\n\u201cWorkflow\u201d is a bit of a subjective term. You\u2019ll need to ask for something more specific about what you\u2019re trying to get an understanding of.",
            "Reduce your iteration time. Unit test where possible. Deploy local where possible.",
            "Automate all the things. We have a sandbox environment for each engineer: a separate aws account where they have near-admin privileges. Engineers create a branch, test locally, then deploy to their sandbox with terraform and the serverless framework from the cli. When they're happy with their work, including unit tests, they open a pr. \n\nOn a pr, we run lint, and do a terraform plan which gets added as a comment on the pr.\n\nWhen the pr is merged, it goes into a merge queue, where we rebase on top of main, then run a load of tests, and make sure everything packages successfully. If it fails tests, it gets kicked out of the queue. If it succeeds, we auto-merge to main\n\nOn main, we package things up again, though that's mostly cached, push the artifacts to some kind of store, then apply terraform and serverless to bring everything up to date on pre-prod. We run some smoke tests to make sure we didn't bork permissions or infra concerns, then deploy immediately to prod.\n\nFrom hitting \"merge\" to production takes anywhere between 15 mins and 40 depending on how much of the monorepo has been updated by the change. We have a team of about 10 people and deploy to prod about 10 times per day.\n\nThe good: safe, regular deployment at a high frequency. I can share code between components easily, refactor a component and be sure that everything is up to date at all times.\n\nThe bad: took a lot of hacking to get it all working. Sandboxes are an expense if you run non-serverless.workloads: about 30% of our bill is idle RDS instances in sandboxes that I need to kill.",
            "AWS CDK",
            "Same, but my credit card isn't on the Org account - so less crying. \ud83d\ude04",
            "HAHA that\u2019s not the first time I\u2019ve heard that \ud83d\ude02\n\nHave you tried anything to minimize costs?",
            "And spending 200 hours to save 75$ / month"
        ]
    },
    "How to configure application load balancer to be scalable for websockets": {
        "title": "How to configure application load balancer to be scalable for websockets",
        "score": 1,
        "url": "https://www.reddit.com/r/aws/comments/1dls1bu/how_to_configure_application_load_balancer_to_be/",
        "content": "I first had my eks cluster use aws CLB,when load testing the the websocket connections stop going through at 1500 websocket traffic,but when i tested internally using my app's cluster ip my cluster was able to hold over 35k websocket traffic without scaling additional worker nodes,which is where i realized that the load balancer was the issue,later found out ALB has more support for websockets but still could only hold up at most 5k websocket connections,i have set timeouts longer and still didn't work.\n\nIs there anything i can do i should i just use nginx instead?",
        "num_comments": 1,
        "comments": [
            "Have you tried idle timeout tuning and WebSocket protocol support on ALB?"
        ]
    },
    "AWS IVS Server whip & whep": {
        "title": "AWS IVS Server whip & whep",
        "score": 1,
        "url": "https://www.reddit.com/r/aws/comments/1dlrqe1/aws_ivs_server_whip_whep/",
        "content": "Hi, does anyone know how to whip & whep AWS IVS real-time stage on behalf of server, instead of mobile or browser?\n\nWhat I want to do: mobile publish to a stage, then server subscribes the audio from this stage, process it, and publish it to another IVS stage, the mobile subscribe the processed audio from the latter stage.\n\nBased on the repos [audio-only-example](https://github.com/mattvick/amazon-ivs-stage-recorder/tree/audio-only-example) and [amazon-ivs-stage-recorder](https://github.com/aws-samples/amazon-ivs-stage-recorder/blob/main/main.go), I have successfully published an audio to AWS IVS stage, but failed to subscribe it from the stage, which caused by `404 Not Found` Error.\n\nAWS IVS [User Guide](https://docs.aws.amazon.com/ivs/latest/RealTimeUserGuide/what-is.html) and [API reference](https://docs.aws.amazon.com/ivs/latest/RealTimeAPIReference/Welcome.html) do not provide any information about the Restful API, or server-side examples about the signaling or connecting, from the two repos mentioned above, I guess the formats are:\n\n* https://<xxx>.global-bm.whip.live-video.net/publish/<participant\\_id>\n* https://<xxx>.global-bm.whip.live-video.net/subscribe/<participant\\_id>\n\nI'm not sure if the two URLs are correct, any resources about the Restful API used by AWS IVS is great help to me.\n\nThanks advance for any help",
        "num_comments": 0,
        "comments": []
    }
}