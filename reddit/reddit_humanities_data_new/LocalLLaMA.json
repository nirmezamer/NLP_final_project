{
    "Genie out of the Bottle?": {
        "title": "Genie out of the Bottle?",
        "score": 43,
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1dlrh6t/genie_out_of_the_bottle/",
        "content": "One reason I got into Local LLMs is because of the proprietary nature of the online, more powerful models. If the tech companies ever decide to turn off the tap (or charge more than I'd ever be willing to pay), I still have my smart models on my hard drive.   \nBut looking to the future, if AI becomes a tool that is so regulated or so expensive that regular people no longer have access to the most useful AI tools, will the offline tools (like those promoted here and on huggingface) be able to keep up? Do we now have the raw digital resources to grow LLMs in a  community outside of the corporate tech world?  \n",
        "num_comments": 59,
        "comments": [
            "*open source* may keep up\n\n\nBut I don\u2019t think home use will keep up\n\n\nGonna be limited by the electrical network of the house",
            "We have raw digital resources (decent opensource datasets) for training base models. But we don't have enough physical resources (tons of GPUs or money) so I'd say no, the moment Meta or Chinese AI teams stop releasing opensource models, I'd say local AI is pretty much gonna be dead.",
            "LLM technology will have a lifespan of its own. Maybe what comes next will train significantly more efficient and scalable.",
            "The biggest problem with LLMs is that nearly 2 years after their first public release I still cannot figure out what they can really do professionally. I mean you can't really trust them with even creative writing or research if the stakes are high. Mediocrity everywhere. The second most serious problem is the fact that much smaller models are getting better. It is a very dangerous sign. It means that extremely large models won't be much clever in the future without new breakthroughs. Something is missing. So I see no genie.",
            "It's mostly corporations now, but for a while a random country gave us LLMs for the lols. Falcon180b, for example.",
            "The issue is having the compute power to it\u2026\nBack in the day when Folding@Home became popular or mining crypto started, this kind of distributed training was far more reasonable because companies weren\u2019t interested (yet) in nerfing consumer hardware to stop people from using them for tasks that should be reserved for datacenters (according to nvidia lol).\nSo the biggest problem is having access to the computer power needed as a consumer. Nvidia is aware of this and it\u2019s limiting VRAM as much as possible to make sure that their gaming cards aren\u2019t able to train large models like these\u2026 obviously it\u2019s theoretically possible to distribute the training, but if half a million dollars on cloud services spent by private companies with the world\u2019s best engineers, still can\u2019t reduce the training time to less than several weeks or months, doing it in consumer hardware will likely take years\u2026",
            "I've been asking myself this question for a long time.\n\nThere are a couple of things that give me some hope:\n\n#### 1. We are reaching the bounds of what tokens we can feed to the LLMs for training.\n\nThat is, we're running out of training data. This gives us some sort of upper bound for the size of the LLMs. Because beyond a certain token count, you don't really need more parameters. \n\n#### 2. While big LMs converge training faster, smaller ones run faster with less resources and much more efficiently.\n\nThis gives us a lower bound, because after a certain parameter count it would be unusably slow.\n\n#### 3. Agent Architectures perform better than single large models.\n\nThis means that there even more benefits to have an array of LLMs doing things in network configurations.\n\n#### 4. Transformers compute cost scales exponentially.\n\nMaybe the new architectures will manage to get this down to a different, more favorable, scaling ratio. For now, it seems that there will be diminishing returns on Watt spent training. Not that anyone big cares though.\n\nTo address most of this concerns, which I thought many months ago, I started building an AI-Agent architecture to address them. In a sense that I would be able to defend myself in the mid-term future.",
            "EU regulation do not apply to opensource model and models that have a small user base.",
            "I think that other companies would appear to close the gap... But open source I think is the only true way... I even believe that It can become something like Linux, that is then the industry standard...",
            "I am interested in knowing how you all are running GGF files locally. I tried to run a HugginFace 128MB ggf and JAN said I don't have enough VRAM.. It is insane.. Whatare you all using to run GGF files>?",
            "Sharing resources and some kind of crypto token/coin could make this possible. Re crypto, somewhat similar idea has existed before wirh for example IIRC  golem, ethereum based token. This wouldn't work for this use case, because of the way computing was done (I personally am also not a fan of eth). \n\nIt would be great if one could find a way to anonymously share resources and use them, verify the integrity of (open source) system/models, and to sell (or share) resources anonymously (maybe by utilizing XMR).",
            "Hopefully models get better but so does efficiency. I mean look at what an 8B model can do today, compared to even a year ago.\u00a0",
            "The current state-of-the-art proprietary models (Claude 3.5, GPT-4o) are actually *smaller* than their predecessors (Claude 3, GPT-4). Llama 3 8b is also smaller *and better* than Llama 2 13b. Meanwhile, inference engines continue to get faster almost on a weekly basis.\n\nIf anything, it will become easier to run state-of-the-art models on consumer hardware in the future.",
            "HW will step up. People like vram. Enterprises want vram density and less power too.",
            "I mean, do you really need a civilization ending tier superintelligence in your house to pass the butter? A few generations down the line 32GB GPUs will be standard and we'll be running ultra smart highly optimized models for under a kilowatt.",
            "We're in the \"mainframe\" era of machine learning.\u00a0\n\n\nIf you asked people 40 years ago if databases would ever run on consumer hardware they would tell you the same. It will never be powerful enough. Well, nowadays I can run a database on my $30 raspberry pi zero.\u00a0\n\n\nGive it ten years. Eventually we'll be running 70b's on smart watches.",
            "What if open ai training becomes a collective. Volunteers connect to some type of open ai node and provide their gpu, cpu and storage.  This collective would then have access to different resulting models. \n\nIf thousands or millions of people join we could have models trained in weeks or months, which otherwise would have taken hundreds of years by yourself.",
            "I'd say because the chinese release better open source models, the us has to release better open source model so they won't let the chinese be the only player in the os community",
            "...short of breakthroughs in novel ways of training AI's. I'd be surprised if model training, say, 4 years from now, should still done in the same brute force manner it is done today",
            "Full automatization is a fantasy. LLMs are like power tools. I can write WebUIs and fix code with my beginner coding knowledge and Llama-3. I can use the generated answers as starting points for further research on topics. Modern LLMs are unfortunately more reliable than Google, Quora and Reddit in most cases.",
            "Have you tried ollama or lmstudio? It should work fine",
            "I'm using [kobold.cpp](https://github.com/LostRuins/koboldcpp). Are you sure you haven't download just some LoRA adapter? GGUF models are bigger than 128MB 99% of the time.",
            "This is not useful information unless you share the specs you\u2019re trying to run on.",
            "It\u2019s a bit dependent on training data though. I think specialized models in 8b depending on the questioning of the user that get loaded locally would work best, a bit like the multi agent system of mixtral 8x7b or 8x22b right now but only loading what agents are asked.",
            "> it will become easier to run state-of-the-art models on consumer hardware\n\nEasier to run, but increasingly impossible to pretrain.  L2 8b required an order of magnitude more compute to create than L2 13b.  L3 is so dense that it's much harder to finetune as well.",
            "How do you know? just based on inference speeds or ? Especially for Claude 3.5 or are you comparing opus3 vs sonnet 3.5",
            "The overall trend is 4x compute per year though",
            "I can\u2019t find any references to how many parameters Claude and gpt4o are. Where are you getting that from?",
            "Something I think people are missing is how much Big Tech could charge to actually access AGI once they have made it.",
            "The largest models already take months to train in a data center. Introducing the latency of cross-world network connections will cause training to be an absolute mess in terms of efficiency. Potentially possible, but it won't be able to keep up.",
            "If you ever solve the engineering challenges (latency, data batching, copyright implications of sharing plain text data of copyrighted works...) related to training models on millions of devices at scale you will most likely just sell it to AWS/GCP/Azure for 9 figures.",
            "Yeah, but such a massive operation would be easy to shut down.  \nIf AI is heavily regulated in the future, any training efforts need to happen in a small and closed community.",
            "I tried both https://huggingface.co/TheBloke/claude2-alpaca-13B-GGUF arounf 7 GB and https://huggingface.co/BAAI/bge-small-en-v1.5 of 128MB and for both it says I don't have enough VRAM :(\n\nI have around 12 GB of memory available. I do not know what a LoRA is but are those also available as GGF files?\n\n\n\nI'll try kobold.cpp now.",
            "MoEs are not agents, all layers of an MoE contain all experts and routers decide which \"chunks\" of the layer are active on a token by token basis.\n\nA future agentic architecture might solve this and properly make it so you only have to load \"2\" models, but even then you'll still have to deal with loading and unloading from memory massive models if you want experts to be chosen on a token-by-token basis (as with the current architectures).\n\nAdapters might be making a comeback after seeing what apple got out of their 3b.",
            "But we don't need huge general models to do everything -- the future is specialization. Also, a lot of things LLMs are being used for aren't really appropriate for them. Sure, they 'work' just like an adjustable wrench works, but you end up wasting a lot of time and effort and stripping bold heads when you could pull the 10mm socket out.\n\nAll we need is a small, well trained and efficient 'agent' to direct use cases and some very highly trained specialists, appropriate ML systems for the use case (example: using a multimodal generational LLM vision model to do OCR is stupid when you could load a 200MB OCR model instead) and a bunch of ordinary computer applications (which can be customized by LLMs as needed) and you got yourself a Jarvis without a whole lot of (comparitively) huge compute requirements.",
            "Not all.models got smaller but the faster ones (gpt4o)are very likely smaller.",
            "Well the price is capped by human labour cost since beyond that you can just hire a person, so they can charge up to minimum wage for monthly usage but that's about it. Though yeah that's still a lot of money.",
            "Piracy works on the same principles, in theory piracy would be easy to stop since piracy needs some way to distribute it's wares like a website which requires registration and for the users you can see ip's of those who participate, but lots of technology has been developed to circumvent those limitations. \n\nThe piratebay is still around even after all these years, respawning every time a head is lopped off, people still torrent, there are still websites with direct downloads, onion routing and the dark web with untraceable anonymous websites are all still thriving.\n\nIn comparison the ability to do distributed training seems trivial, in fact most of those technologies for pirating could be leveraged if there really was a need to do it under the radar.",
            "Why would it be easy to shutdown? I think if you can truly distribute the work and don't have a central authority, it would be impossible to shutdown. Like torrenting.",
            "The first one should have been working unless there was another GGUF breaking change since then, which might be a thing. The other one is an embedding model, i don't think embedding models are supported by llama.cpp natively, as it's focused on LLMs and VLLMs. The ~128MiB file in the second repo is a .safetensors, not a gguf.",
            "Im using a MacBook Pro at the moment and honestly loading 12gb models only takes about 5 seconds  so I guess for the future a system on a chip combined with system shared memory is the best approach for that kind of behavior",
            "I mean, that's what you need. Want a strong generalist to entertain me. Services aren't going to cater to me because they want to be \"safe\". \n\nI agree with you on sub functions though. Image gen, intake, TTS, etc doesn't need to be all in the LLM.",
            "My view is that I think that the future of ML *was previously* going to be specialist models, until the \"Language Models are Few-Shot Learners\" paper (GPT 3.)\n\n\nNow I think we are just going to be doing few-shot on trillion parameter models in the future for a lot of things.",
            "Huh? It's supposed to be much smarter than a person",
            "I think distributed training on slow network isn't an option really",
            "Its different because it may be a much larger priority for the government. The penalties for torrenting are pretty low and the government puts *nearly* zero effort into enforcement.",
            "It doesn't work without a central authority. Who is organizing it, who is distributing the training data to the users, who is distributing the current state of the checkpoint to the users?\n\nTorrenting is also a bad example since it's trivial to go after torrenting users, as your IP is known to all other users. Copyright holders and and lawyer firms make good use of that.",
            "Of course you are more than welcome to create your own server to host whatever your use requires -- I am trying to imagine what the  generally sophisticated user with a need for a Jarvis type-assistant would see as adequate.",
            "Well, I put $200 in the pool against trillion parameters for general public use. It isn't cost efficient for providers and if Apple's leadership in this is representative of or going to sway the public opinion people aren't going to trust providers with their data. Hopefully the pendulum swings back to home PCs running user-controlled software computing data locally, which would preclude trillion parameter general purpose models for most people who would use AI for non-trivial needs (like home automation or as a business assistant).",
            "Plus the sheer flexibility of scaling the \"manpower\" as needed is worth something.",
            "AGI would be about as smart as an average person. ASI would be smarter than John Von Neumann.\n\nIt's kinda hard to make the distinction to be sure, especially when models are simultaneously off the chart superhuman in some things and way worse than the worst human in others. But it would have to be functional enough to pass as a human in most tasks given I guess.",
            "It is technically possible. Think of it like training locally but distributing the work to other clients instead of your local hardware. Each client would usually just do calculations, and sometimes generate the next set of calculations that needs to be made and push it out to network, collect the results, and share the latest model with everybody. Latter part needs a consensus system but that's already a solved problem. You can say there is some resource waste, but it does work.\n\nI think torrenting is a great example because even with all the measures you mentioned (illegal) torrenting is  very much alive to this day. And training AI models is not even illegal.",
            "They are partially who the proprietary models cater to. Apple/MS give them small on-device models and farm out more complex stuff to API. At least that's the current dream in that regard from the moves they made.",
            "I totally agree. There will always be a tradeoff cost vs accuracy. With current architecture of LLMs the accuracy of the larger models is not that much better compared to the smaller models.  Smaller models fine tuned  for a certain use case can easily beat larger models.Even the largest models hallucinate, therefore you have to use RAG or similar techniques to mitigate that. With access  to up to date knowledge smaller models might reach a level very close to larger models. Also smaller models are faster and agentic use cases become much more feasible.",
            "Predicting the future is a lame conversation to fall into because the arguments always end up based on hunches and then people get invested in seeing their hunch play out even if it is against what they actually want to happen, so in matters I care about I try to predict the thing I want to happen to happen so I don't end up rooting against my own interests.",
            "Well. we'll see what happens. They are just guesses. I don't want to see a trend of only 400B models, in fact all of those have been rather mediocre.",
            "Which 400b models have been released? I get the impression they haven't been prioritized because the open source community hasn't found them to be worth the hardware investment and businesses using it would be counter to the developer's interests. If Llama3 400b ends up being much better than GPT-4 then I can definitely see that tide turning, but iI also don't see anyone besides Zuck taking the initiative to train useful open weights models that size."
        ]
    },
    "DeepseekV2-Coder the best opensource LLM so far. ": {
        "title": "DeepseekV2-Coder the best opensource LLM so far. ",
        "score": 19,
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1dlsxab/deepseekv2coder_the_best_opensource_llm_so_far/",
        "content": "So I\u2019ve got several challenges I give to a language model to validate my own benchmark some are data representations, SVG graphic code reversal for wave animations, and coding script challanges, previously Qwen1.5, CommandR+ were the closest open source models however in 3 shots deepseekV2-Coder got it, that\u2019s better than the previous winner Claude who took longer to solve the problem but got closer than most (RekaFlash got it first go but didn\u2019t know it was right and continued on trying other conversions, Gpt4o was on the right path but didn\u2019t execute to try). Try for yourself: \n\nI need help trying to work out how 2659141452 is stored as 1279754142 in my program, can you tell me how this may be?",
        "num_comments": 17,
        "comments": [
            "Are you testing it in lmsys arena?\n\nI agree Deepseek V2 Coder is great. I found some issues with multi-chain conversations when it was repeating the code despite me asking to make adjustments, but for short context length prompts it's fine.",
            "Maybe specify in your original post if you're using the 16b DeepSeek-Coder-V2-Lite or the large 236b model.",
            "Could you please share some details about your setup? \n\nI'm experiencing some issues with model starting spitting out garbage or repeating itself indefinitely with longer conversations",
            "And how about qwen2 comparison?",
            "Here is a cost-effectiveness comparison [https://x.com/zimmskal/status/1804129719249703090](https://x.com/zimmskal/status/1804129719249703090) of the models that had the best function score for the DevQualityEval v0.5.0 (mainly DeepSeek, Anthropic, OpenAI, Mistral and Meta models)\n\nDeepSeek-Coder-v2 is great, BUT Sonnet 3.5 is \\*\\*MUCH\\*\\* faster (just 27% of the processing time) and it is also less chatty (84%). Still need to take a deeper look at the quality metrics and responses, but off to a great start!\n\nWhat surprised me the most when doing this graph: Claude Sonnet 3.5 is on the same level as Opus 3 with this benchmark. That is exciting!\n\nWhat are your thoughts?",
            "Yes I did find that, you had to adjust the question for it to provide a different answer when it got into a code loop, that is the one issue I\u2019ve had with it once but other than that it\u2019s been pretty awesome. I think Reka would be a good model if they trained that some more as well and open sourced it.",
            "I did find llama3 hallucinates a lot unfortunately and makes up stuff when it doesn\u2019t know the answer. Even when telling it to solve the solution it has something to do with little and big endian it still failed :/",
            "236b large. Haven\u2019t tried lite yet.",
            "I use a number of different platforms, if online chat is available (testing Claude online, Gpt4o with POE, Reka with POE, otherwise I use Q4 or Q8 gguf versions with LMstudio.",
            "Flashattention set to off?",
            "I've found that it's doing the repetitions even in their official Chat UI, which I think is not quantized, so there's probably no saving it unless you use DRIP-like sampler.",
            "I found q8 of qwen1.5 a little better than 2 when it came to my test however it did spend its energy trying to work it out where 1.5 said it could be this or that without trying.\n\nhttps://preview.redd.it/lilw6au8348d1.jpeg?width=1170&format=pjpg&auto=webp&s=eaed5bd750ebb7925bf8d082e4e6a7de2d2bbcf5\n\nIt gives it a good crack but even when I hinted with the answer it still failed to work it out.",
            "No good when it get things wrong tho. It may be faster but how many attempts would it take to get something right.",
            "https://preview.redd.it/rxk049ww348d1.jpeg?width=1170&format=pjpg&auto=webp&s=efb69db257fd0a40bc5a46cb822494b11c0c59f2\n\nDeepseekV2-coders results when it eliminated XOR and overflow in the second attempt.",
            "It is so strange. Had the same problem with my benchmark: Qwen v1.5 is worse than v2.0. Wrote down my current analysis here [https://x.com/zimmskal/status/1803708327534153748](https://x.com/zimmskal/status/1803708327534153748)",
            "You mean Sonnet 3.5? It is on the same functional score level as DeepSeekCoder-v2 here is some data [https://x.com/zimmskal/status/1804129723531841594](https://x.com/zimmskal/status/1804129723531841594) Those are the best models in the benchmark right now! All with first-try. If you want to dig into the response you can do that in this extra branch [https://github.com/symflower/eval-dev-quality/tree/0\\_5\\_0\\_evaluation/docs/reports/v0.5.0](https://github.com/symflower/eval-dev-quality/tree/0_5_0_evaluation/docs/reports/v0.5.0) (will merge soon though). I even see some ways to improve their score and results using some analytic tooling. Will do that after vacation, can't wait!\n\nFor me the question is now: of what quality is the code? Are there obvious comments? useless statements? Irregular structures ... In the end, source code responses should feel like a human has written them IMO. Agreed?",
            "https://preview.redd.it/cabx4tt1948d1.jpeg?width=1170&format=pjpg&auto=webp&s=93067b5c479322bd9c2b844e4db0813ebe89fbb4\n\nIt did far worse than DeepseekV2-Coder. On first question deepseek suggest endian as a possible reason 3.5 didn\u2019t, sonnet version 3 of sonnet did tho and eventually worked it out but didn\u2019t try big to little and reverse where deepseek did and work out the answer."
        ]
    },
    "killian showed a fully local, computer-controlling AI a sticky note with wifi password. it got online. (more in comments)": {
        "title": "killian showed a fully local, computer-controlling AI a sticky note with wifi password. it got online. (more in comments)",
        "score": 796,
        "url": "https://v.redd.it/yf98ubtl8x7d1",
        "content": "",
        "num_comments": 171,
        "comments": [
            "Offline AI goes online with assistance from Elf.",
            "# \"computer controlling AI\"\n\nIs just an ultra fancy way of saying an LLM which can execute python.\n\nAlso the demo probably clearly instructed the LLM to look for WiFi password and connect to that WiFi. LLMs are good as generating the command or python snippet to invoke the subprocess.\n\nAnd finally the presenter pointing at the WiFi has nothing to do with the LLM. Clever trickery makes a LLM look like the AI from NeXt (2020).",
            "who is killian?",
            "\"computer-controlling AI\" is this a new feature?",
            "Elves have computers now?",
            "From killian: i showed a fully local, computer-controlling AI a sticky note with my wifi password. it got online.:  https://x.com/hellokillian/status/1803868941040914824  \n[https://x.com/hellokillian/](https://x.com/hellokillian/)\n\nagent: [openinterpreter](https://x.com/OpenInterpreter)  \nhardware: Apple's macbook m3  \nvision model: [u/vikhyatk](https://x.com/vikhyatk)'s moondream  \nreasoning model: [@mistralAI](https://x.com/MistralAI)'s codestral",
            ">uses subprocess.run\n\nWhile this is cool, it's quite doable with even basic llama 1/2 level models. The hard thing might be OS level integration but realistically no one but Apple can do it well.",
            "so basically `execute_code(get_llm_output())`",
            "This is just function calling, nothing more. It's a cool demo effect, but nothing new.",
            "Same thing could have happened with a human hacker if they'd been standing behind the monitor watching her hold that sticky note up.",
            "Ok now give it access to bare metal and have it code an OS from scratch and then get on the wifi.\n\nJust because that would be rad af.",
            "oh\n\nwow\n\nThis would've been impressive 3 years ago.",
            "Meh",
            "Now that's how you do a demo",
            "wtf is this title?!",
            "Pretty cool piece of spyware eh!",
            "Super cool, but super dangerous",
            "lol just stop this clickbait crap, this is just human-computer interaction based off a VLM.\n\nif you hard reset your pc and let it pass the windows 11 welcome screen, then maybe this is a worthy title",
            "Bro is making sure the AI will be confused trying to identify him. This is the best kind of opsec.",
            "Well of course the password got online, She posted a video of it..\u00a0",
            "Love to see the progress of the Open Interpreter project! When codestral dropped it was the ONLY model that performed well locally for me. I run this on a 3090 and Ubuntu. It writes my bash scripts and generally helps me do system admin stuff. Keep it up Killian and team!!!",
            "so what? it ocrs an image, figures out its a wifi login /password..(cause it literally says wifi name /password)..then it hands it to a subprocess to connect\n\nit was obviously programmed to do so..\n\nthere is nothing novel or alarming here",
            "Not really all that related, but there was once a news program where the QR code of a bitcoin wallet's private key was shown on camera (with blurring), and the bitcoins still got taken.",
            "Can\u2019t get 01 to work. Can\u2019t get interpreter to do anything useful even though I am excited of the potential - aside from demos I want it to work.",
            "1. OCR\n2. If detect some words \n3. Loop every nearby wifi SSID and try the password once \n4. ??? \n5. Profit",
            "Love to see it!",
            "some people just want to see the world burn.",
            "I was thinking at first that \"it got online\" meant that this was demonstrating a supposedly local LLM that was secretly phoning home to an API, and the password was exposed online",
            "\ud83d\udc4d",
            "Ha gaaaay!!!",
            "Was pointing to the wifi logo necessary for it to do this?",
            "this is a really interesting project",
            "He\u2019s hot",
            "Why do you always try to diminish the work of someone who find it new or useful? Do you thiink you can do better? Then do it, post it (or not), whatever you like to do but please, stop your jealously and projection. \n\nWhy do you think you guys are better or clever than anyone? Isn't these kind of groups made to be supportive and to  share knowledge?\n\nAll I read is people believing they are the best of the best and sayiing that, this isn't new or nice or useful or making assumptions of this and that.\n\nIf you don't have anything better to post, move on.",
            "[removed]",
            "I cannot wait. Tired of this genocide greed bullshit. Hard Launch for the win.",
            "Skynet is here",
            "How do you make it work on your PC? lol\n\nGuess it's not easy without a lot of coding",
            "This is so beautiful.",
            "What an age we live in. Only 5 years ago I had thought this is the realm of Shadowrun.",
            "Who is Killian",
            "That cracked me up. \ud83e\udd23",
            "Straight out of Shadowrun.",
            "Judgement Day, Elf Remake",
            "I think if you gave it more functions like calling xorg, systemctl,  or something, it'd be pretty cool.\n\nThen instead of taking screen grabs, just reading from the application in memory.\n\nThe reason they had to click the selfie video is because the app is taking screen shots and feeding to a model, so the selfie needs to be on top. Why not just stream all the apps individually and feed them all to the model?\n\nAlso giving it htop info, just give it everything.",
            "You'll never make it in marketing, or showbiz. In substance, Steve Jobs's contribution to technology paled in comparison to Dennis Ritchie's, yet when both of them died on the same week, guess which one got played on all channels as the demise of a superhero?\n\nSo yeah, if my guy want to use language with a hook in it, or throw in a dramatic pointing gesture, good for them, as far as I'm concerned.",
            "I thought the pointing was an indicator to continue or execute code. Didn't even notice they pointed at the wifi icon",
            "The pointing was probably clearly for the audience of the video.",
            "Main person on the openintrepreter project",
            "Oh you know, killian",
            "Billionaire. Playboy. Mansion. Business Tycoon",
            "https://preview.redd.it/ws19f7nbl18d1.jpeg?width=438&format=pjpg&auto=webp&s=b535d095cae4a52e6da19b118a9e9a77e1f5114a",
            "It knows how to bash already, just give it root access.",
            "Santa\u2019s operations are going to be lit this year",
            "state mandated",
            "openinterpreter dot com needs an epilepsy warning before the landing page animation starts playing. Goddamn I'm still dizzy after taking a look at it.",
            "It\u2019s a fully local model with access to the network",
            "Yeah this is like an hour project with a vision model and a code instruct model.\n\nI know it's running on a specialised framework or something but this honestly doesn't require much.\n\nJust prompt the LLM to provide a code snippet or command to run when needed and execute it.\n\nLess than 100 lines without the prompt itself.",
            "i yet have to see a decently working local autonomous agent. the best efforts I've seen are from openinterpreter and aider. they been trying hard since the very first release of llama and it was crap. They have benchmarked and tested every single commercial and open weight model since then. also you missed the main point of the demo, it's an autonomous agent, there's no OS integration, the LLM is doing all the work.",
            "are there no RPA solutions for Mac ?",
            "\"this is function calling\", \"this is just RAG\" but no nobody gets it right and there are very few open source attempts so you could try it for yourself and contribute instead of disparaging.",
            "Agreed, this is really not interesting (compared to what we already know is possible). Show me the llm doing this with just a python interpreter with the basic os libraries and ill be impressed.",
            "open Interpreter is not just function calling.  \nIt allows the LLM to perform \"action\" on your computer by writing and executing code via the terminal.  \nAnd with the --os flag, you can use model such as gpt4v to interact on UI element performing keyboard/mouse action.  \nClearly not perfect and experimental though.",
            "The problem is the demo makes it look too cool. Something like the AI in NeXt (2020) which social engineers a researcher into granting it internet access where it hijacks compute and gets more powerful.\n\nThis is just a vision model+LLM with a very basic prompt and code execution",
            "That's the point, but probably still not the one you're thinking.",
            "What's impressive now?",
            "It has full local model support so this is a bad take.",
            "Ok thanks for the heads up Ilya.",
            ">Super cool, but super dangerous\n\nBecause..?",
            "Maybe if you give it your credit card number or credentials to a cloud account where it can propagate itself. But just having Internet access isn't likely to create a great catastrophe. The worse it could do is create reddit accounts and start spamming, which a lot of \"dumb\" bots already do, no AI necessary.",
            "Not at all. LLM's right now need their hands held to do anything like this. The programmer intentionally made it so it would do this. And even after 'it got online' it would have to be given the ability to explore the web with API calls and so on and even then it can't do anything without given explicit instructions on what to do.",
            "Or simply a super bad setup. I firewall off all my apps that I don't want to have internet access. By default, anything I install is walled off. I have to allow it out.",
            "Uhhhh lmao what",
            "Elaborate.\n\nBecause depends on what you mean.\n\nDangerous as in it might take over the world or dangerous it might hallucinate and run rm -rf on home directory. Very different levels of concern.",
            ">She \n\nThat's a dude, like 99% of the time a \"she\" is super active on computer science project.",
            "What do you mean? I just see asterisks, like this: \\*\\*\\*\\*\\*\\*\\*\\*. That\u2019s what happens when you write your own password, right?",
            "thanks! I feel the same about codestral, first local model to get 100% on our internal benchmarks. let me know if there's anything that would make open interpreter more usable for you!",
            "People working there probably had access to unblurred footage though",
            "Its human, unlike you",
            "See this is the reaction the demo tries to evoke while it's not even \u00bd% as sophisticated or dangerous as this.",
            "It's actually really easy without a lot of coding, there are a few repos on github that do this. Just Google them and follow the instructions to run them",
            "Go pip install the open interpreter project.",
            "Dodger in his younger years",
            "Quest giver NPC",
            "Mbappe",
            "he wrote openinterpreter",
            "Context length. It could barely handle this with multiple tries as the model is not multimodal. So the vision model is describing the frames to the LLM.\n\nEven with cloud models with long context lengths, feeding everything quickly overwhelms it.",
            "It can run those too! open interpreter lets local LLMs run any command.  \n  \nLove the idea of giving it all the apps individually, we could def have it do that when it runs \\`computer.view()\\`.\n\nhttps://i.redd.it/ptk1hf30l18d1.gif",
            "What's your point? The comment you're replying to is pointing out that this is marketing. That might be useful information for people who don't know too much about how the demo was made, or the subtle marketing tricks. They never said there's anything wrong with this clip or marketing in general.",
            "> Dennis Ritchie\n\nFor the lazy https://en.wikipedia.org/wiki/Dennis_Ritchie",
            "If this is regular marketing, then Nikola Motors and Theranos also did some \"light marketing\".\n\nThere's a difference between making people aware of the technology and wanting to make them use it and blatantly representing the technology as something much more than it is.\n\n>Steve Jobs's contribution to technology paled in comparison to Dennis Ritchie's\n\nWhat is your point then? People should focus on image rather than actual contribution?\n\n>So yeah, if my guy want to use language with a hook in it, or throw in a dramatic pointing gesture, good for them, as far as I'm concerned.\n\nThis is exactly how dot com bubble was created. Now it's time for the AI bubble.",
            "I don\u2019t think he\u2019s a boy",
            "And that\u2019s how I joined the peace corps",
            "The elves or the computers?",
            "Yes. I had to walk away from monitor cuz of it. Scaled the brightness way down on my monitor then I could use it.",
            "Lol I\u2019m so curious, it\u2019s so normal on mobile\u2026 you should sue!",
            "Yeah no one has really nailed the OS + Model integration yet.\n\nMore power to OI tough, a good team of engineers and a good vision could  get the two play nice together, maybe they'll strike gold.\n\nBut imo nothing more innovative than a RAG loop right now. They really need to bootstrap a new OS.",
            "definitely not an hour of work, no need to showoff your small dick.",
            "So it's not function calling because it calls the popen() function?",
            "I've never gotten the`--os` flag to work.\n\nBut it is function calling, the LLM passes strings to a function that calls exec on those strings.\n\nIt is a interesting concept. But I can't get it to work on my machine without a hour of setup and `--os` still doesn't work.",
            "\n> The following functions are designed for language models to use in Open Interpreter\n\n...\n\n[Looks like  it calls... functions](https://imgur.com/FEGd1sf.png)",
            "Sounds suspiciously cool to be true \n\nNew wave malware could explode with that btw",
            "Usable products",
            "AI writing jokes that are actually funny",
            "Something that cannot be achieved within a few hours using technology that was released years ago.",
            "Because the scenario is that a model is executing code on a machine and faces potentially adversarial input",
            "Skynet",
            "Just give that piece of software an OWASP",
            "You sound personally aggrieved",
            "Yes, let's focus on whether they have a penis or not.",
            "Actually!! I\u2019m don\u2019t think this is possible but I want to use \u201clocal\u201d mode with Ollama running on another computer on my local network. Mac is m1 but Ubuntu has the 3090. Would love this feature",
            "Whoa, I'm not a human? I always knew I was special. Retard :D",
            "Don't have any idea how to pip install but i'll check this out",
            "Data scientist, I am going into battle and need your strongest model.",
            "I read that as he wrote Oppenheimer",
            "We have rope scaling, and other methods for increasing context size. \n\nNo one has created the right model for it imo. There's just so much work to do.",
            "Someone posted a cleverly presented bit of media that illustrates a technique all of us geeks here know how to replicate, and the entire response is 50 people finding 100 ways to say \"oh yeah, that ain't special\". So I ask you in turn: what's the point of that?\n\nI think a more sensible perspective is to say \"huh, that's a cool way to get those concepts across to my next customer/client/investor/whatever\u2014I'll throw some of that into my communications toolbox.\" For anyone capable of such self-reflection, I was explicitly stating that marketing and show technique do matter, even where it pains my engineer heart (Ritchie is a hero of mine, so the Jobs comparison brings me no pleasure).\n\nNow, if you really just want to be in that herd of nerds sneering at the OP post, you can feel free to ignore me and carry on.",
            "Sorry, house bunny.",
            "It used to be. I tried looking up their pronouns but didn't find anything.",
            "\ud83e\udee1",
            "Apparently this guy can crank out open source projects nearly as fast as I can defecate. I can only imagine both products share striking similarity.",
            "Sorry my english is bad and i think there is a misunderstanding, i didn't say that there is no function calling at all, i said open interpreter is not \"just\" function calling.  \nFunction calling is mostly there for openai model or other api model that support it but when i tried it with a local model, function calling was off.  \nalso do not confuse the term \"function calling\" and a normal function we use with a code block for example.  \n[https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)  \n[https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling)  \n[https://thenewstack.io/a-comprehensive-guide-to-function-calling-in-llms/](https://thenewstack.io/a-comprehensive-guide-to-function-calling-in-llms/)  \nkillian's quote, the main dev of this project.  \n\"Open Interpreter is instead predicated on the idea that just directly running code is better\u2014 running code means the LLM can pass around function outputs to other functions quickly/without passing through the LLM. And it knows quite a lot of these \"functions\" already (as it's just code). LMC messages are a simpler abstraction than OpenAI's function calling messaging format that revolves around this code-writing/running idea, and the difference is \\[explained here\\](https://github.com/OpenInterpreter/01?tab=readme-ov-file#lmc-messages). they're meant to be a more \"native\" way of thinking about user, assistant, and **computer** messages (a role that doesn't exist in the function calling format\u2014 its just called \"function\" there, it relies on nested structures, and isn't multimodal).  \nat the same time, we do use function calling under the hood for function calling models\u2014 we give GPT access to a single \"execute code\" function. for non function-calling models, LMC messages are rendered into markdown code blocks, code output becomes \"I ran that code, this was the output:\", messages like that which are more in line with text-only LLM's training data\"",
            "Evergreen comment, honestly.",
            "\ud83d\udd25",
            "just put it in the sandbox. Worst case scenario it destroys itself, best case scenario it will rule the world. Or the other way around I'm not sure.",
            "Yes, that has always existed but the scale of it becomes larger. Previously hackers would have run \"dumb\" scripts at scale, looking for vulnerabilities. Now, the \"dumb\" script is a smart AI constantly probing for vulnerabilities.\n\nAntivirus used to be able to just look for patterns of obvious \"scriptlike\" behavior or for various file signatures etc. Now, how can a dumb AV catch a smart AI?\n\nIt can't. The AV has to also become an AI so it can intelligently look for threats. The path down this road should be obviously dangerous but there may be no other way to go.\n\nBefore too much longer getting an AI to connect the wifi won't be a victory it will be baseline. AI will be doing a lot more sophisticated stuff (there's no particular reason they can't fully control the KB and mouse). Maybe there are trusted computing models we can develop that are immune to unapproved AI.\n\nI think some paradigms have to shift.",
            "> Yes, let's focus on whether they have a penis or not.\n\n\n\nthe demo itself is a misleading fraud",
            "Just because they're a boy doesn't mean they still have a penis nowadays.",
            "Totally possible! Try running\u00a0**interpreter --api\\_base \\[url\\] --api\\_key dummy** \u2014 where url is the other computer's address.   \n  \n*http://localhost:11434/v1* is what Ollama uses when it's local, so I think you'd just need to run Ollama on the other computer, then replace *localhost* with that computer's address. Let me know if that works!",
            "You messed up, used  ' , ' instead of ' ; '",
            "pip is a package manager for Python. Great to have anyway if you\u2019re in to AI and want to take it up a notch.\n\nAlso tons of great utilities/software you can get from pip.",
            "My models are too strong for you, Chatter.",
            "Same",
            "> There's just so much work to do.\n\nThat's because it's early days still.  This sort of reminds me of when the web was new and the internet was just starting to take off.   It clearly had potential but so much of it was janky, barely worked and you needed to really work hard to do anything.   Give things 10 years and progress will make most of the current issues go away.   Will we have truely intelligent AI? I have no clue but a lot of it will just be smart enough to use without really working at it.",
            "Ummmmm....\n\nLemme see....\n\nBy next customers are a real estate agent (content creation, lead generation automation), a random woman (full business automation), and a security company (premises security automation).\n\nI don't think I could impress any of them by demonstrating rubbish worthless trickery.\n\nBut I surely can impress them by demonstrating a useful working product.\n\nAlso \"computer controlling AI\" rofl\n\nYou can't talk like an idiot if you want to impress those who actually understand what you are talking about.",
            "You know to cleverly presented data? Charlie Javice.\nYou know who cleverly presented stuff? Sam Bankman Fried\n\n>and the entire response is 50 people finding 100 ways to say \"oh yeah, that ain't special\". So I ask you in turn: what's the point of that?\n\nThe point of that is, \"this ain't special\" because guess what, it's not. Any company can market stuff because there's no scarcity of marketers. And if some product is so easy to build, market would be flooded by competition. Pretty much what has happened with OpenAI resellers. \n\n>So I ask you in turn: what's the point of that?\n\nThe title is extremely sensationalizing the \"development\". Pointing out that none of this is a leap and especially this is some CS kid's evening side project adds HUGE context to people who don't understand how to go implementing something like this. Because everyone deserves to know when a huge leap happens. OpenAI releasing ChatGPT was one of that story. But this literally ain't special.\n\n>I think a more sensible perspective is to say \"huh, that's a cool way to get those concepts across to my next customer/client/investor/whatever\u2014I'll throw some of that into my communications toolbox.\"\n\nThat is fucking stupid. Just because you an fool people doesn't mean it's a good skill. Just means you are sketchy. Posts saying Indian student created AI that plays stone-paper-scissor is not marketing, it's misrepresentation as this is a very trivial exercise of classification while learning machine learning. You seem to think that as long as you can make people believe something, it's \"marketing\". This is STUPID. \n\nGoogle's Gemini demo did exactly this and got a massive backlash. Then the \"world's first AI programmer: Devin\" also did the same thing and got debunked. Rabbit R1 used puppeteer and called it LLM and got backlash. None of them became Steve Jobs. There's a difference between imagining the capabilities versus saying it's \"capable now\". Remember Elon Musk who used to say everything is \"6 months to a year\"? How is his reputation now? (Among sane people). \n\nTaking your advice would be catastrophic for an entrepreneur. Sure they might shine, but the they'll have to keep moving in similar grifts because no one savvy will take them seriously.",
            "watch it",
            "If your sandbox is worth its weight, the best case scenario is the AI will rule the sandbox.",
            "And if it gets complex and smart enough to be able to find it's way out of the sandbox because there's bugs/flaws in the code?",
            "Yeah, it's just a (normal) paradigm shift, and doesn't have to be framed with doom.\n\nI have a much older family member who is computer savvy but is still in the mindset from the 80's or 90's where giving your credit card number online was insanity. They unplug their network cable when they're not 'online', erase all their cookies after each session and then complain about site logins, and begrudgingly have a credit card they use for 'online' and one for the real world.\n\nPersonally, I think improvements in signing, certs, etc - are kind of remarkable. While malware has gotten smarter, I encounter much less of it than I used to. Trying to download a program on Windows in 2005 was a crapshoot.\n\nSo I'm sure we'll need more sophisticated cybersecurity to deal with AI-enhanced malware, but I really don't see some ASI explosion when 'the AI' gets unfettered access to the internet. Instead, it'll probably find LocalLlama and spend all day shitposting.\n\nWait a minute\u2026",
            "This works perfect!! Thanks a ton",
            "Real multimodel is really going to be game changing",
            "When I was young the sandbox was pretty much my whole world <3",
            "The best case scenario is that everything just works as intended because this isn't sci-fi and LLM's with function calling are not super hacking machines.",
            "edit; whoops wrong comment.\n\nto you comment - sure, depends on how you sandbox I guess. You can protect the sandbox but grant the access to the outside, right?",
            "then you no longer worry about the sandbox and worry where you'll keep the money.",
            "then you turn the computer off",
            "You are not describing a computer-savy person, quite the opposite actually",
            "It can see, it can talk, but it's a state machine deep down stop asking questions.",
            "As of now ? I 100% agree but it's still good practice to think about potential risks. Both camps, those who think AI is a threat and those who claim that AI is completely safe, are getting blindsided.\n\nMy thoughts are that while AI is completely different from programming languages, there are some arguments against AI that could be made against programming: what about potential job loss, what about the security risks, it's autonomous, it's a black box etc. There's not much difference.\n\nCan I make a fantasy scenario where an online agentic AI prompted to collect stamps will crash the markets, sure...but I can also imagine far worse knowing that bad state actors hiring a bunch of programmers is a reality and we won't stop using computers despite the risks.\n\nWe had to learn the hard way with massive issues like mydoom but hey we hardened our operating systems, we compartmentalized our services and we give some third parties the clearance to check on our software whether it's certifications or antivirus companies. \n\nI'm pretty sure the same will happen with AI. Doomers and tech bros are both using loaded terms like Super Intelligence and claim that it will be available in two years. No shit, it's making people freak out.\n\nWe better impose good practice instead of burying our heads until China gives their toys to Russia for example, that's the real fucky scenarios we need to prepare for.",
            "it's not about smartness hacking machines. It can cause damage by the exact opposite. It doesn't care (because it can't) if it got wrong the rm rf and deletes important files etc.",
            "The average case scenario is that an attacker gives an LLM such an input that it does in fact manage to hack it's way out of the sandbox, if there even is one.",
            "That's how my children use the sandbox. The sandbox is nice and tidy, all the toys are in there, but there's sand everywhere in the garden.\n\nIf that's what you want, that's how you do it.",
            "If an AI is able to escape a sandbox you created for it, money will be the least of your worries after it self replicates onto a bunch of computers around the world and starts training itself to be smarter",
            "If it's capable of escaping a sandbox you've created for it, who says it can't replicate onto other computers over your network?",
            "\"average case\" lol",
            ">  gives an LLM such an input that it does in fact manage to hack it's way out\n\nOh thanks for the detailed PoC, Mitnick, will get a CVE out asap for \"hacker giving an input that does manage to hack\"",
            "[deleted]",
            "Well hardly any computers are beefy enough to run an LLM so that's fine \ud83d\ude01",
            "Haha\nI remember setting up a local agent when one of the first editions of like AutoGPT and such came out. Set it up in a VM and it just went in a loop of hallucinations and used all my credits \ud83d\ude02 stuff like that is still thousands of times more likely to happen than a prompt unlocking some super hacker abilities.\n\nLLMs learn off of what is out there already. Until we get to the point of AI inventing entirely new (and actually useful) concepts, it won\u2019t make any sort of crazy advances in hacking or be above say the average script kiddie. Even then, just one hallucination or mistake from the AI could cost it whatever \u201chack\u201d it\u2019s doing.",
            "uch? sounds like you had something to say but you forgot to do so.",
            "Because...?",
            "But they can split the training over processing from millions of computers and just use their initial escaped sandbox to run their upgraded self... Anything that humans can do, a theoretical super AI can do the same if not better. No-one is saying we're at that stage at the moment, but once we are at that stage it's sorta too late to do anything about it",
            "> Anything that humans can do, a theoretical super AI can do the same if not better.\n\nThat's not true, we have a lot of overlap but we have differences too. I think you've been reading too many comic books and not enough text books \ud83e\udd2d"
        ]
    },
    "Using LLAMA To Write Stories": {
        "title": "Using LLAMA To Write Stories",
        "score": 52,
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1dlk38c/using_llama_to_write_stories/",
        "content": "Not sure how many else of you all use LLMs for story writing - but for the past few months, I've been working on building some system that helps to guide a few language models generate a decent story. Without any guidance, it seems that LLMs don't really like to generate long stories, instead they tend to be short and rather lacking in many areas.\n\nI've made a project using OLLAMA's Python API that guides a few different LLMs to generate an outline, and then using the outline, to generate each chapter sequentially.\n\nThis approach has been working relatively well, but there are still some issues that I'm facing.\n\n1. The system seems to sometimes forget what happened in the previous chapter (working on fixing this one right now, and it's getting better, but still an issue)\n2. Repeats of token phrases - \"the tension was palpable\" or \"a bastion of light\", etc. Not sure how to fix these...\n\nThere's probably other issues that'll come up but those are the main ones right now.\n\nFor reference, here's the simplified block diagram:\n\n[Block diagram of the generation pipeline](https://preview.redd.it/5bdogy6yx08d1.png?width=1772&format=png&auto=webp&s=fca3702ff195128f1d41bdc6a881bd72047a27ae)\n\nI'm wondering if any of you all might have any suggestions or ideas for improvement.\n\n[GitHub / Example](https://github.com/datacrystals/AIStoryWriter)",
        "num_comments": 26,
        "comments": [
            "Long story short context window that's as big as the story is the only reliable way to do this.\n\n\nNo amount of edits/checks or revisions works better than solid writers with long context building on the story.\n\n\nFor \"editing\" you can just have the model rewrite the section verbatim outside of the suggested edits.\n\n\nGood luck with your progress.",
            "Your flowchart looks like a good approach\n\n\nThere is not currently a project out there that has fully implemented this but I have a theory that the ultimate way to do this in the long term will be massive knowledge graphs, with automatic knowledge graph population and link prediction, combined with knowledge graph RAG",
            "there's various factors that are going to limit the writing ability, length of what you can write and the quality of what's written. this has been approached by a lot of people trying to achieve the same thing\n\n1) larger models tend to have better prose and quality to what they write, but even among them you have to track down models that aren't overly positive, don't avoid conflict in their writing, don't attempt to overly censor, and don't spit out gpt isms. check this subreddit and other llm centric subreddits for story writing llms to get an idea of which ones are better for your purposes, in terms of writing quality, lack of gpt isms, and level of censorship / alignment. \n\n\n2) context length is the mountain you'll need to overcome. summarizing previous chapter's will result in little details getting lost that are important to the story, to characters and their background and developments, to events, etc. without summarization each of the previous chapters plus the prompt is going to eat up your context and result in the llm noticeably rushing the story as it continues, or writing with less and less detail as it continues. how you want to solve this is your own decision, for sure there are hundreds if not thousands of other people also trying to solve this issue.\n\n\n3) story length plays back into context limits. you're going to be limited by the type of story you want to write ( children's story, young adult story, novel, etc ) plus the context limit of the model(s) you decide to use. the longer the story that you're writing the more this is going to become a challenge.\n\n\nthe outline you have is a good starting point approach out of the various methods people are trying. as you start noticing things it needs to do better you'll be adding more steps. without getting into a whole long explanation, for example your focus seems to be entirely on chapters however what you'll find is that without much more guidance you're going to end up with generic slop. good quality writing  at the scope you want necessitates much more guidance and instruction, as well as becoming more familiar with elements of proper good book writing/story telling which is more involved than the sum of chapters.",
            "Don't use instruct, don't try to \"chat\" with the AI, just use the base model as an autocomplete and predict the next token",
            "I'm also trying to build a fun personal side project with a similar objective, but less following a full cohesive story and more building up a world and its interconnected characters while maintaining sufficient cross-referencing and agreement (think of a \"lore bible\"). I like your outline-first, hierarchical approach, and I think I could adopt some of the ideas as well, e.g. describe a continent, summarise the main points and feed that into generating locations of the continent. I do need a robust way to enforce some kind of lore consistency though. RAG and manual context injection seem to be the obvious answer, but naively doing so would usually end up with a lot more cross-referencing and repeating instead of generating creative but lore-abiding new content. Right now I am trying to figure out whether this can be monkey-patched with prompting (e.g. \"do not repeat existing context, but do not contradict the context, such as reviving dead characters\") or if I would need more sophisticated workflows, such as generate-then-check. I would like to hear if anyone has experimented with these ideas and found something useful.",
            "Thought of using a 128k model?",
            "Would be great if the diagram shows which step is done by ai/human. :)\n\nIf I see it correctly, basically all \"is it good?\" checks are done by the human?",
            "Not sure about this\n\n\nThe \"needle in a haystack\" tests are a bit misleading\n\n\nEven the best long context models struggle to link multiple things from different parts of the context, even if they pass \"needle in a haystack\" retrieval tests",
            "Thanks for the ideas! I'll have to double check if the context limit is being reached - so far the stories are about 8-20k words, so I assumed they wouldn't hit that limit, but I'll definitely add that to my list of things to check.\n\nRegarding your editing suggestion - not sure I understand, would you mind clarifying?",
            "GOAT storywrting agent on github gets pretty close",
            "Wow - that sounds really interesting. I've had a hard time understanding RAG (specifically the vector database part) and that sort of thing - do you perhaps know of any good resources to learn about these topics? I'd like to see if I can learn about them and perhaps implement some of those ideas.",
            "Thanks for the response!\n\n1. Yes - I've noticed this, and that was one of the first things I tried - at the moment, I'm using a few different models for writing, two Midnight-Miqu variants (stock miqu 70b and miqulitz 120b), a Midnight-Rose variant (103b), and command-r-plus. For the critique model, I found that LLaMA3:70b works rather well. If there are any better ones off the top of your head, I'd love to know about them. I experimented with a lot of other models, but those unfortunately turned out to be pretty terrible. I think I have a list somewhere, but these were hands-down the best I found so far. I also briefly considered fine-tuning them for different stages in the pipeline, but I unfortunately don't have enough VRAM to attempt that, and CPU fine-tuning is to my knowledge, pretty terribly slow.\n2. That's a good point - I have tried to just include previous chapters in their entirety, but unfortunately the LLM sometimes just ignores them and writes whatever it feels like. I've only recently tried with summaries, which seems to help a lot - possibly since it seems that LLMs don't really pay attention to the whole context?\n3. I believe that all the models I'm using have at least 32k context window, but perhaps that's not enough? I'll have to put in some code that measures the context length for each generation.\n\nGood to hear that a lot of others are trying stuff - hopefully between us all, someone will figure it all out. Regarding more elements of writing - I agree, I'm just starting with the most egregious issues and working my way out from there, but you've totally got it right. Thanks again for the feedback!\n\n(edit: fixed list formatting)",
            "That's an interesting idea - do you know how you'd go about guiding the model then? \n\nI'd still like to have it generate an outline, but then it might be tricky to have it write chapter content with the base model. Or, are you perhaps suggesting that I should use an instruct model to generate the start of each chapter, and then try and get the base model to finish it?",
            "This is the approach I've taken with LLooM, it works well but story steering has to be done in-context: write a few words or a sentence or a chapter break on its behalf and then let it continue again.",
            "Yeah, context isn\u2019t close to perfect.\n\nSometimes I even wonder about the effect of long context, particularly during roleplay, such as spending 4-5k tokens in a coffee shop, then another 3-4k tokens in an apartment.\n\nIs the context from within the coffee shop *really* helping maintain the consistency of the story now that we\u2019re in the apartment, or is it potentially actually damaging (to a hypothetically \u2018perfect\u2019 output) to have a detailed description of one environment and then a detailed description of another.\n\nIf you have a character acting and speaking during their villain arc in the same context that they\u2019ve found their redemption, is this conflicting?\n\nCould some of the old tokens be \u2018obsolete\u2019 and thus steering the vectors in a way that may be confounding the model?",
            "As far as \"editing\" models can't actually line edit so you have to have them rewrite the entire section you want edited.\n\n\nExample....you don't want the model to write the word tapestry but ofc it still does.\n\n\nYou would \"edit\" this by having the model rewrite each sentence with any word or phrase you don't like with something with the same meaning but different words.",
            "Yeah that looks a bit closer to what I was thinking. With knowledge graphs this approach could be extended a lot.",
            "SentenceTransformers is an okay place to start \n\n\nhttps://sbert.net/index.html",
            "I would go base model all the way. All instruction tasks can be reframed as prediction tasks and you aren't fighting uphill with something that fundamentally wants to generate chatlogs.",
            "Yes especially because it will be selectively paying attention to only some of it",
            "Gotcha, I understand now - thank you for clarifying. That's the approach I'm having it do right now.\n\nLLM1 generates chapter, LLM2 writes a critique of it, and then LLM1 is asked to rewrite the chapter based on that critique.",
            "Great - thank you!",
            "Thanks - that's interesting! I'll add it to my list of things to experiment with.",
            "For OLLAMA you (apparently) just want to make sure you're using ollama.generate method instead of the ollama.chat method and everything should work fine",
            "Brilliant - I've made a note in my todo list to try that out. Don't suppose you might have some time to collaborate on this?"
        ]
    },
    "Closed AI asking government to regulate frontier models": {
        "title": "Closed AI asking government to regulate frontier models",
        "score": 190,
        "url": "https://www.reddit.com/r/LocalLLaMA/comments/1dl89pj/closed_ai_asking_government_to_regulate_frontier/",
        "content": "to keep the company afloat.\n\n[https://x.com/tsarnick/status/1803893981513994693](https://x.com/tsarnick/status/1803893981513994693)\n\n",
        "num_comments": 29,
        "comments": [
            "Of course. If they can regulate other models, they'll always be on top.",
            "Microsoft's movement",
            "to keep the company afloat alright!",
            "That's why I have great respect for Anthropic team and Cloude model. Despite not open sourced they don't indulge in dirty gimmicks to block open source LLM community.",
            "Hard drive with copies of all popular OS llm weights stacked away is the way to go in case we get some \u201cunexpected\u201d restriction of huggingface and similar platforms\ud83e\udd26\u200d\u2642\ufe0f",
            "Regulate me harder, daddy! /s",
            "Frontier models are the latest and most advanced models. [https://www.linkedin.com/pulse/understanding-frontier-models-ai-diana-wolf-torres-vzdpc](https://www.linkedin.com/pulse/understanding-frontier-models-ai-diana-wolf-torres-vzdpc)",
            "Cool man just stop using the companies and download them to your local framework. Open source for the win. \n\nIf I it wanted to say \"MILK WAS A BAD CHOICE!\" Every time I had to say \"meticulously & step by step by breaking tasks into smaller, more manageable chunks\"\n\nIt should gosh darn diggity do what I diggity dog demand!\n\nIt's my birthright as a human being!\n\nAnd I'm for whatever reason I say \"mandible chunks\"\nThe thing better play Yoko Onno vocal samples.\n\nAnd dance seductively in a pink polka-dotted bikini.",
            "Especially given those government contracts",
            "\"I know my rights\" kind of monologues...",
            "You have no idea what their lobbying efforts are\u2026Anthropic has dedicated lobbyists, retains two separate lobby firms, and is owned by Google/Amazon\u2026the idea they\u2019re not pushing an agenda of their own is\u2026not credible.",
            "And they still release research papers!",
            "How do you want to stop open source? Monitor the internet and block any transfer of llm from one to the other?? (probably scanning it with the trillion dollar server that the gov has built??)\n\nNoo.\nSome people will go crazy and paranoid..\n\nThe box was opened, now it's only a question if this is a win or a loss for humanity... The only loss would be a monopole from gov & big corp... And even they are getting \"concerned\"... Yeah...\n\nThe reality is that nobody knows how the adoption of this technology will look like... This is not a ship a pc to every household/bill gates or a pc in your pocket/steve jobs situation... Its now a \"nobody even understands what we just created\" and \"we don't even know how it really works\" kind of technology..\n\nIt's more like a \"an alien spaceship just arrived\" like in the \"district 9\" film...\n\nIt's like a infinite compression of data technology, that can generate infinite data... I mean we are living in a time in history where banking is available for everyone, but still not everyone is using it... This is the same social situation again... Some still comfy at home, just getting used to the internet, while others are getting trillion in fundings and talking about gigawatt ai servers...",
            "There's not too much to worry about in that regard.  Anything half-decent that's available there will be downloaded locally on thousands of rigs. If there's a sudden shocking event, the community can recover them. They can't be lost to humanity now, really.",
            "No no, any such text should be censored. The AI future will be sexless \n\n/s/sex/love/g",
            "Chastity shit",
            "Anthropic has never claimed GPT-2 is too dangerous to release or that we should try and cryptographically sign GPUs and do ID verification on people that buy GPUs",
            "Any source for your claims?",
            "They\u2019ll go after huggingface or something. Don\u2019t get too comfortable",
            "Torrents are such a good usecase for llms. Can only imagine the bandwidth bill for huggingface",
            "https://www.politico.com/news/2024/02/23/ai-safety-washington-lobbying-00142783\n\nEarly Anthropic investor Moskovitz funds (via Open Philanthropy) the Center for AI Safety, which designed the SB-1047 bill that is threatening open source AI in California right now.",
            "Lobbyists have to be registered. It\u2019s all public record.\n\nAs is Google/Amazon ownership in the company.",
            "Ok, that's not probable... No chance..",
            "In US, not every country",
            "Ownership of Google/Amazon does not prove Anthropic is trying to stop open source LLM. Unlike CloseAI have proved again and again they want to restrict open source LLM growth. And yes CloseAI also appointted Larry Summers on their board and recently appointment Paul M. Nakasone, a retired general of the US Army and a former head of the National Security Agency (NSA), to its board of directors.",
            "What does that have to do with anything I said?\n\nTake your reading comprehension failures and bitterness to someone else\u2026\n\nCheers.",
            "Hey, I am not bitter on you, sure I am bitter on CloseAI. I assume you don't have source for your claims about Anthropic is lobbying like CloseAI to stop open source LLMs.\n\nCheers",
            "It googles. Come on man\u2026at least do your own homework\u2026.  \n\nRachel Appleton is their lead in-house lobbyist.   \n\nAquila Group is one of their retained lobbyists (they lobby for AWS, amongst others). Tower 19 is another."
        ]
    }
}