{
    "Game Ready & Studio Driver 555.99 FAQ/Discussion": {
        "title": "Game Ready & Studio Driver 555.99 FAQ/Discussion",
        "score": 188,
        "url": "https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/",
        "content": "# Game Ready & Studio Driver 555.99 has been released.\n\n**Article Here**: [https://www.nvidia.com/en-us/geforce/news/pax-dei-elden-ring-shadow-of-erdtree-geforce-game-ready-driver/](https://www.nvidia.com/en-us/geforce/news/pax-dei-elden-ring-shadow-of-erdtree-geforce-game-ready-driver/)\n\n**Game Ready Driver Download Link**: [Link Here](https://us.download.nvidia.com/Windows/555.99/555.99-desktop-win10-win11-64bit-international-dch-whql.exe)\n\n**Studio Driver Download Link**: [Link Here](https://us.download.nvidia.com/Windows/555.99/555.99-desktop-win10-win11-64bit-international-nsd-dch-whql.exe)\n\n**New feature and fixes in driver 555.99:**\n\n**Game Ready** \\- This new Game Ready Driver provides the best gaming experience for the latest new games supporting DLSS 3 technology including Pax Dei and Still Wakes the Deep. Further support for new titles includes the launch of the Elden Ring Shadow of the Erdtree expansion.\n\n**Applications** \\- The June NVIDIA Studio Driver provides optimal support for the latest new creative applications and updates announced at Computex including the NVIDIA RTX Remix Toolkit and REST API, VLC Media Player with RTX Video HDR support, and the arrival of DLSS 3.5 support in Womp, Chaos Vantage and D5 Render.\n\n**Fixed Gaming Bugs**\n\n* \\[GeForce Experience\\] Flickering or black screen if Instant Replay is enabled \\[4665009\\]\n* \\[NVIDIA app\\] FPS overlay showing NA in multiple games \\[4608943\\]\n\n**Fixed General Bugs**\n\n* CUDA 12.5 does not work with CUDA enabled Docker images \\[4668302\\]\n* \\[Microsoft New Teams/Outlook\\] Visual artifacts when MFAA is enabled from the NVIDIA Control Panel \\[4608670\\]\n* LG OLED48C4 TV is not detected as G-SYNC Compatible \\[4645783\\]\n* LG 32GS95UE is not detected as G-SYNC Compatible \\[4564083\\]\n\n**Open Issues**\n\n* \\[GeForce Experience\\] Driver page may not detect new driver has been installed until user reboots PC \\[4662117\\]\n\n**Additional Open Issues from** [GeForce Forums](https://www.nvidia.com/en-us/geforce/forums/game-ready-drivers/13/543951/geforce-grd-55599-feedback-thread-released-6424/)\n\n* \\[GeForce Experience\\] Driver page may not detect new driver has been installed until user reboots PC \\[4662117\\]\n* GeForce Experience Performance Monitoring overlay may stop refreshing GPU information \\[4679970\\]\n\n**Driver Downloads and Tools**\n\nDriver Download Page: [Nvidia Download Page](https://www.nvidia.com/Download/Find.aspx?lang=en-us)\n\nLatest Game Ready Driver: **555.99** WHQL\n\nLatest Studio Driver: **555.99** WHQL\n\nDDU Download: [Source 1](https://www.wagnardsoft.com/) or [Source 2](http://www.guru3d.com/files-details/display-driver-uninstaller-download.html)\n\nDDU Guide: [Guide Here](https://docs.google.com/document/d/1xRRx_3r8GgCpBAMuhT9n5kK6Zse_DYKWvjsW0rLcYQ0/edit)\n\n**DDU/WagnardSoft Patreon:** [**Link Here**](https://www.patreon.com/wagnardsoft)\n\nDocumentation: [Game Ready Driver 555.99 Release Notes](https://us.download.nvidia.com/Windows/555.99/555.99-win11-win10-release-notes.pdf) | [Studio Driver 555.99 Release Notes](https://us.download.nvidia.com/Windows/555.99/555.99-win10-win11-nsd-release-notes.pdf)\n\nNVIDIA Driver Forum for Feedback: [Driver 555.99 Forum Link](https://www.nvidia.com/en-us/geforce/forums/game-ready-drivers/13/543951/geforce-grd-55599-feedback-thread-released-6424/)\n\n**Submit driver feedback directly to NVIDIA**: [Link Here](https://forms.gle/kJ9Bqcaicvjb82SdA)\n\n**RodroG's Driver Benchmark:** TBD\n\n[r/NVIDIA](https://new.reddit.com/r/NVIDIA/) Discord Driver Feedback: [Invite Link Here](https://discord.gg/y3TERmG)\n\nHaving Issues with your driver? Read here!\n\n**Before you start - Make sure you Submit Feedback for your Nvidia Driver Issue**\n\nThere is only one real way for any of these problems to get solved, and that\u2019s if the Driver Team at Nvidia knows what those problems are. So in order for them to know what\u2019s going on it would be good for any users who are having problems with the drivers to [Submit Feedback](https://forms.gle/kJ9Bqcaicvjb82SdA) to Nvidia. A guide to the information that is needed to submit feedback can be found [here](http://nvidia.custhelp.com/app/answers/detail/a_id/3141).\n\n**Additionally, if you see someone having the same issue you are having in this thread, reply and mention you are having the same issue. The more people that are affected by a particular bug, the higher the priority that bug will receive from NVIDIA!!**\n\n**Common Troubleshooting Steps**\n\n* Be sure you are on the latest build of Windows 10 or 11\n* Please visit the following link for [DDU guide](https://goo.gl/JChbVf) which contains full detailed information on how to do Fresh Driver Install.\n* If your driver still crashes after DDU reinstall, try going to Go to Nvidia Control Panel -> Managed 3D Settings -> Power Management Mode: Prefer Maximum Performance\n\nIf it still crashes, we have a few other troubleshooting steps but this is fairly involved and you should not do it if you do not feel comfortable. Proceed below at your own risk:\n\n* A lot of driver crashing is caused by Windows TDR issue. There is a huge post on GeForce forum about this [here](https://forums.geforce.com/default/topic/413110/the-nvlddmkm-error-what-is-it-an-fyi-for-those-seeing-this-issue/). This post dated back to 2009 (Thanks Microsoft) and it can affect both Nvidia and AMD cards.\n* Unfortunately this issue can be caused by many different things so it\u2019s difficult to pin down. However, editing the [windows registry](https://www.reddit.com/r/battlefield_4/comments/1xzzn4/tdrdelay_10_fixed_my_crashes_since_last_patch/) might solve the problem.\n* Additionally, there is also a tool made by Wagnard (maker of DDU) that can be used to change this TDR value. [Download here](http://www.wagnardmobile.com/Tdr%20Manipulator/Tdr%20Manipulator%20v1.1.zip). Note that I have not personally tested this tool.\n\nIf you are still having issue at this point, visit [GeForce Forum for support](https://forums.geforce.com/default/board/33/geforce-drivers/) or contact your manufacturer for RMA.\n\n**Common Questions**\n\n* **Is it safe to upgrade to <insert driver version here>?** *Fact of the matter is that the result will differ person by person due to different configurations. The only way to know is to try it yourself. My rule of thumb is to wait a few days. If there\u2019s no confirmed widespread issue, I would try the new driver.*\n\n**Bear in mind that people who have no issues tend to not post on Reddit or forums. Unless there is significant coverage about specific driver issue, chances are they are fine. Try it yourself and you can always DDU and reinstall old driver if needed.**\n\n* **My color is washed out after upgrading/installing driver. Help!** *Try going to the Nvidia Control Panel -> Change Resolution -> Scroll all the way down -> Output Dynamic Range = FULL.*\n* **My game is stuttering when processing physics calculation** *Try going to the Nvidia Control Panel and to the Surround and PhysX settings and ensure the PhysX processor is set to your GPU*\n* **What does the new Power Management option \u201cOptimal Power\u201d means? How does this differ from Adaptive?** *The new power management mode is related to what was said in the Geforce GTX 1080 keynote video. To further reduce power consumption while the computer is idle and nothing is changing on the screen, the driver will not make the GPU render a new frame; the driver will get the one (already rendered) frame from the framebuffer and output directly to monitor.*\n\nRemember, driver codes are extremely complex and there are billions of different possible configurations. The software will not be perfect and there will be issues for some people. For a more comprehensive list of open issues, please take a look at the Release Notes. Again, I encourage folks who installed the driver to post their experience here... good or bad.\n\n*Did you know NVIDIA has a Developer Program with 150+ free SDKs, state-of-the-art Deep Learning courses, certification, and access to expert help. Sound interesting?* [Learn more here](https://nvda.ws/3wCfH6X)*.*",
        "num_comments": 751,
        "comments": [
            "Did they fixed broken gpu acceleration with some Vst Plugins (like FabFilter Pro-q3) that use open gl?\n\nGot this problem with 555.85, after many years it's appeared again (i remember same approx 2015-2016).\n\nDAW (FL Studio 21 here) freezing  when these vst are used (very badly if used in Linear Phase mode)\n\nDisabling gpu acceleration via regedit on Fabfilter registry everything back normal, but of course it become laggy at show frequencies.\n\nAnd got nasty problems with 555.85 on my usb audio interface Motu m2, which it start distorting sounds heavily when playing audio with MPC-BE while doing other stuff with pc.\n\nSeems some latency problems, connected with that  i said upper, while in gaming no problems at all with 555.85.\n\nRolled back to 552.22 ad everything is back normal as usual.",
            "555.99 is crashing to desktop on the Steam version of Halo Infinite during loading screen / shader compilation.",
            "Multiple screen RTX HDR \u00af\\\\_(\u30c4)_/\u00af",
            "This driver update stole my wife",
            "The last two drives are broken for Samsung monitors that are using DSC. Let the bug tracking journey begin.",
            "With the amount of complaints these drivers are getting, you start wondering why that bug tracker isn't filled yet, I'm sure there's a lot more looking at the comments the past few months, a lot of people having similiar issues with each other, can't be coincidences.\n\nI hope NVIDIA is cooking.. and not just all about this godforsaken thing that's called AI.",
            "555.99 was the first driver that gave me \"real\" issues on my new PC since I bought it.\n\nPerformance dropped a bit (1-2fps both from average and 1% Low) but was not the worst part:\n\n* Halo Infinite crashes on every boot while recompiling shaders (known issue but still sucks)\n* My 4080 started to consume more Watts in the same games & scenarios, getting hotter, therefore increasing fans' RPM to the point it was LOUD for the very first time since I have it\n* Win11 Explorer first opening became laggier (subsequent openings were fine)\n\nSomething is definitely off with these (but also 555.85 felt a little worse than pre-555).\n\nI used DDU to remove it completely in Safe Mode and clean installed 552.44 again and ALL the issues above were gone and everything went back to normal: fast, smooth, slient.\n\nI think I'll stick with it until the community won't accept another \"real\" best candidate after.",
            "Why do they seem to be having so many issues with the recent drivers? Did they change the development team or something?",
            "This driver breaks NVENC 10-bit encoding. 8-bit encoding works normally. With 10-bit encoding, quality settings or bit rate settings are ignored. The files produced are small and inferior quality. Rolling back to driver 552.44 resolves the issue. Tested with a 2080 super and laptop 4090. Tested with ffmpeg based programs. Forgot to add, tested with both the Studio and Game Ready driver.",
            "3DMark Timespy\n\n537.58  \nScore: 13538  \nTest 1: 88.98 FPS  \nTest 2: 77.06 FPS\n\n555.99  \nScore: 8887  \nTest 1: 59.48 FPS  \nTest 2: 49.81 FPS\n\nYeah...rollback.",
            "This comment is unofficial and not maintained by Nvidia. I'm not a Nvidia employee, just a fellow Redditor helping the Reddit community by tracking issues and collating information / troubleshooting.\n\nWill keep the comment updated [like I did with previous driver](https://www.reddit.com/r/nvidia/comments/1cx9i85/game_ready_studio_driver_55585_faqdiscussion/l50z04c/). \n\nEDITs section in footer can be used to track additions/updates.\n\n---\n\n**Notes**\n\n* NVIDIA Broadcast Hotfix for Windows 11 24H2 is available at https://nvidia.custhelp.com/app/answers/detail/a_id/5553/\n\n* Chromium Checkboard issue, Microsoft have officially released a patch to the Windows 11 stable channel as KB5039212 (OS Builds 22621.3737 / 22631.3737) - https://support.microsoft.com/help/5039212\n\n* 'The Last Of Us Part 1' crash during shader compile, a workaround will be in the next game ready driver - https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l8roext/ \n\n* Security Bulletin issues listed by Nvidia in the 'NVIDIA GPU Display Driver - June 2024' bulletin are addressed by 555.99 and later - https://nvidia.custhelp.com/app/answers/detail/a_id/5551\n\n* 555.99 has game profiles for Hades II, Indiana Jones and The Great Circle, Wuthering Waves despite not being in the release notes\n\n* Watch Dogs 2 flickering issue fix, see 'Troubleshooting and Workarounds' section\n\n* Halo Wars 2 in-game foliage issue fix, see 'Troubleshooting and Workarounds' section\n\n* NVGPUCOMP64.dll and/or 'out of VRAM' crashes, see 'Troubleshooting and Workarounds' section\n\n* [Nvidia App (BETA)](https://nvidia.custhelp.com/app/answers/detail/a_id/5521) may impact frame rates (?CPU bound scenarios?) on some systems, this isn't a Nvidia driver issue\n\n\n\n* Shader cache locations have been updated for 545.37 or newer drivers  - *%USERPROFILE%\\AppData\\LocalLow\\NVIDIA\\PerDriverVersion\\DXCache* and *%USERPROFILE%\\AppData\\LocalLow\\NVIDIA\\PerDriverVersion\\GLCache* \n\n---\n\n**Benchmarks and Analysis**\n\n* TBC\n\n---\n\n**Branch Information**\n\nRecent branch information only, an extensive and detailed list can be found in the subcomment at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72k0eq/\n\n* 560.38: r560_bugfix_main (Insider)\n\n* **555.99:** r555_97-02 (Game Ready/Quadro/Studio)\n\n* 555.85: r555_79-02 (Game Ready/Studio)\n\n* 552.61: VK551_06-15 (Developer)\n\n* 552.44: r552_19-08 (Game Ready)\n\n* 552.22: r552_19-02 (Game Ready/Studio)\n\n* 551.86: r550_00-192 (Game Ready/Studio)\n\n---\n\n**Confirmed Additional Issues** \n\nThis section covers issues officially acknowledged by Nvidia that are not in the original driver [release notes](https://uk.download.nvidia.com/Windows/555.99/555.99-win11-win10-release-notes.pdf) **OR** in the Reddit moderator's /r/Nvidia driver post\n\n* [GeForce RTX Series 40] graphics cards running older firmware could experience blank screens on boot with certain motherboards in UEFI mode until the OS loads - https://nvidia.custhelp.com/app/answers/detail/a_id/5411/\n\n* [NVIDIA App] [Instant Replay] automatic Tuning scanning is interrupted when NVIDIA Instant Replay is enabled [4686661]\n\n* [Halo Infinite] game crashes during the initial loading screen after updating to GRD 555.99 [4685335] **UPDATE:** *We are still investigating this issue but have added the crash to Open Issues list for now. For now, you will need to roll back to driver 555.85 or earlier driver to play Halo Infinite*\n\n* [Last of Us] game is unable to complete building shaders after updating to R555 drivers on GeForce RTX 30 series and older GPUs [4663766] **UPDATE:** workaround will be in the next game ready driver - https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l8roext/\n\n* [Core 2 Duo/Core 2 Quad CPUs] after installing R555 drivers, PCs with older Core 2 Duo/Core 2 Quad CPUs may experience a bugcheck [4696051]\n\n---\n\n**Unconfirmed Additional Issues** \n\nThis section contains issues not officially acknowledged by Nvidia but are reported across multiple forums:\n\n* [GeForce RTX Series 40 GPUs] stability/TDR/black screen issues. Check for a *motherboard BIOS* update that states '*compatibility updates for Lovelace*'. The motherboard update is in *addition* to any VBIOS update\n\n* [Microsoft Store/GamePass/Xbox Application] PC games may not have Nvidia 'game ready driver profile' settings applied due to the APPID not being present in the Nvidia game profile\n\n* [NVENC] when using 10bit encoding, bitrate and quality settings aren't applied - reverting to a 552.44 or older driver resolves the issue\n\n* [Vulkan] interop change/regression with Direct3D -  https://redd.it/1dbksr6 \n\nFor any issues not officially acknowledged by Nvidia please submit a report using the [official form](https://forms.gle/kJ9Bqcaicvjb82SdA). General guidance in [provide valuable feedback](http://nvidia.custhelp.com/app/answers/detail/a_id/3141) document. Display issue guidance in [collecting logs for display issues](https://nvidia.custhelp.com/app/answers/detail/a_id/5149)\n\n---\n\n**Resizable Bar (ReBAR) Support** \n\nNo ReBAR additions / changes since the previous driver.\n\n**NOTE** Full discussion and a complete list of ReBAR enabled games/applications can be found in the subcomment at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72kb2d/\n\n37 *unique* profiles out of 6915 profiles in the driver have official ReBAR support enabled, however restrictions may apply based on CPU platform:\n\n* Intel CPU based platforms have ReBAR disallowed for F1 2021, F1 2022, Hitman 3 and Horizon Zero Dawn\n\n* AMD CPU based platforms have no restrictions, all 36 unique profiles are supported\n\n---\n\n**Troubleshooting and Workarounds**\n\n* Watch Dogs 2 flickering issue workaround: using Nvidia Profile Inspector, in the 'Watch Dogs 2' profile add/change *0x00fc7620* to *0x00000001* AND *0x00a4f311* to *0xa2b53761*, thanks Guzz over on Guru3D\n\n* Halo Wars 2 in-game foliage issue, workaround: using Nvidia Profile Inspector, in the 'Halo Wars 2' profile add/change *0x0049C741* to *0x00000000*, thanks Guzz over on Guru3D\n\n* NVGPUCOMP64.dll and/or 'out of VRAM' crashes (shader compilation), check CPU clock/voltage/multipliers etc, particularly if system has an Intel 13th or 14th gen CPU - https://www.radgametools.com/oodleintel.htm and https://community.intel.com/t5/Processors/Regarding-Reports-of-13th-14th-Gen-Unlocked-Desktop-Users/td-p/1575863 Motherboard manufacturers are releasing updated BIOSes to mitigate the issue e.g. adding 'Intel Default Settings' profile or similar\n\n* NVLDDMKM / TDR / Stability issues, if troubleshooting (re-evaluating RAM/CPU/GPU overclocks voltage timings, disabling PCI Express Link State Power Management/PCI Express ASPM (PCH)/PCI Express Clock Power Gating in BIOS, testing with Nvidia Debug Mode, testing Powersupply [PSU] e.g set to Single Rail, disabling Hardware Accelerated GPU Scheduling setting, disabling Windows hibernation/fast startup, disabling Low Level Driver options in Afterburner/PrecisionX etc) hasn't helped try a driver considered by the community as stable/consistent\n\n* Drivers 512.95, 516.94, 517.48, 522.25, 526.86, 528.49, 532.34, 537.58, 551.86 are generally considered stable/consistent by the community. Other driver versions worth trying despite issues include 536.99, 537.13, 537.42, 546.17(W10), 551.23(W10), 552.22\n\n* Performance dropping significantly in all games after installing a driver, see https://nvidia.custhelp.com/app/answers/detail/a_id/5408/\n\n* Laptop users with major stutter/framerate issues when running games with official ReBAR support, try setting flag *0x00e942f***e** to 1 in the game's profile(DirectX) / flag *0x20feaf0d* to 1 in the game's profile(Vulkan) *OR* disable ReBAR in Nvidia's game profile *OR* use Hybrid GPU mode rather than Discrete GPU mode\n\n* DPC Latency spikes, if drivers 536.67 and later haven't resolved the issue try [potential workaround(s) in subcomment](https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72k6i7/)\n\n* Display blinking/temporary blackout, try setting *'Content Type Detection'* in *'Nvidia Control Panel > Display > Adjust Desktop Colour Settings'* to *'Desktop Programs'* if setting is available *AND/OR* adjust monitor/TV's display settings for auto-content detection *AND/OR* check for a display firmware update  *AND/OR* reset display to default settings in OSD *AND/OR* avoid triggering Display Stream Compression (DSC) *AND/OR*  disable display 'HDMI Consumer Electronics Control' (CEC) *AND/OR* disable 'AMD Instant Game Response' on LG displays\n\n* NIS (Nvidia Image Scaling) this driver still has an 8% to 10% NIS performance penalty on Maxwell/Pascal based GPUs. Workaround is available for Maxwell/Pascal based cards: https://www.reddit.com/r/nvidia/comments/tkca3g/game_ready_studio_driver_51215_faqdiscussion/i1sod9e/ \n\n---\n\n**EDITs**\n \n01: tested 'NIS (Nvidia Image Scaling)' on Pascal/Maxwell GPUs\n\n02: updated branch information\n\n03: updated profile counts and ReBAR support \n\n04: updated notes: game profiles not in release notes\n\n05: added to notes and unconfirmed: The Last of Us Part I issue\n\n06: added to unconfirmed: Halo Infinite issue\n\n07: updated notes: Security Bulletin (June 2024)\n\n08: added to confirmed: NVIDIA App Automatic Tuning issue\n\n09: updated notes: 'Halo Infinite' and 'The Last Of Us Part 1' crash\n\n10: moved from unconfirmed to confirmed: Halo Infinite issue\n\n11: added to unconfirmed: NVENC issue and Vulkan interop issue\n\n12: updated notes: Chromium Checkboard Windows 11 patch\n\n13: updated notes and unconfirmed: 'The Last Of Us Part 1' crash\n\n14: moved to confirmed: 'The Last Of Us Part 1' crash \n\n15: added to confirmed: Core 2 Duo/Core 2 Quad CPUs issue\n\n16: added to notes: NVIDIA Broadcast Hotfix\n\n---",
            "I reverted back to 555.85 all issues fixed this is probably the worst driver i have installed.",
            "How is it possible that I have to use a 1 year old driver because all other drivers are incredibly bad.\n\nI have no words to express how bad your team is, I have to keep changing the driver every time because I still hope that with a new driver I can improve my performance, but that never happens! I use the 546.33 driver, because when I update to the latest one, my computer's performance drops drastically!",
            "\u201cMost valuable company in the world\u201d and they can\u2019t get drivers right.",
            "Better than 537.58???",
            "Express Installation for 555.99 won't Install even after restart. The driver previous to this 555.85 did the same thing which i then used DDU and manually installed it but it ended up causing issues in games in terms of flickering and lighting. Anybody else having issues?",
            "Rolled back to 555.85, I haven't had a bad driver update in a long time like on 555.95. My frames were lower on some games. Some kept crashing. Some didn't open up like Halo Infinite. When I rolled back I saw a significant spike in performance and stability.\n\n3080ti with 13900k, 128gig ddr5",
            "Team 537 is saying hello! .o7",
            "I already subscribe to Gamepass Ultimate, so here's my code for whoever would like to use it:\n\n22Q62-99H4F-DVGPV-7D34K-FC4HZ",
            "I installed this driver and it greatly reduced my stuttering in Fortnite, it also improved the frame rate in some ways. I tested it in The Witcher 3, xdefiant and it seemed to be very stable and similar to the performance of what I was using: 537.58. For now I recommend it.\n\nGPU: RTX 4060",
            "I have installed this driver like 3 days ago, before everything was fine, yesterday and today while playing poe my game crashed multiple times and when i restarted the game i got bluescreen, is it possible that the driver is crashing something? RTX 3060Ti",
            "This is the absolute worst driver I've ever had from NVIDIA.  I started getting consistent freezing on desktop, was driving me insane.  Thought maybe my RAM was going bad, ran a full Memtest with no errors.  Reinstalled OS, still happening.  Rolled back to 551.86 and completely fine now.",
            "[removed]",
            "Loved the last driver. This driver brought back some of the older issues with micro stutters and things like that. Dang nvidia come on been an issue this entire year need to figure it out. Some people's cpu usage is jumping way up also because of it. \n\nLuckily isn't as bad as before but I tested it on the 4060 was much worse then on my 4070 super. This latest drive legit crashed my game acc twice and had heavy stutters and lag with the 4060. 4070 super only has some micro stutters still fun luckily and overall good. Just am sensitive to it and I know how buttery smooth these can be. Stop it up! No1 thing with every driver launch nvidia should be doing is making sure no stutters happen. Should be like the final check or something.",
            "Anyone else experiencing High Temps? i used to run 57C-61C on this game and now im running 65C-79C, and with another game i used to run 59C-64C now i run 69C-83C so ive stopped playing those games.",
            "Anyone else had trouble with youtube videos freezing while the sound stays on and the whole system hangs (can't even bring up task manager)? I had this issue along with a blue screen and I think I have traced it back to 555.99. I have reverted back to 552.22 and the freezing has disappeared so I am pretty sure the nvidia update caused this.\n\nGTX 1660Ti with HDR and G Sync enabled LG 144hz monitor, WIN10",
            "What are they doing there? Still crushes the TLOU on RTX  3080TI shaders rebuild with VRAM/RAM nonsense. Disaster.",
            "I'm interested in the VLC RTX HDR support, might have it with some SDR movies",
            "4070 here, installed fine and works fine with no noticable performance changes. The instant replay flickering fix is nice. Win11 with i7-12700k",
            "Nvidia Overlay's GPU utilization, GPU power usage, GPU temperature is sometimes stuck at a certain number and is only fixed after restarting my PC.",
            "I was getting stuttering in both destiny 2 and cyberpunk after this update.  Went back to 551.86 and stuttering gone.  Probably went back too far.",
            "Nvidia Container 10% CPU load bug is BACK! :\\\\",
            "Last Driver that isn't shit for my 3060 ti is 537.58.\n\n  \nMaybe one day one will work.\n\n  \nSystem is extremely stable with this driver, some newer games ask me to update on launch(Ghost of Tsushima for example) but is also smooth while playing, so guess its fine...would be nice to have a solid new driver though....",
            "This new 555.99 driver is hot garbage u/nvidia , and unfortunately so was 555.85\n\nThe best performing GPU driver for RTX 4060, 4070, 4080, and 4090 is still 552.44. There are quite a few older demonstration videos comparing 555.85 and 552.44 and the older driver performed significantly better at the average and 1% lows level. To follow up to that, there are videos comparing 555.85 and 555.99 and it shows worse performance (slightly, around 2-3fps) with the new driver versus 555.85. Simply put, 552.44 is still superior to the last 2 driver versions. I rather stay on the 2 month old driver that performs well, then jump to a newer one with worse performance. The demonstration videos compared 7+ games and clearly shows its not a isolated issue with any one particular game (the cards tested were the 4070 Super and the 4090)",
            "I'm not sure if this is related to 555.85 or this one 555.99, but my 300hz monitor can't go beyond 240hz anymore. 300hz is not showing up anymore in Windows or NVCP. Is this a known issue?",
            "stuttering or video skipping frames while playing videos at 60fps is back ...",
            "is anyone experiencing any fps drop with this update?",
            "This driver screwed over my performance in games, it\u2019s laggy and spiking af downgraded to the drivee before this and everything works fine, and nvidia performance overlay is bugged aswell now, gpu utilazation is stuck and everything else on the performance tab. And i get alot less fps in games on this driver than the one before this",
            "Both this version and 555.85 will crash my machine often when waking the displays from sleep.  Tap my keyboard or mouse, I see a brief indication that the displays are waking up, but nothing happens.  The keyboard stops responding (caps lock no longer changes the led), and I have to reset the machine to recover.  On reboot there are a number of failures for the nvidia driver in the log at the time I taped the keyboard to wake the display.\n\nReverting back to 552.44 and the problem is gone.   My main display is a Odyssey G8 set to 240Mhz.",
            "RTX 30 series owner here. Any driver pass 537.58 that won't bring any weird bugs or glitches? I don't game much but do  need the pc for daily tasks",
            "Doesn't work for msfs\u00a0",
            "My buddy uses this driver, has no issues. Played No mans sky and Cyberpunk.  \nI'm on 555.85 it's fine.",
            "Was having a weird issue where the screen would freeze when going into fullscreen applications and videos (youtube/twitch). I had to wait for it to respond again and exit fullscreen and go back into it for it work. It persisted for several days and reboots/shutdowns.\n\nDDU'd and installed 551.86 and it now works as it should. Can't say for sure if it was the driver or general wonkiness that a reinstall fixed, but posting in case anyone with same issues comes in looking.",
            "I'm running a 3060 Razor laptop and have nothing but problems with it. Screen flickers, sometimes when I play games it goes blue screen. \n\nNow I've updated the drivers to 555.99 and my performance has dropped dramatically even without playing games. I've been playing Eldon ring to get ready for the DLC. Is stutters so bad it's unplayable. \n\nI've rolled back to the previous driver and it made it even worse. I'm trying 551.86 now.\n\nPlease let me know anyone is dealing with the same issues with a similar setup.",
            "It's been two weeks... a newer driver could kick in any time!",
            "[GeForce Experience] Flickering or black screen if Instant Replay is enabled [4665009]\n\nThis isn't fixed, I'm still experiencing it and I just upgraded last night.",
            "Any one else is having terrible frame drops  with this new driver?",
            "Updated to 555.99 last night tried to play Halo Infinite and it would just crash to desktop immediately.. Rolled back to 555.85 re launched worked perfectly. wtf.",
            "3tr dollar company, still pumping out one donkey shit driver after another, intentionally bricking old GPUs to force consumers to buy new one. Trash-vidya.",
            "I painstakingly did the dumb thing of updating without reading community notes today. I immediately regret my decision. I was on drivers 551.86 and had performance issues on valhiem, trying to diagnose them I updated, and clearly that was a mistake.\n\n  \nWhat driver do you guys think I should rollback too? I have a RTX 3080.\n\nThank you very much!",
            "Hey, ever since i downloaded this new driver my games just seem to tab themselves out every 10 or so minutes regardless if they're on fullscreen or borderless. Any reason/idea why that might be happening?",
            "Can't install it at all. I click express installation, it says it's downloading and installing, but it doesn't and I can still hit express installation.",
            "https://preview.redd.it/bcv0rwepo35d1.png?width=1920&format=png&auto=webp&s=5114f76e15a36e5405a1dc86b8180314bea078f1\n\nanyone can help me? I cannot Install it",
            "The last few drivers have had an issue with stuttering, at least in DX12 titles I've tested, where \"exclusive fullscreen\" games are randomly stuttery, but swapping to windowed borderless fixes the issue.",
            "I'm **still** stuck on 551.xx on NVIDIA for Lovelace. It looks like this is still the same. I'm so sick of NVIDIA I put my NVIDIA card in my second PC and I'm using my RDNA 3 AMD card in my primary. Web browsing is SO much smoother on AMD it's just sad, and I don't have to change several flags in Chrome to make it work. Not only is NVIDIA trading places with AMD in terms of driver reliability, but NVIDIA broke web browsing nearly 18 months ago and has still refused to fix it. YOU CANNOT browse the web on NVIDIA without changing Chrome flags. Without at least ANGLE set to OpenGL and disabling the \"new scheduler\", good luck browsing the web with NVIDIA. No one on AMD or Intel suffers this problem. What is your problem?",
            "Based on the comments... think I'll stick to my 552.22 that's been treating me nicely/stable atm on my 3070 Laptop.\n\nMaybe when they roll out 556.XX/560.XX and/or beyond. I'll take a look again.",
            "I just had to roll back to 555.85 as 99 was causing crazy artifacts and game crashes the difference is night and day. 3060ti",
            "555.99 made my text really blurry. i tried a few things but ended up only getting fixed by rolling back",
            "running a 3080, never had any issues until this update. all of my games keep losing frames randomly. rolling back to 555.85 UPDATE: rolling back further 555 is buggy in general",
            "Still no fix for Monster Hunter World. Absolutely ridiculous.",
            "Was having issues with unplayability(Left 4 Dead 2 of all things on an rtx 3060 was unplayable due to lag) until i rolled back. I've only had an nvidia card for less than a year after using an r9 390 8gb for years. This scared me as money is really rough and i cant afford another graphics card and i thought i was done for. They really should have tested this more. People are paying alot for videos cards you would think they would test this stuff. Update forgot to mention i rolled back to 555.85",
            "555.99 has been ass",
            "New encounter today wow\n\nPlaying Elden Ring DLC plus video on second monitor then checkerboxes everywhere -> black screen -> crash\n\nRolled back to 552.44 thanks most valuable company in the world!",
            "The last two driver have broken the sleep monitor of my benq, connected dysplay port  :( If lock my Windows (windows+L) the screen turn black with lights  after 1minutes and monitor dont close and my other one connected hdmi is correct.... please fix!",
            "I like this version #.",
            "Crazy how many issues 555.99 caused. Anyone know what the last stable version is and where i can get it/how i can install it?",
            ">DLSS 3.5 support in Womp\n\nHuh??",
            "Guess I'm sticking with 555.85 for the time being",
            "Anyone else getting screen dropouts on this driver? I have a DSC 4K monitor that black screens for 1-3 seconds occasionally and a HDMI 2.1 TV that sometimes drops input altogether on 555.99",
            "Haven't updated in a long while is 537 still the best driver or did they release a new one that is better performance wise?",
            "I refuse to be a beta tester for nvidia drivers",
            "Jesus christ where the hell is the QC on this driver update ? \n\nI thought it was my SSD so upgraded that and swapped out the fucking RAM. Turns out all the stutters i was experiencing were due to these bloody drivers. I reverted back to 552.22 and now everything works perfectly. \n\nWhat an absolute joke",
            "define \"game ready\"",
            "maybe one day the rebar settings wont get reset after every driver update",
            "IDK if it's driver related but since the update today I can't play videos here on Reddit with Firefox. With Chrome works fine. Anyone?",
            "Prolly wrong place to ask this but does anyone find gsync is not working like it should? For me dx12 games are fine completely smooth as they should be with gsync (I use an 80fps lock to ensure smooth frametimes) but in dx11 games there\u2019s a stutter that isn\u2019t frametime spikes. I figured out that my monitors hz changes spontaneously and never sits near 80 in dx11 games with an 80fps lock but in dx12 games it sits very happily around 79-80 as it should, indicating that gsync isn\u2019t matching fps to hz as it should on dx11 games like Jedi fallen order, tales of arise etc. I can\u2019t be the only one noticing this. Tried it over multiple monitors (all gsync compatible like the lg c2 and lg gp860).\n\nBtw set every setting in nvcp correct before I get asked, and I\u2019ve tried this over 3 different gaming laptops (all attached to external monitors with displayport)\n\nIf anyone else noticed anything like this please reply feel like I\u2019m going crazy knowing I\u2019m the only one with this issue \ud83d\ude4f",
            "Ever since I downloaded this driver, even when I manually downloaded and went back to the previous driver, Yuzu and Ryujinx crash every time I launch a game. Something with this driver update messed up Vulcan.",
            "Hey Guys after updating my graphic drivers to latest game ready driver my laptop RTX 3050 is randomly utilized one second is 0% after its going up to 100 and coming down again its giving me quite an issue and i don't know what to do thought might some of you could help me",
            "Anyone know what's up with the GeForce experience app still showing \"Express Installation / Custom Installation\" after you've installed the Driver (Allowing you to re-install it again, and again, and...) ?  \n  \nIt's been a weird issue for the past 2 or so updates.  \n  \nEdit: https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72soqj/",
            "After updating to 555.99 my gpu won't use dynamic boost. It's literally impossible to change the power limit by switching profiles, only fans are going harder the power limit stay s exactly the same. 4090 mobile.",
            "I just got ntosknrel.exe as my first ever genuine BSOD on my new build, just reverted to 551.76 and Siege (bad game I know, did not crash so far)",
            "Gpu temp stuck on 50 degrees in the overlay",
            "4070ti super with a 7700x cpu and this driver update is absolute garbage. I'm noticing microstutters/frame drops in games I never had microstutters prior to these last two updates besides ghost of tsuishima for some reason.\n\nLegitimately frustrated with this.",
            "My notebook with GTX 1050 goes black screen after update, even from geforce experience or manual driver install\n\n  \nThere's a way to fix or i should keep my old driver installed, by the way my actual driver version is 457",
            "Halo Infinite is crashing at startup now",
            "New update seems to be making my 3060 fans spin more. Went from a pretty quit pc to know audibly hearing the gpu fans spin",
            "I have logging screen black issue, need restart once. But gaming performance same.",
            "I've been getting \"bad_pool_caller\" blue screens on these last 2 drivers. I also just replaced gpus so it's hard to tell what's going on. I went back to the last \"552\" release and that's cleared it up so far.",
            "After 555.85  when i install this driver or newer\nMy pc will crash instantly every time i get to windows \nOn 552.44 it is fine with no crashes \nDid they add any thing require to have newer cpu than core 2quad ?\nGpu is gtx 1650",
            "Performance in my games will gradually get worse over time, and the only short-term fix that works I can find is to reinstall the driver. This only started happening after updating these past 2 drivers.",
            "I'm still using 552.12 for games like CS2 and Elden Ring and it runs perfectly smooth without any stuttering or issues. Using a 13600k+4070ti",
            "bruh i am suffering from many lags and freeze in valorant idk whats the problem i have 1060 6gb can anyone suggest me best driver for this",
            "help, I'm having terrible frame drops with this new driver, I tried uninstalling the driver with DDU and rollback to an older version but nothing to work",
            "I updated to the latest drivers, and now every time I alt-tab, the game freezes? Never had this before... I guess I should roll back?",
            "cada ves que sacan nuevos drivers me da miedo probarlos en mi pc...",
            "How do I reroll to the previous driver ? I got no audio output through my HDMI anymore after this update and it's very annoying todeal with...",
            "This driver seems so pointless because I'm playing the Elden Ring DLC on driver 537.58 and the game has been smooth with no crashes. And when I look for benchmarks on YT with the same GPU as me on the latest driver, I don't see any noticeable frame rate improvement.",
            "RTX 4090 here, Driver works without any issues and stable. Win11 & 5800X3D",
            "something very wrong with this driver, D4 is unplayable\n\nthe game stutters during loading and will infinite load when I go from waypoint to another waypoint\n\nthe 3D spinny thing (from the bottom right corner) changes color if you let it alone long enough: https://i.imgur.com/cRa3Z5b.png",
            "Strange that with dlss for me (using a fsr3 mod) 552.22 gives me much better performance than 537.58 and 551.86 on a few games. Does anybody know why?\n\nI feel tempted to try this newer driver.....\n\nEdit: Just to give an example in icarus I get dips to say 27 but with 552.22 it is always above 30 mostly dipping to around 34. I have seen the same with cyberpunk even though the latter is at a much higher fps than icarus.",
            "Does anyone know if this fixes the stuttering issues that was reported on 555.85?",
            "RTX 4080 here, I am using famous 537.58, should I update to this one? Or I\u2019m stuck forever in the past?",
            "This driver causes almost immediate crashes while playing warframe.",
            "For windows 11 and a 3090, what's the most stable driver currently? Using 537.58 ATM",
            "Does this fix the last of us part 1 telling you it can't run because you don't have enough vram? It was an issue with last driver but not mentioned here.\n\nEdit and before anyone says this is because you don't have enough vram how does a 4090 not have enough vram?",
            "Calling out the Elden Ring DLC makes me hopeful they've implimented DLSS",
            "oh man this driver has a lot of problems. rolled back. big performance loss. crashing, stutter.\n\nwtf?",
            "Anyone else have this driver update still showing as available in GeForce Experience even after installing it?",
            "I have troubles with 555.99. So my fps getting lower, geforce experience interface is broken (showing me wrong stats with \"Alt + R\"  and filters not working as well). I have RTX 3050 laptop. Anyone know how to fix that? Or I just gonna set back 552.44?",
            "Another driver update .. another disaster\nBlockikg from update until they fix the crashes in the last of us .. \nWhy?? I mean .. the Game was perfectly playable before .. so the issue is on Nvidia",
            "Halo Infinite crashes during shader compilation on my system with this driver, cleaned installed of course. The last of us no problem during compilation.",
            "555.99 crashes halo infinite on start up loading screen",
            "did anyone else's idle temp drop by a lot? i was idling at 52c and after the update im down to 38c. What's that about?",
            "When i install the Driver trough the GeForce Experience app it says after the installation is complete that i still have the old driver version.\n\n  \nIt will finish installing but then saying its ready for installation again. Anyone with the same problem?",
            "I updated to this version and it has completely destroyed my Lg ultragear 49in 240hz capability. I can get the display to not have lines of pixels when i change the display refresh rate, or manually change the connection to 1.4 instead of 1.4(DSC) making it locked at 120hz. I have tried multiple DP cables and even bought new Vesa cert 16k to test and still unable to use 5120x1440p 240hz to full potential on 4090.",
            "Another great driver, no issues with my 4070 on Intel Z590. Nice to see Windows eliminate the Chromium checkerboard flicker recently, and I suspect RTX HDR for multimonitor will arrive soon enough.",
            "Not to big of a deal but this driver absolutely brought back some of the stutter issues again. Especially in games like acc for us sim racers. It was buttery smooth before it. A bit of a bummer luckily my 4070 super is a champ and def helping compared to the 4060 I was using so that's a positive.\n\nLuckily still enjoyable not horrific but it's noticeable when comparing old vs new.",
            "Insane fps drops and gpu time in the menu of COD since upgrading to 555.99.",
            "I updated my drivers to this version, now my games are stuttering from 150 fps to 40. PLEASE HELP!!. I tried reinstalling all the drivers and even tried to install the old versions of drivers, but still doesnt work.",
            "https://preview.redd.it/jiwmjgcas66d1.jpeg?width=4032&format=pjpg&auto=webp&s=5674fe7e872fd233257c6df12e8c2e0bc58de816\n\nThis is a first also.",
            "Went from running ultra presets on Cyberpunk at 80-110 FPS to 6-25 at high preset! Shit!",
            "I don't think this driver is stable, sometimes its stuck on \"compiling shaders\" but MOSTLY its CRASHING games. Especially Ghost of Tsushima \n\n(Windows 11)",
            "Guys, NVIDIA is killing gpus to make them look old and force you to buy new 50 series. That is same with Apple. The strategy is downgrade older and sell new.",
            "Can\u2019t play Cyberpunk 2077 for more than 15 minutes now without crashing",
            "I recently just got issues recording with shadowplay with modern warfare 3, it stops itself right after i start recording manually i can record other games like cyberpunk 2077 and desktop just fine, is shadowplay being broken in some games something that is a known issue guys?",
            "Don't know if this was just a fluck or if its happened to anybody else, but my update didn't actually update until I restarted. Normally I don't have to it just goes until done and it's good. Today though it kept saying the update was ready/finished until I finally just rebooted.\n\n  \nWeird, but something I thought people should look out for just in case.",
            "So should I download it? I just got a nividia driver so I\u2019m new to this. I just updated drives a day ago and this is the new one. Should I get it?",
            "I cant see DLSS option in warzone after the update there's only nvidia image scaling??\n\nAnyone had this issue?",
            "Says there is a problem with my graphics card (4080). First time this has ever happened to me for a driver update. Just the driver having issues?",
            "Had to un install to the May 21st update. The June one was crashing all my games.",
            "Dragons dogma 1 (Dark Arisen)\n\nThis game didnt launch at all with the previous [update.Is](http://update.Is) this fixed in this update or will i need to keep on using a older driver still?\n\nI have the 2 previous driver back so the one previous to the previous or how i should put it\n\ndriver 552.44 is the last known driver to work with this game!\n\n  \nDRAGONS DOGMA DARK ARISEN...\n\n  \nDO ANYBODY KNOW IF THIS UPDATE WORKS W THIS GAME(PREVIOUS UPDATE DIDNT WORK AT ALL)\n\n  \nTHANKS 4 ALL INFO ABOUT THIS(BECAUSE I CANT BACK MY UPDATES ITS WAY HARDER TO UNINSTAL N INSTAL A OLDER DRIVER,I ACTUALLY DONT KNOW WHY I CANT BACK TO PREVIOUS DRIVER)\n\n  \ni USE THE GEFORCE EXPERIENCE FOR MY UPDATES(BUT I CANT BACK DRIVERS IF IT DONT WORK)",
            "Fixed my flicker issue, prevelent in MWIII and other game menus only... had to change refresh of my 2nd monitor to 120hz from 144hz, main is 144hz.\n\nDoesn't happen with 551.x drivers, not a big issue using 120hz but a bit odd.",
            ">\\[GeForce Experience\\] Driver page may not detect new driver has been installed until user reboots PC \\[4662117\\]\n\nI thought something is wrong with my pc",
            "I am not able to install this driver with nvcleanstall it throws an exception i never heard of but i can install 555.85 using nvcleanstall!? Is it just this driver?",
            "anyone have a solution so that my geforce now captures my gameplay on old driver? this and the last driver is giving me a lot of perfomrace issues and stutters, for example when i alt tab between games my entire right monitor goes dark.",
            "555.99 somehow broke my mouse input kn XDefiant. Installed the one version older drivers and it works now",
            "what about fixing the fact that overlays break vrr completely? (steam , epic, obs)",
            "had to roll back because nvidia high definition audio just disappeared for me after the update. 1050ti laptop",
            "my 4070 has micro stutter as of this driver update. MSI ventus.",
            "I'm experience constant crashes when I try to open Warframe and frame drops in other games like War Thunder, Helldivers 2 and even Team Fortress 2 (I have a 4070 in case you're wondering)\n\nEdit: I was also experiencing these problems on 555.85 as well and I've had to rollback to an older version",
            "I get blue screens, load crashes from games and in games crahes after playing a while...",
            "I have experienced a random freeze and black screen for 20 seconds under no load (I was checking idle temps using the NVidia Overlay). The drivers crashed and took about that time to restore them (with errors with ID 153 and 14, no better explained in the Event Log). To be completely safe I have disabled XMP, running stock speeds, voltage and timings. I have also updated BIOS. I will see how It goes. Otherwise had no issues until now. Stessing system with OCCT in different modes has brought no issue (No overclocks applied outside of XMP).\n\nGPU: RTX 4060",
            "So this started happenign out of nowhere. Interface comes on and stuck. I can not exit it using any means during game play. I try to use ALt+Z and it doesn't work.   \nI went to the Geforce experience trying to check what is going on? I can access nothing. Tried scanning for games. Scan failed. and in highlights, my mic is not recording. Updatd the driver and it worked well but today it is back. Running a Lenovo Legion with 4060ti.\n\nPlease help\n\nhttps://preview.redd.it/c78aue2h3a7d1.png?width=1213&format=png&auto=webp&s=41148b44394a9942d75ea4d53f4831ebec7fefe3",
            "Constant frametime spikes 555.99\nhttps://youtu.be/7gttxw3CdOo?si=vM3dFajQFb5errVq",
            "Not an expert on nvidia specifics, but do production so know what you\u2019re saying\u2026 last update before this 555 one was the first time an audio driver update happened in a looong time as well\u2026 so that may be involved.  Or it could be specific to the 555, but I thought to mention this to you",
            "Haven\u2019t tried rolling back, but I\u2019ve seen similar issues with the Elgato Nvidia Broadcast VST plugin for their wave link software. It sounds really bad and I get tons of crackling, popping, cutouts, and the noise suppression as a whole doesn\u2019t work very well. I also tried a third party VST for this as well (still making use of the RTX noise removal) and saw the same issues.\n\nSeems specific to the VST plugins, as using the dedicated Nvidia Broadcast app works great.",
            "I was just noticing this too! couldn't figure out what caused it but you're right - must be this drive. I'm on Ableton Live 12 but it's particularly noticeable with Fabfilter and NI stuff",
            "**EDIT:** confirmed by Nvidia and issued bugtrackID [4685335]\n\n> [Halo Infinite] Game crashes during the initial loading screen after updating to GRD 555.99 [4685335]\n\n> *We are still investigating this issue but I have added the crash to our Open Issues list for now. For now, you will need to roll back to driver 555.85 or earlier driver to play Halo Infinite*\n\n---\n\nYes, added to unofficial tracking comment at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> [Halo Infinite] crash to desktop (CTD) during loading screen [?shader compilation?]\n\nIt's being investigated by Nvidia and will be added to [confirmed open issues](https://www.nvidia.com/en-us/geforce/forums/game-ready-drivers/13/543951/geforce-grd-55599-feedback-thread-released-6424/3457028/) -",
            "Seems like the same thing is happening on the Xbox app as well for me.",
            "Same here! I just rolled back to the 5/13 driver (32.0.15.5585) and it boots up fine now",
            "Same here, RTX 4080. Win11 with drivers cleaned installed.",
            "it's not just the Steam version it's Gamepass version as well.",
            "Just on one game? I play older games, so for me, it doesn't matter if the new driver will function or not, since 2D will always be 2D in my book.",
            "I can confirm, try everything and nothing worked. Rolled back to 555.85 game works perfectly",
            "I can confirm, had to rollback.",
            "It\u2019s wild this isn\u2019t fixed yet. No hotfix driver for this?",
            "I\u2019m having this issue but with destiny 2, I could play for hours and hours on max settings getting really high stable fps but ever since the final shape dlc and this graphics driver update I haven\u2019t been able to play with out constant freezing and or crashing to desktop with no error code, I\u2019ve tried everyone\u2019s supposed fix except for rolling back to a stable driver",
            "two days ago there was an article announcing the updates to nvidia app. it claims they're working on it at least.\n\nhttps://www.nvidia.com/en-us/geforce/news/nvidia-app-beta-update-av1-performance-tuning/\n\n> Next Steps\n> \n> Your feedback matters; thank you for your continued support. We plan to continue adding the remaining NVIDIA Control Panel options in future updates, which encompass Display and Video settings, and will also introduce new features, including DLSS controls. **We\u2019re also working on the much-requested multi-monitor support for RTX HDR.**",
            "It's funny because there was that guy that released an app that enabled RTX HDR before it was announced for games (it was originally announced just for videos), and that enabled the HDR to work with multiple monitors.\n\n[Link to NvTrueHDR](https://www.nexusmods.com/site/mods/781) for anyone who wants to check it out.",
            "Another driver release of disappointment...",
            "still not working.   \n@ u/pidge2k do you happen to know if this feature will be rolled out in parallel to multiple HDR monitors AND all of us stuck with just one HDR monitor and a couple SDR monitors connected? the latter is working fine when its forced through profile inspector",
            "[https://www.nvidia.com/en-us/geforce/news/nvidia-app-beta-update-av1-performance-tuning/](https://www.nvidia.com/en-us/geforce/news/nvidia-app-beta-update-av1-performance-tuning/)\n\nThey have mentioned as next steps in the article.",
            "Yep and they still didnt acknowledge the issue.",
            "Here's hoping they enable the use of DLDSR once it gets fixed. My biggest gripe with my samsung monitor is that I can't use DLDSR because DSC can't be disabled.",
            "I had a good number of issues with the Samsung desktop monitors that I owned. I moved to this LG and its pretty much flawless. I'm not surprised to hear this. One thing you could try for DSC is using HDMI. Mine is DP 1.4a like most, but 48Gbps HDMI 2.1 which is enough to disable DSC for most resolutions/refresh rates. No random blanking and my alt-tab in fullscreen mode is instant.",
            "With the recent drivers, what are these issues that Samsung monitors(using DSC) are facing exactly? I never noticed any issues with DSC on my Samsung monitor(Neo G9).",
            "Awesome that the bug won't concern me, but that's because I'm still running my G9 from a GTX 1070 \ud83e\udd72  \n  \nDSC became available in the supported Displayport versions maybe in 2000-series? (And one would think it's all so standardised that if DP works in manufacturer's monitors, DSC would too.)",
            "Is it the same with LG monitors, anyone knows?",
            "Windows users have it good. Linux is a mess.",
            "Because it's too complicated of a process. I want to open up a tool like with AMD, WITHOUT LOGGING IN, and submit all my bug reports, which are becoming endless. They cannot be getting complaints because most people don't waste time with their process. No, AMD gets their complaints box filled because people can actually fill them. \n\n  \nNVIDIA is not cooking, they are slipping and pouring the grease on their face like that creepy/freaky Canadian workplace safety commercial (There's been an accident! \"There are no accidents\") from 15+ years ago.",
            "Running latest drivers, zero issues so far.",
            "[deleted]",
            "what better for 4080  555 85 or 552 44 ?",
            "Yes I also noticed slightly less fps so I reverted but i'm using 552.22 it's probably identical to 552.44",
            "I doubt it. 551 is the last branch that seems to be working for everyone. 552 and 555 have endless issues.",
            "whats better for a laptop 3050 6gb? ignore the deleted, it sent multiple times randomly",
            "Same this is the first driver that gave me serious issuss. Crashing, stuttering lagging and micro stutters. Nvidia has had issues all year with this it should be checked 100% before every driver release. Ridiculous but luckily my 4070 super compared to my 4060 runs on wirh only minor micro stutters.\n\nI may do what you did or just deal with it until I get a good latest driver they release. I can tolerate it for now. At least I know its nothing else because I've tried everything and I'm not wasting anymore time messing with it. I'm just going to get on game and enjiy and ignore the issues knowing I'm not the only one.",
            "Why not using amazing 537.58?",
            "It has been this way for a long time they can't fix much it is incredible some issues are still there years later. I think team red suffers from worse drivers or used to until recently, i'm not sure.",
            "Can confirm, added to the unofficial tracking comment 'unconfirmed by Nvidia' section at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/",
            "Which Windows 11 version are you on?  23H2 or 24H2?",
            "Glad I came to the comments to see this first before installing!",
            "Even on my 4060 laptop, I am rolling back to 537.58. None of the newer drivers have been stable in games or normal usage. Quite shocked at the quality of Nvidia drivers in the past 9 months I have owned an Nvidia GPU.",
            "Assuming MSI Afterburner is still installed, can you confirm that 'Power Limit' was at 100% for the 555.99 benchmark?\n\nhttp://nvidia.custhelp.com/rnt/rnw/img/enduser/aid_5408_02.png\n\nSee https://nvidia.custhelp.com/app/answers/detail/a_id/5408 for reasons/guide",
            "Detailed breakdown of driver branch information grouped by release/mainline branch\n\n---\n\n**560 Release/Mainline**\n\n> Support for DirectSR (Super Resolution)\n\n> Agility SDK 1.714.0 support\n\n* 560.38: r560_bugfix_main (Insider/Developer)\n\n* 560.36: r560_bugfix_main (????)\n\n---\n\n**555 Release/Mainline**\n\n> Support for OpenGL GL_NV_gpu_program_multiview\n\n> WDDM 3.2 support\n\n> Shader Model 6.8 support\n\n> CUDA 12.5 is supported in 555.85 and above\n\n> Undocumented shader compiler backend optimisations\n\n* 555.99: r555_97-02 (Game Ready/Quadro/Studio)\n\n* 555.85: r555_79-02 (Game Ready/Studio)\n\n* 555.41: r555_bugfix_main (Insider)\n\n---\n\n**550 Release/Mainline**\n\n>  RTX Video HDR available on all RTX GPUS in 551.23 and above\n\n> CUDA 12.4 is supported in 551.23 and above\n\n> NVIDIA Ultra Low Latency Mode is supported with DirectX 12 titles in 551.23 and above\n\n> Microsoft Windows 11 WDDM 3 bug is mitigated in 551.52 and above\n\n* 552.61: VK551_06-15 (Developer)\n\n* 552.44: r552_19-08 (Game Ready)\n\n* 552.31: VK551_06-11 (Developer)\n\n* 552.22: r552_19-02 (Game Ready/Studio)\n\n* 552.12: r552_05-04 (Game Ready)\n\n* 552.04: VK551_06-9 (Developer)\n\n* 551.86: r550_00-192 (Game Ready/Studio)\n\n* 551.81: VK551_06-6 (Developer)\n\n* 551:78: r551_40-15 (Tesla DCD)\n\n* 551.76: r551_71-2 (Game Ready)\n\n* 551.70: VK551_06-4 (Developer)\n\n* 551.68: r551_40-14 (Hotfix)\n\n* 551.61: r551_40-11 (Game Ready/Quadro/Studio)\n\n* 551.52: r551_06-24 (Game Ready/Quadro)\n\n* 551.46: r551_06-21 (Hotfix) includes support for 4070 SUPER, 4070Ti SUPER and 4080 SUPER\n\n* 551.34: r550_00-133 (OEM HP)\n\n* 551.32: r551_06-17 (3050 6GB Only)\n\n* 551.31: r551_06-16 (4080 Super Only)\n\n* 551.23: r551_06-14  (Game Ready/Quadro/Studio)\n\n* 551.15: r550 (4070Ti Super Review Driver)\n\n* 550.09: r550_bugfix_main (Insider)\n\n---\n\n**545 Release/Mainline**\n\n> WDDM 3.0 (Windows 11) feature 'Hardware Flip Queues' support added in 545.84\n\n> RTX Video Super Resolution available on all RTX GPUS in 545.84 and above\n\n> DirectX 12 Agility SDK 1.711.3 Preview appears to be supported in 545.84 and above\n\n> Microsoft Windows 11 WDDM 3 bug exposed by this mainline\n\n* 546.65: r546_33-9 (Game Ready)\n\n* 546.52: r545_00- (4070 Super Review Driver)\n\n* 546.46: r545_00-175 (OEM)\n\n* 546.34: r545_00-158 (RTX 4090 D)\n\n* 546.33: r545_00-157 (Game Ready/Studio)\n\n* 546.31: r546_21-5 (Hotfix)\n\n* 546.29: r546_21-4 (Game Ready)\n\n* 546.17: r545_96-8 (Game Ready)\n\n* 546.08: r545_96-5 (Hotfix)\n\n* 546.01: r545_96-2 (Game Ready/Studio)\n\n* 545.92: r545_74-10 (Game Ready)\n\n* 545.84: r545_74-7 (Game Ready)\n\n* 545.37: r545_bugfix_main (Insider)\n\n---\n\n**535 Release/Mainline**\n\n> DLSS 3.5 official support in 537.42 and later\n\n> GPU System Processor (GSP) support files in 535.98 and later\n\n> Undocumented improvements to shader compiler backend\n\n* 538.67: r???_??-???? (Quadro)\n\n* 538.62: r535_00-549 (Quadro)\n\n* 538.42: VK535_87-29 (Developer)\n\n* 538.37: VK535_87-26 (Developer) ^(**Incorrectly** states Series 40 Super support - https://i.imgur.com/cVnaUDL.png)\n\n* 538.31: VK535_87-24 (Developer)\n\n* 538.09: VK535_87-22 (Developer)\n\n* 537.99: r537_94-2 (Quadro)\n\n* 537.96: VK535_87-20 (Developer)\n\n* 537.80: VK535_87-16 (Developer)\n\n* 537.72: VK535_87-15 (Developer)\n\n* 537.63: VK535_87-13 (Developer)\n\n* 537.58: r537_56-2 (Game Ready/Studio)\n\n* 537.42: r537_41-1 (Game Ready/Quadro/Studio)\n\n* 537.34: r537_13-6 (Game Ready)\n\n* 537.13: r535_00-291 (Game Ready)\n\n* 536.99: r536_92-8 (Game Ready/Studio)\n\n* 536.67: r536_62-3 (Game Ready/Studio)\n\n* 536.40: r536_08-15 (Game Ready/Studio)\n\n* 536.23: r536_08-8 (Game Ready)\n\n* 535.98: r535_87-6 (Game Ready/Studio)\n\n---\n\n**530 Release/Mainline**\n\n* 532.03: r531_79-4 (Game Ready/Studio)\n\n* 531.79: r530_00-178 (Game Ready)\n\n* 531.68: r531_66-2 (Game Ready)\n\n* 531.61: r530_00-155 (Game Ready/Studio)\n\n* 531.41: r531_37-2 (Game Ready/Quadro/Studio)\n\n---\n\n**525 Release/Mainline**\n\n* 532.34: VK526_25-43 (Developer)\n\n* 528.49: r528_37-5 (Game Ready/Studio)\n\n* 528.34: VK526_25-11 (Developer)\n\n* 528.33: r528_10-10 (CUDA)\n\n* 528.24: r528_10-7 (Game Ready/Quadro/Studio)\n\n* 528.02: r527_92-2 (Game Ready/Studio)\n\n* 527.56: r527_32-11 (Game Ready/Studio)\n\n* 527.37: r527_32-4 (Game Ready/Studio)\n\n* 526.98: r526_91-3 (Game Ready)\n\n* 526.86: r526_25-17 (Game Ready)\n\n* 526.47: r526_25-3 (Game Ready)\n\n---\n\n**520 Release/Mainline**\n\n* 522.33: r521_82-7 (Developer)\n\n* 522.30: r521_90-18 (Studio)\n\n* 522.25: r521_90-15 (Game Ready/Studio)\n\n* 522.06: r521_82-4 (Developer)\n\n---\n\n**515 Release/Mainline**\n\n* 517.48: r517_40-6 (Game Ready)\n\n* 517.40: r515_00-323 (Studio)\n\n* 516.94: r516_87-3 (Game Ready/Studio)\n\n* 516.93: r516_56-8 (Studio)\n\n* 516.79: r516_56-6 (Hotfix)\n\n* 516.59: r515_00-212 (Game Ready)\n\n* 516.40: r516_10-14 (Game Ready)\n\n---\n\n**510 Release/Mainline**\n\n* 512.96: r512_72-6 (Studio)\n\n* 512.95: r512_48-11 (Game Ready)\n\n* 511.79: r511_75-3 (Game Ready)\n\n---\n\nNOTE: this comment is separate from the [555.99 unofficial tracking comment](https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/) due to comment character limits.",
            "DPC Latency Spikes\n\n---\n\nIf drivers 536.67 and later haven't resolved the issue try one or more of the following potential workaround(s): \n\n* Enable Message Signaled Interrupts (MSI / MSI-X) for the Nvidia HDMI Audio and Nvidia GPU instances using [MSI Utility v3](https://forums.guru3d.com/threads/windows-line-based-vs-message-signaled-based-interrupts-msi-tool.378044/) i.e. those reported with 'Instance ID' starting *PCI\\VEN_10DE*. Don't change interrupt priorities, leave then as 'undefined' and just tick MSI mode setting (reboot computer after change)\n\n* Disable both [*'PEG - ASPM'* / *'PCI Express Clock Gating'* (credit Astyanax)](https://forums.guru3d.com/threads/nvidia-geforce-531-61-whql-driver-download-and-discussion.447454/page-7#post-6123944) if supported in the motherboard BIOS\n\n* Disable Hardware Accelerated GPU Scheduling (HAGS) in Windows settings. May help with DPC Latency on some systems but impacts Series 40 (Lovelace) DLLS3 Frame Generation feature and AMD's FSR3 Frame Generation feature\n\n* Set Nvidia's 'Power Management Mode' to 'Prefer Maximum Performance' in the Nvidia Control Panel for each application/game impacted (not globally)\n\n* Force higher VRAM clocks for idle GPU States (P8/P5) using a tool such as the official [Nvidia System Management Interface (SMI)](https://developer.nvidia.com/nvidia-system-management-interface) tool OR the unofficial [Nvidia Power Management Tool](https://forums.guru3d.com/threads/improve-dpc-latency-spikes-for-ada-lovelace-based-gpus-in-r536-67-driver.448143/)\n\n* Disable 'Interrupt Moderation' in Ethernet / Network Interface Card (NIC) settings, when enabled this can negatively contribute to system DPC latency as well as network packet latency\n\n* ASMedia USB 3.0 Controller driver. In Device Manager replace ASMedia driver with Microsoft's 'xHCI Host Controller USB driver', after install they will show as 'ASMedia USB 3.0 eXtensible Host Controller (Microsoft)'\n\n* Set Windows 10 CPU Power Plan to *at least* 'Balanced Performance'. For Windows 11 systems with Intel Hybrid CPUs ([**E-Cores and P-Cores**](https://www.intel.com/content/dam/develop/external/us/en/documents-tps/348851-optimizing-x86-hybrid-cpus.pdf\n)) set power settings in *Control Panel* to ''Balanced Performance' and *Windows >  Settings > System > Power [& Battery] > Power Mode* to 'Best Performance'\n\n* Disable CPU's Core parking in both Windows 10 and Windows 11 unless system is using 7900X3D or 7950X3D \n\n* Microsoft's older official ['Interrupt Affinity Tool']( https://www.techpowerup.com/download/microsoft-interrupt-affinity-tool/) or the more modern and Windows 11 compatible ['GoInterruptPolicy'](https://github.com/spddl/GoInterruptPolicy/releases) can be used as a last resort to bind/partition/force Nvidia GPU interrupts to specific CPU core(s) so that Nvidia GPU's DPCs and ISRs are serviced by the assigned CPU core(s)\n\n* Nvidia HDMI 2.x capable GPUs connected via HDMI to displays set to 10bit/12bit depth and/or 4K 100Hz+ may experience high DPC latency >1000ms. Try DisplayPort connection OR reduce display depth to 8bit and/or 60Hz ([credit Janos666](https://forums.guru3d.com/threads/nvidia-geforce-game-ready-536-67-whql-download-discussion.448706/page-23#post-6157467) with multiple confirmation reports) **NOTE:** officially acknowledged by Nvidia as bugtrack ID [4253581] and fixed in driver 537.58 and above\n\nIf the issue isn't resolved after troubleshooting, please submit a report to Nvidia using the [official form](https://forms.gle/kJ9Bqcaicvjb82SdA)\n\n---\n\nNOTE: this comment is separate from the [555.99 unofficial tracking comment](https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/) due to comment character limits.",
            "ReBAR Notes and Observations:\n\n---\n\n551.23 or later drivers omit ReBAR flags *Options* and *Size* in some ReBAR game profiles. Nvidia have streamlined ReBAR profiles to only include changes to defaults e.g. \n\n* 'Cyberpunk 2077' only has ReBAR *Feature* flag meaning driver defaults used for *Options* and *Size*\n\n* 'Red Dead Redemption 2' has ReBAR flags *Feature*, *Options* and *Size*, no defaults are used\n\n* 'Deathloop' has ReBAR flags *Feature* and *Size* meaning driver default used for *Options*\n\nA new flag *0x00e942f***c** disables ReBAR for Intel CPU based platforms rather than using the *Options* flag. ReBAR is disabled in a number of game profiles for Intel CPU based platforms due to a [ReBAR issue with some Intel motherboard chipsets](https://twitter.com/CapFrameX/status/1618670022951567360) that results in detrimental performance.\n\nFlag *0x00e942f***e**(DirectX) is still present as a [workaround for laptop ReBAR issues](https://www.reddit.com/r/nvidia/comments/17v3i31/game_ready_driver_54617_faqdiscussion/kb8le6q/) in some ReBAR profiles. Hotfix driver 551.46 added a Vulkan specific flag *0x20feaf0d* as a [workaround for laptop ReBAR issues](https://www.reddit.com/r/nvidia/comments/17v3i31/game_ready_driver_54617_faqdiscussion/kpgm33x/)\n\nThe original ReBAR flags *Feature* and *Size* can still be used to enable/adjust ReBAR for unsupported titles. Leave *Options* at default for now.\n\n---\n\nA complete list of all ReBAR enabled applications and games follows, additions for this driver (if any) are in **bold**: \n\n* Assassin's Creed Mirage\n* Assassin's Creed Valhalla\n* Battlefield V\n* Borderlands 3\n* Control\n* Company of Heroes 3\n* Cyberpunk 2077 ^(Laptop workaround present for DirectX)\n* Dead Space (Remake)\n* Deathloop\n* Diablo 4\n* DiRT 5\n* Dying Light 2 Stay Human\n* F1 2020 ^(DX12)\n* F1 2021 ^(Not enabled for Intel CPU Platforms)\n* F1 2022 ^(Not enabled for Intel CPU Platforms)\n* F1 23 \n* Forza Horizon 4\n* Forza Horizon 5\n* Gears of War 5\n* Ghost of Tsushima: Directors Cut\n* Godfall\n* Halo Infinite\n* Hitman 2\n* Hitman 3 ^(Not enabled for Intel CPU Platforms)\n* Horizon Forbidden West Complete Edition ^(Laptop workaround present for DirectX)\n* Horizon Zero Dawn Complete Edition ^(Not enabled for Intel CPU Platforms)\n* Lords of the Fallen (2023)\n* Metro Exodus including Enhanced Edition ^(Laptop workaround present for DirectX)\n* Red Dead Redemption 2 ^(Laptop workaround present for DirectX and Vulkan in 551.46)\n* Returnal\n* Senua's Saga: Hellblade II\n* Starfield ^(Laptop workaround present for DirectX)\n* STAR WARS Jedi: Survivor\n* The Finals\n* The Riftbreaker\n* Watch Dogs: Legion\n* Witchfire\n\n---\n\nNOTE: this comment is separate from the [555.99 unofficial tracking comment](https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/) due to comment character limits.",
            "Thanks as always! u/m_w_h no problems so far with this release!",
            "There seems to be a hidden change in how Vulkan interop works in this driver. https://www.reddit.com/r/nvidia/s/xr74Bri7P6",
            "please keep doing this it\u2019s very informative",
            "> Indiana Jones and The Great Circle\n\nWe don't know when it releases, but is it common for games to receive profiles far in advance of their release?",
            "Having micro stutters in aasetro corsa competizione. Don't forget about us sim racers lol. Seems to have issues with high action high speed games.",
            "Hey, how would I roll back? I'm having issues and when I go to the driver properties the rollback is grayed out.",
            "Im still on 536.23 and i always need to roll back when updating.",
            "FOR THE LOVE OF GOD NVIDIA... FIX YOUR DRIVERS IM STILL ON 537.58!  SO FKN ANNOYING !!!!!!!!",
            "Nope. Still crap. I have random desktop stutters with this one with 4090.",
            "Damn 4080 super can\u2019t even use 537.58 smh",
            "yeah seems so",
            "[removed]",
            "What's up with this driver? Why is it your best? What's your gpu? I'm trying to find more info cause ever since I upgraded to 555.99 I've noticed that my fps on valorant has been randomly falling. I'm on a 3080 12GB .",
            "Yup, having the same issue. It wont install after restart",
            "Solution, just download it from their drivers website.",
            "I'm having the exact same issue, it's saying it installed but it doesn't actually work...",
            "I have the same issue",
            "Aaah - I was wondering what was up with that. It says it's installed, but allows you to re-install it.  \n  \ndxdiag even shows that you have it installed, but it will still allow you to install it.",
            "What gpu are you on and is this a driver that has no issues for you?",
            "Yes. Wipe with DDU and install an older driver.",
            "wich old driver did u use for ur 3060 ti?",
            "Same here. Also happening on 555.85. It's so annoying",
            "I have been having the same issue, I thought it was my ram or other components... I need to try this",
            "Increased CPU load, is the BETA Nvidia APP installed?\n\nDoes the issue occur with driver 552.22 or 551.86?",
            "Samae, 3080ti. get random black screen now and have to manually hit the keys to reset drivers and it comes back. Its ticking me off. sometimes games or apps will minimize on their own",
            "people have had a shitton of issues with this new driver so its definitely not just you",
            "been having the exact same issues, I just reverted back to 555.85 and so far so good.  Fingers crossed",
            "Update in the unofficial tracking comment at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> The Last of Us Part 1 out of memory crash during shader compilation: *[Manuel@NVIDIA](https://www.nvidia.com/en-us/geforce/forums/game-ready-drivers/13/543951/geforce-grd-55599-feedback-thread-released-6424/3456871/) \"This is unlikely a driver issue but we are still investigating this. If after further investigation, it does point to our driver, I'll add it back\"*",
            "There are some video players that already support RTX HDR if it doesn't need to be VLC for you:\n\nMPC-HC (from K-Lite codec pack)\n\nPotPlayer",
            "Isn\u2019t that because of GeForce experience?",
            "UPDATE: So, i have uninstalled the new NVIDIA App and installed the latest version of GeForce Experience (3.28.0.417) the issue is gone for now, but i will keep monitoring.",
            "https://preview.redd.it/fm4ags6vvc7d1.png?width=863&format=png&auto=webp&s=15f2f2c96ae3082f39e6fb047ea565ad8c7202ef",
            "what about your monitor's osd?",
            "What if you set a custom resolution with 300hz in CRU utility?",
            "which browser?",
            "yup",
            "lost a lot of fps on cod . ROLLING BACK RIGHT NOW",
            "Yeah, its not your SSD or GPU, its the driver. This driver is absolute trash. I reverted back to 552.22, works just fine now 0 micro stutters",
            "Performance Monitoring issue is officially acknowledged by Nvidia as issue [4679970]\n> GeForce Experience Performance Monitoring overlay may stop refreshing GPU information [4679970]",
            "Same. So fucking annoying. Thanks for the tip. Going back to 552.44 as well.\n\nUpdate: It's 552.22 not .44 but yeah, it worked. My computer comes back from sleep now. \nFor reference I'm running Win 11 with a 2080 Super.",
            "how do i change the version back to the one before 555.85? Fortnite has been running like shit ever since i updated to 555.99 and i wanna get stable framerates again",
            "Yep, Neo G8 here, same exact issue when in 240hz mode. 120hz mode, doesn't crash.\n\nAlso lucky enough to have a 13900k, thought it could be a stability issue there lol. Good times.\n\nSaw some people mention issues with Samsung, so ended up doing the safe mode DDU uninstall, then rolled back to 552.22 as well (reset all settings), and haven't crashed since. Bizarre stuff.",
            "May be of interest, there's a list of drivers considered by the community as generally stable/consistent in the 'troubleshooting and workarounds' section of the unofficial tracking comments e.g. latest at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> * Drivers 512.95, 516.94, 517.48, 522.25, 526.86, 528.49, 532.34, **537.58**, **551.86** are generally considered stable/consistent by the community\n\n> * Other driver versions worth trying despite issues include 536.99, 537.13, 537.42, 546.17(W10), 551.23(W10), **552.22**\n\nThat doesn't mean that all other drivers are 'bad'. Driver updates may have compatibility issues, stress a system differently and expose edge case system instability so it's always worth re-evaluating any RAM/CPU/GPU overclocks, voltage, memory timings, etc.",
            "I'm happy with 551.61 on my 3080. No stutter, no glitch.",
            "Ampere? Oh lord have mercy on your soul.",
            "What do you mean?",
            "This is what I am trying to tell folks, the previous update (555.85) works flawlessly. It's the new 555.99 that causes the uproar here. Fortunately, I am staying with 555.85 for now.",
            "Thursday will be the fastest for NVIDIA to release. Mostly every two weeks either Tuesday or Thursday they are released.",
            "yup. microstutters and frame drops on games i did not have before.\n\ni think they tried to optimize ghost of tsuishima (which is the only \"newer\" game that runs smooth with this driver mostly) but then didn't take into account other games.",
            "Confirmed by Nvidia and under investigation with reference [4685335]\n\nFrom the unofficial tracking comment at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> * [Halo Infinite] Game crashes during the initial loading screen after updating to GRD 555.99 [4685335] *We are still investigating this issue but I have added the crash to our Open Issues list for now. For now, you will need to roll back to driver 555.85 or earlier driver to play Halo Infinite*",
            "Also 3080\n\nHave sat on 536.23 for a long time now and every time I think about updating something's wrong, which is revealed in the discussion here. There's nothing inherently wrong with that version just instability in frames on 2K in some titles. Screen flickers on dual monitor setup but unsure if it's related to driver or transient spikes or something else.\n\nWould also be interested about the \"best\" 30 series driver for that matter as they do make optimizations for games at times...\n\nEdit: seems to be 537.58 and 551.86 based on comments.\n\nRolled to 537.58 for now. Slightly different fps in Tekken 8, but overall seems to increase fps count during load in.",
            "Same problem",
            "Try downloading and installing the driver directly from the Nvidia website, not using Geforce Experience - https://www.nvidia.com/download/driverResults.aspx/226798/",
            "What games? \n\nWindows 11? \n\n---\n\nFor reference.\n\nDirectX12 games that show a \"Fullscreen Exclusive\" option are really just using/promoted to a borderless window mode known as \"**emulated** Fullscreen Exclusive Mde\" (eFSE).\n\nTrue \"Fullscreen Exclusive (FSE)\" doesn't exist for native DirectX 12 games.\n\n---",
            "FYI I just DDU'd and installed the newest game ready nvidia driver (555.59) and noticed yesterday when playing overwatch 2 in exclusive fullscreen that I would randomly get these stutters. It was weird stutters too - like I would experience zero frame loss before or after. At first I thought it was packet loss / lag but then I noticed once when tabbing out my mouse was acting a bit janky for a brief moment stuttering even. \n\nHopefully going into borderless window alleviates this. Will try to edit this in a week or two with results.",
            "I'm gonna do this thanks. Even stuttering in assetto corsa competizione. Micro stutters nothing to horrible on my 4070 super but it's annoying when my 4060 was buttery on the last driver yet I iodated it to this one adn it was way worse then my new 4070 super with crashing twice and stuttering big time not just micro.",
            "Windows 11 checkerboard issue?\n\nConfirmed as a Microsoft issue, the Nvidia driver is just exposing a flaw in the Microsoft stack/API.\n\nMicrosoft's Windows 11 **KB5037853**  Cumulative Preview Update that addresses the issue should be listed under Windows Update as optional. \n\nNo ETA yet for the Windows 10 update.",
            "Beat course of action mate. 552.22 is what im using right now, the newest update 559.99 is just trash, its causing a massive amount of micro stutters. Use 552.22, its stable and won't stutter all day and night",
            "Same here, rolling it back to 555.85",
            "I had artifacts on siege too in my 4060",
            "555.85 has been perfect for me too.",
            "what's a blurry text have to do with driver?",
            "Which one did you land on?",
            "Pretty vague. What's broken? I looked around a bit but can't find any other reports of an issue.",
            "Wanted to give an update i used ddu and went back further to 552.44 and it is even better. I am getting more frames less browser stutter and everything just feels nicer overall.",
            "I haven't tested locking screen, but if I let my monitor go to sleep (or force it with a command/app), it will flicker every time I wake it up. It's pretty annoying.\n\nAlso, when I open some apps/games, my mouse freezes for a second.",
            "Having the same issue \ud83d\udc4e\ud83d\ude31",
            "537.58 from October 2023.",
            "Been using 555.85 for a few weeks here it's been absolutey flawless.",
            "See the 'troubleshooting and workarounds' section of the unofficial tracking comments e.g. latest at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/ for a list of drivers considered by the community as generally stable/consistent\n\n> * Drivers 512.95, 516.94, 517.48, 522.25, 526.86, 528.49, 532.34, **537.58**, **551.86** are generally considered stable/consistent by the community\n\n> * Other driver versions worth trying despite issues include 536.99, 537.13, 537.42, 546.17(W10), 551.23(W10), **552.22**\n\nFor download, use Nvidia's advanced driver search https://www.nvidia.com/Download/Find.aspx\n\n---\n\nEDIT: bolded latest driver versions for clarity\n\n---",
            "I imagine it is this Womp: [https://womp.com](https://womp.com)",
            "Same here. I am happy with how 555.85 performs so far.",
            "Windows 10 or Windows 11?\n\nMay be of interest, there's always a list of drivers considered by the community as generally stable/consistent in the 'troubleshooting and workarounds' section of the unofficial tracking comments e.g. latest at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> * Drivers 512.95, 516.94, 517.48, 522.25, 526.86, 528.49, 532.34, **537.58**, **551.86** are generally considered stable/consistent by the community\n\n> * Other driver versions worth trying despite issues include 536.99, 537.13, 537.42, 546.17(W10), 551.23(W10), **552.22**\n\nThat doesn't mean that all other drivers are 'bad'. Driver updates may have compatibility issues, stress a system differently and expose edge case system instability so it's always worth re-evaluating any RAM/CPU/GPU overclocks, voltage, memory timings, etc.",
            "I can\u2019t even get the good drivers on 4080 super",
            "I also didn't start getting them until the more recent drivers. I think allowing this is crazy, especially for such expensive products and when they are now the second most valuable company. Just doesn't work out in my head.",
            "what are your specs?",
            "I use Firefox and I can play them fine on this driver. RTX 4080 Super/ Windows 11.",
            "enable vsync on top of gsync, and ultra low latency.",
            "If not already done, please submit a report to Nvidia using the [official form](https://forms.gle/kJ9Bqcaicvjb82SdA). General guidance in [provide valuable feedback](http://nvidia.custhelp.com/app/answers/detail/a_id/3141) document.",
            "Get gpu shark and try spot what exactly is doing it\n\nClick view > detailed mode and look at the bottom it will give an idea at what may be using the card but probably not the %'s but there is other software for that.",
            "Rebooted PC after installing the driver?\n\n> Open Issues\n\n> [GeForce Experience] Driver page may not detect new driver has been installed **until user reboots PC** [4662117]",
            "I have the same thing. SYSTEM\\_SERVICE\\_EXCEPTION ntoskrnl.exe  \nAnalysis showed that the problem is in the drivers, and the Nvidia driver is the only one that has been updated recently.",
            "Ntosknrel.exe is the Windows Kernel, not a Nvidia process - it may be CPU/RAM related.\n\nWhoCrashed *Free Home Edition* may be able to identify what is responsible for crash / BSOD - https://www.resplendence.com/whocrashed",
            "> Additional Open Issues from GeForce Forums\n> \n>  **GeForce Experience Performance Monitoring overlay may stop refreshing GPU information [4679970]**",
            "Agreed I tried everything and finally realized it was the driver. 4070 super. Testednon my 4060 and it wss even worse. Much worse, crashed twice and had heavy stutters/lagging. Mostly happening to me in assetto corsa competizione. 4070 super isn't as bad but it has micro stuttering and more pop in/pop out then usual.",
            "**EDIT:** confirmed by Nvidia and issued bugtrackID [4685335]\n\n> [Halo Infinite] Game crashes during the initial loading screen after updating to GRD 555.99 [4685335]\n\n> *We are still investigating this issue but I have added the crash to our Open Issues list for now. For now, you will need to roll back to driver 555.85 or earlier driver to play Halo Infinite*\n\n---\n\nMay be of interest, from the unofficial tracking comment at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> [Halo Infinite] crash to desktop (CTD) during loading screen [?shader compilation?]\n\n> 'Halo Infinite' and 'The Last Of Us Part 1' crash during shader compile may be due to a Microsoft issue with DirectX Shader Compiler (thanks Astyanax)\n\nIt's being investigated by Nvidia - https://www.nvidia.com/en-us/geforce/forums/game-ready-drivers/13/543951/geforce-grd-55599-feedback-thread-released-6424/3457028/",
            "Last driver had the same issue. I think 555 branch is a downer.",
            "exactly as me i have a 4080",
            "Reverted to 552.22?",
            "It seem so. Same problem with my gtx 980 on X38/48 chipset. Maybe new driver need PCIe lines managed from CPU, not chipset. Or missing AVX instructions on this CPU.\u00a0",
            "537.58",
            "When you're using DDU make sure its in safe mode and you click on the one that wipes and restarts. You may wanna go back a couple months of driver updates",
            "Which game?",
            "DDU then manually reinstall older drivers",
            "So far so good here too, same GPU+CPU. Monster Hunter World randomly crashing on the previous driver and seems better now.",
            "Did you roll back to your previous driver? Could have been server issues maybe?",
            "Made mine worse. I actually didn't have stuttering on 555.85. But I only play sim racing games.",
            "May be of interest, there's a list of drivers considered by the community as generally stable/consistent in the troubleshooting and workarounds section of the unofficial tracking comments e.g. latest at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> * Drivers 512.95, 516.94, 517.48, 522.25, 526.86, 528.49, 532.34, 537.58, **551.86** are generally considered stable/consistent by the community\n\n> * Other driver versions worth trying despite issues include 536.99, 537.13, 537.42, 546.17(W10), 551.23(W10), 552.22\n\nOf note, 551.86 has been in the 'worth trying' section of the unofficial tracking comments since it was released in March 2024. 550 Release/Mainline, [which includes 551.86](https://www.reddit.com/r/nvidia/comments/1cx9i85/game_ready_studio_driver_55585_faqdiscussion/l50z6aj/) is also a long term support branch (LTSB). \n\nThat doesn't mean that all other drivers are 'bad'. Driver updates may have compatibility issues, stress a system differently and expose edge case system instability so it's always worth re-evaluating any RAM/CPU/GPU overclocks, voltage, memory timings, etc.",
            "How? I have a 4070 Ti Super and it won't let me install 537.58",
            "DirectX 12?\n\nCrash on startup?\n\nNot seeing the issue with Warframe here using DirectX11 (Enhanced Engine) and Optimized Flip Model settings.\n\nWarframe's **EE.log** may be able to narrow down the cause\n\nRelated troubleshooting including how to access EE.logs:\n\n* https://support.warframe.com/hc/en-us/articles/200182250-What-information-should-I-give-when-submitting-a-ticket\n\n* https://support.warframe.com/hc/en-us/articles/212890418-What-should-I-do-if-my-game-keeps-crashing\n\n* https://support.warframe.com/hc/en-us/articles/204424844-How-to-Submit-a-Crash-Reports-using-the-Evolution-Engine-Crash-Reporter",
            "See the 'troubleshooting and workarounds' section of the unofficial tracking comments e.g. latest at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n i.e for Windows 11: 537.58, 551.86 are generally considered stable/consistent by the community, other driver versions worth trying despite issues include 552.22",
            "No, according to people on NVIDIA forums: https://www.nvidia.com/en-us/geforce/forums/game-ready-drivers/13/543951/geforce-grd-55599-feedback-thread-released-6424/",
            "Update in the tracking comment at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> The Last of Us Part 1 out of memory crash during shader compilation: *[Manuel@NVIDIA](https://www.nvidia.com/en-us/geforce/forums/game-ready-drivers/13/543951/geforce-grd-55599-feedback-thread-released-6424/3456871/) \"This is unlikely a driver issue but we are still investigating this. If after further investigation, it does point to our driver, I'll add it back\"*",
            "NOP. Same Goddamn Shite",
            "Elden Ring is so weird optimization wise. Not only is it locked to 60 fps but they implemented ray tracing before dlss.",
            "Have you restarted? Could be from below -\n\n**Open Issues**\n\n* \\[GeForce Experience\\] Driver page may not detect new driver has been installed until user reboots PC \\[4662117\\]",
            "you should probably roll back to 552.44. i was having the same issues on rtx 3050 laptop, rollin back fixed it.",
            "https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l76y69k/",
            "What driver version(s) don't have the issue?\n\nIf not already done, please submit a report to Nvidia using the [official form](https://forms.gle/kJ9Bqcaicvjb82SdA). Display issue guidance in [collecting logs for display issues](https://nvidia.custhelp.com/app/answers/detail/a_id/5149)",
            "Increased CPU load? If yes, is the Nvidia APP installed?\n\nDoes the issue occur with driver 552.22 or 551.86?",
            "what are your specs? havent seen that experienced that myself but I'll keep a lookout",
            "I'm still on 555.85 and I even got frame drops in-game, pretty much a typical performance drop every time they update the game to a new season\n\nRTX 4080 super here and I have rock solid 170fps(170hz display) before the season update.",
            "If performance dropped significantly in all games check that Power Limit in Afterburner or similar utilities is at 100% after installing a driver e.g. http://nvidia.custhelp.com/rnt/rnw/img/enduser/aid_5408_02.png\n\nSee https://nvidia.custhelp.com/app/answers/detail/a_id/5408 for reasons/guide",
            "Yikes.",
            "All games impacted just Cyberpunk?\n\nIf performance dropped significantly in all games check that Power Limit in Afterburner or similar utilities is at 100% after installing a driver e.g. http://nvidia.custhelp.com/rnt/rnw/img/enduser/aid_5408_02.png\n\nSee https://nvidia.custhelp.com/app/answers/detail/a_id/5408 for reasons/guide",
            "NVIDIA is the new AMD.",
            "the 50 series isnt even out yet so Nvidia trying to gimp the 40xx cards already would make no sense",
            "Was it also crashing on startup?",
            "Yea it's been happening for a little bit now.  Its a gamble for me to work in Tekken 8.  I've just swapped to obs for time being.",
            "If the installed driver is working well on the system and supports the features/games/applications being used then no need to update.",
            "I was able so try running as admin",
            "Same here, 4070 TI and I get micro stutters on system\n\nTested everything and it all links back to this driver",
            "pls lmk how it goes. I having some crashing too rn",
            "Wipe with DDU and install an older driver.",
            "Dunno, i always install nvidia drivers with NVSlimmer without gfe, audio, telemetry etc.. and disable all audio but my usb interface, but could be tho. Seems that this problem is related to Ampere gpus (and may olders as well?) as a i have 3060 12gb; a buddy tested it with his 4090 in my same other condition (plugins, software, audio etc..) and hes not found theese bugs.",
            "Confirm, back to 552.22 (but keeping new physx driver) and got not a single problem in days, like as common before 555.85.",
            "Solution/workaround for wave link nvidia broastcast VST audio problem is to add wavelink in nvidia control panel (or nvidia app) list of games, and set power for it to 'prefer maximum performance'.\n\nIt's not ideal, given your idle power consumption will increase (in my case from 20-30W to 60-70W when this setting is this way), but it is a working way to use it.",
            "\ud83d\udc4d",
            "Yep, CTD immediately after update :)",
            "Same! I'm not sure if my graphics card is overheating or what even with good temps I get black screens then crash\u00a0",
            "This time, I tried keeping the monitor at 120hz while installing the driver trick you recommened last time, and it looks like it works. I guess we need to do this each time installing a new driver until it gets fixed. Have you had other issues with the previous driver or is it good after installation?",
            "The last two drivers fails to install and brick the pc on 144hz. The trick seems to be installing on 120hz and switching later. Other than that I have been having short random black screens on most games for two years.",
            "I have GT 1030. I am using Dell desktop with an LG monitor. Should I be concerned?",
            "Same here. Not sure why you were downvoted for saying you don't have any issues",
            "DDU is necessary only for rollback like these to me (to be 100% sure to remove everything from the crappy drivers and clean install the previous, working one)\n\nThen you can easily restore a settings' backup from NVIDIA Profile Inspector.",
            "Definitely 552.44",
            "552.44 has a little better 1% Lows",
            "23H2",
            "Laptop manufacturer and model?\n\nWere all 21 [Game Ready/Hotfix/Studio](https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72k0eq/) drivers tested since 537.58? If no, can you try 551.86 and 552.22",
            "I have never ran into this problem with previous drivers, so I gave it a go. The former bench was after a clean DDU install, this time I just updated the drivers, moved the Power Limit slider and put it back to 100% and applied. New results:\n\n555.99  \nScore: 13470  \nTest 1: 88.57 FPS  \nTest 2: 76.64 FPS\n\nSo that must've been it. Good to know, thanks.",
            "Why don't you mention 538.67 quadro driver in 535 branch? Only previous 538.62",
            "Thank you, monitoring.",
            "It's occurred in the past, albeit rarely.\n\nThe 'Indiana Jones and The Great Circle' profile is quite bare / just a stub but is present.",
            "https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l7xcca6/",
            "All you have to do is download the older driver and install it. Your current driver will automatically be uninstalled. You can't rollback using the official Nvidia app. If you don't mind using third-party software, I suggest trying NVCleaninstall, which allows you to pick and install any driver version you want.",
            "    I get really angry, because I know that more lay people don't know how bad an update can be, then they think it's the computer itself that's causing the problem, but it's Nvidia's ****  and its dirty drivers.",
            "if you want to get out of 537.58, the latest driver I'd recommend is **551.86**.",
            "me too U\\_U",
            "have you tried the new driver?",
            "What gpu do you have? and what kind of issues do you have with the newer drivers?",
            "#metoo :))",
            "I was on 512 up untill like 3 days ago.",
            "And here I am installing the latest drivers the day they release. At least I\u2019m a beta tester\u2026 I guess?",
            "FFS still got that one :))  \nit's like more than 6 months now or even more?",
            "Same with me, on a Asus Strix G15 laptop. Any newer drivers seem to give constant stutters doing anything on the computer, occasional graphical glitches.",
            "have you tried some of the solutions?",
            "wait what, is this caused by the nvidia driver!?",
            "Do you have intel 12\\\\13\\\\14 generations?",
            "May be of interest, there's a list of drivers considered by the community as generally stable/consistent in the troubleshooting and workarounds section of the unofficial tracking comments e.g. latest at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> Drivers 512.95, 516.94, 517.48, 522.25, 526.86, 528.49, 532.34, 537.58, **551.86** are generally considered stable/consistent by the community. Other driver versions worth trying despite issues include 536.99, 537.13, 537.42, 546.17(W10), 551.23(W10), 552.22\n\nOut of those, try 551.86 - it supports all current Nvidia Maxwell/Pascal/Turing/Ampere/Lovelace GPUS.",
            "What issues do you have? My  fps drops after this update with this card",
            "Stop spreading misinformation. 555 is a fantastic branch. Zero issues here, there might be an issue with older gen cards but nothing widespread.",
            "I too am having fps issue with Val on the 3080 10gb. (Also on the previous driver version, but seeing as they are not fixed with this one then I will find a previous good version).\n\nMy issue is simply very bad fps drops. My only way to get rid of them is to disable gsync, vsync, and the frame limit in the control panel. I have them enabled on other games with no issues, only in Valorant.",
            "Same here, installer runs through to the end but current version remains at 552.44 afterward. Didn't try rebooting.",
            "Yes that does work. But essentially manually installing the drivers can cause issues when there's ab existing driver files on the GPU itself which is why for 558 I had to use DDU to wipe the drivers clean and then manually install it. Considering geforce us the updating software we shouldn't have to do such things every time a driver gets released they haven't bothered to test properly. \n\nLast 2 drivers have been terrible in terms of performance.",
            "Yup - I needed to for this and the previous driver\n\nAlso noticed that the version number in Afterburner didn't change until I closed and reopened it",
            "Yeah, there's a driver under such number. 537.38.",
            "i have no clue, is it possible to look it up? edit: i found it 546.17",
            "Update: I installed 551.86, and I am still screen tearing and crashing... this is very frustrating and started out of the blue after the update.",
            "Just a few minutes ago, I resumed a paused youtube video and the system hanged again... I could not bring up task manager and the mouse froze, weirdly enough the volume up and down bar was still working. Nothing reported on event viewer prior to restarting... \nBack to square 1, although it doesn't happen nearly as often as it did before, I had 2 full days without crashing prior to this.\n\nAre you using firefox as well?",
            "Maybe, but I have the new Nvidia App installed...",
            "Can't modify whether it's on Windows, NVCP or monitor's osd",
            "Not sure what CRU is. I was able to fix this issue by disconnecting the PSU and the DisplayPort cable. I don't really know if it was replugging the cable or powering off PSU but it's fixed now. It was probably a bug",
            "Chrome - Edge, twitter player stutters too",
            "What about the insane performance drop and fps spiking ingame? I get 40 fps in cod with a 4080 with the new drivers\ud83d\ude2d",
            "Download that version from Nvidia and run the installer.  If you select the advanced setting, you may also opt for it to reset all setting back to defaults.",
            "Thank you. Will try an updated driver",
            "Thanks for the write up. Running 536.99 on my 3070 for some time now. Do you know what the known issues are with 536.99?",
            "Thanks a ton! Just wondering how you get that list?",
            "Thank you for the feedback. Using 551.86 and so far no issues. Didn't game yet though, more general desktop usage",
            "Hi, Any update on the last of us VRAM crash since last 2 drivers?",
            "The longer they will take with this release - the better. It's better to have a working driver than a broken one, in my opinion.",
            "I originally rolled back to 551.61, but I upgraded to 551.86 today, I'll let you know how it goes.\n\n  \nThank you very much!",
            "Please did you have a screen flicker for a second and dissapeared I got it to since I updated to latest version...I got afrraid to death when I saw it.\n\n  \nAnd which Version do you guys think it's best for Rtx 3080?",
            "Thank you! It work. But any idea why geforce experience cant update from there? Last update before this also the same.",
            "Well then that's what I mean. But regardless the option to do full screen causes stutter. \n\nAnd honestly it's most games I've tried. It doesn't seem to be game specific. Jedi Survivor recently, but it's already a stuttery mess, gets even worse with the full screen setting. Horizon Forbidden West was doing it to me as well. Swapping to borderless makes it super smooth. \n\nI've also discovered it might not just be DX12 games, because overwatch 2 is having the same issue and I believe its a dx11 title. \n\nCyberpunk is also having this issue.",
            "Yea my only problem with this issue is that in games that I wanna use DLDSR, I can't because it requires fullscreen, and that means it'll come with stutter issues.",
            "LOL nope, I have and have had no issues on my RDNA 2/3 cards over the past 18 months give or take this have been happening on NVIDIA. So it's 100% NVIDIA. AMD might have 99 problems, but browsing the web on Chrome AIN'T one of them..",
            "I didn't even bother to install it. Question: If a new driver update will be released, do I need to install the 555.99 prior to installing the new update?",
            "552.22 fixed all my problems",
            "DX11 version of the game is completely unplayable. Everything flashes on the screen, textures and UI get all messed up.",
            "ty going to test that one",
            "[deleted]",
            "Deleted the other comment posts. I have discovered my main issue with fps drops is when i have an internet browser playing anything up at the same time. Closing firefox helps the issue i have not tried another internet browser and extensions do not matter. Hardware acceleration is also off. I've ran sfc /scannow and dism commands, there are no drivers or windows updates of any kind, temps are normal and there is no clock anything going on as i don't touch any of those settings ever. i've tried things like letting programs through anti virus which i have also ran two scans one with defender and the other with malwarebytes.  i have also paused windows updates stopped the system main service and have done memory and disk checks. I have removed geforce experience as well and have seen slight improvement.  The last thing i will try after this post is clearing cache of my browser or just load up edge briefly to figure to test if its firefox exclusively tomorrow. I would much rather them just finally release a new driver but it is what it is.  EDIT tested microsoft edge(has not been used ever minus the accidental click of the help button from time to time) before bed with a 4k 60fps video no lag on the browser and hardly any affect on my game while playing.",
            "I got the mouse freezing recently too (Razer Deadadder v2). Did you find the fix for that?",
            "okay me too \u00a0my mouse freezes for a second. ( Razer DeathAdder) but I dont think it's the program synapse because if I reboot my computer no problem but when my motitor sleep same problem then you . Do you have the feeze when you go to \"Power options\" ( windows 11) like right clic windows  icon and power options after a sleep monitor?",
            "im using 536.67 still, will be 1 year, damm what happened to nvidia drivers",
            "Same.",
            "Whats the idea in using DLSS when modeling?",
            "And here. I think I noticed that the camera in The Sims 1 game became closer than usual with the 555.85 update, but maybe that's my eye sight.",
            "537.70 seems to be great for me on win10. There seem to be quite a few 537 drivers in that list interestingly.",
            "I am currently on windows 10, thanks for the answer.",
            "May be of interest, there's a list of drivers considered by the community as generally stable/consistent in the 'troubleshooting and workarounds' section of the unofficial tracking comments e.g. latest at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\nDrivers that support 4080 Super are in **BOLD**\n\n> * Drivers 512.95, 516.94, 517.48, 522.25, 526.86, 528.49, 532.34, 537.58, **551.86** are generally considered stable/consistent by the community\n\n> * Other driver versions worth trying despite issues include 536.99, 537.13, 537.42, 546.17(W10), 551.23(W10), **552.22**\n\nThat doesn't mean that all other drivers are 'bad'. Driver updates may have compatibility issues, stress a system differently and expose edge case system instability so it's always worth re-evaluating any RAM/CPU/GPU overclocks, voltage, memory timings, etc.",
            "Mate honestly id just roll back to 552.22 and wait a month or two and see what happens with the newer ones",
            "Rtx 4080, 13th gen I7, 32Gb DDR5, 2tb NVME M.2 SSD Asus motherboard",
            "Ye thanks, I don't know what happened, but a few hours later it started to work as usual \\^\\^",
            "Yeah ik tried all that already",
            "Aaah yea - That's the one. Thanks :)",
            "so far i've reverted just the GPU driver to 551.76 and no issues when i tried to trigger it again (opening siege) \n\nno issues thus far.",
            ":(\nI googled the address stack failures and it pointed me to a GeForce forum back in June 2020. I\u2019m gonna be upset if my i9-13900k is having issues.",
            "It was strange. I restarted my computer again and my games were buttery smooth again. I'll test it out tomorrow but I was shocked. I've never had games run this well.\n\nWill update tomorrow",
            "Update: yeah everything works fine. Even better than before. Guess it needed a restart and not a shut down.",
            "Thank you, hope they will be as fast as last time!",
            "Ima roll back the drivers then. Hope its fixed by FFXIV Dawntrail driver update if they put one out for it",
            "Yes I believe so. The last one I could find on the Nvidia site. Haven't had any blue screens yet.",
            "If its any help mate, i tested a bunch of drivers, the best i can see that is stable for gaming is 552.22, its has 0 game micro stutters. 559.99 is just an abomination",
            "I posted this on Nvidia's Feedback thread  \nand they responded with this\n\n\"We likely no longer have this CPU in our labs as it is not supported on the latest Windows OS. In order to look into this, we will need a complete Windows BSOD crash dmp file. Please review the FAQ below to capture a complete Windows bugcheck crash dmp file from your PC with the latest driver and send it to us at\u00a0[driverfeedback@nvidia.com](mailto:driverfeedback@nvidia.com).\"\n\nbut now I dont have time because of the collage  \nso if you can make the file and send it to them  \nHere is the link for that Thread  \n[https://www.nvidia.com/en-us/geforce/forums/game-ready-drivers/13/543951/geforce-grd-55599-feedback-thread-released-6424/3459000/](https://www.nvidia.com/en-us/geforce/forums/game-ready-drivers/13/543951/geforce-grd-55599-feedback-thread-released-6424/3459000/)",
            "Every game seemingly. Was happening in Dota 2, then I tried Deep Rock Galactic and same thing. Rolled back and it's fine again.",
            "It fixed itself after a while\n\nProbably compiling shaders, but tell me you're compiling shaders",
            "I'm on 552.44 and I still have the stuttering issue. Thinking of downgrading to the version before 552.",
            "537.58 not supported for your gpu.",
            "I've already rolledback to 552.22 but it seems Warframe wasn't the only application to crash.  \nWindows explorer also did with the same Fault Module as warframe. nvgpucomp64.dll.\n\nSince rolling back ive had 0 issues.  \nusing a 4090 and i9 13900ks",
            "Some change in the driver caused it. Last patch the game had was end of April, and the driver from mid May caused this issue to appear.",
            "FromSoftware makes incredible games but their engine needs a lot of work sadly. I beat and love all 7 Soulsborne games, but none of them run well above 60fps so they just cap it there.",
            "They also never fixed that GPU utilization bug that skyrockets GPU usage ~5x (tanking FPS) when trying to render some invisible effect at some viewing angles. My new GPU just powers through it now, but that was seriously annoying, and their previous games didn't have that bug.",
            "thank you!",
            "It's just like what happened with earlier drivers they fixed with another driver. Just not as bad.",
            "mine dropped a lot of fps , like 40 to 50",
            "Hey , did they put the last of us part 1 out of VRAM issue back to the bugs list? Or did they just ignore it?",
            "THANK YOU.  I have *never* had a driver update cause the power limit in afterburner to change to 20%.  For whatever reason, it did with 555.99.",
            "That's a good question.  I didn't try it on any other games.  I just clean installed 552.44 and am running nice and smooth again.",
            "Crashed several times in a row at startup before i could even get to the main menu. Now I can get in a very short session before it crashes",
            "Same here, just rolled back and it's a night & day difference. Using I believe the May 16th release version .88. Hope this helps.",
            "It has been 5 days without crashes, but I will keep monitoring. Sometimes these crashes could be very rare, that or disabling XMP actually helped.",
            "By the way, are you sure are you experiencing my same crashes? I got a single one, with ID 153 and 14 lasting about 20 seconds. It could be the same issue, but ne necessarily the same.",
            "I did same. Last driver was working fine. I rushed to update in hope they fixed it.",
            "Maybe I didn't face this issue because I'm using at 120hz(I literally can't use my monitor at 120hz+ because of the lack of DP 2.1). I also never faced this black screen issue though.",
            "[deleted]",
            "Thanks for confirming and updating benchmarks, it's appreciated.",
            "Released 4th June, just haven't got around to installing and checking the branch.\n\nI'll add a stub and include branch information later.",
            "Good to know, thanks.",
            "never nvidia again. buying amd moving forward, fuck nvidia. paid 2k for a piece of brick that doesn't work",
            "I\u2019ll try it",
            "4080\n\nFPS drops, horrible 1% lows in CS2. \n\nI lose about \\~100FPS on newer drivers no idea why. 537.58 is the most stable driver.",
            "My personal case, I haven't had that issue. I had the issue of it not updating, but not it breaking with previous driver files. But yeah, I do agree that GF should just work properly :(",
            "I just remember seeing that there was an issue with GeForce experience causing this to happen I\u2019m not sure if that\u2019s the issue",
            "Ok glad you got it sorted and CRU is custom resolution utility it allows you to create custom resolutions with refresh rates",
            "When performance drops significantly it's always worth checking Power Target in any GPU utility that was open when a driver was installed e.g. http://nvidia.custhelp.com/rnt/rnw/img/enduser/aid_5408_02.png\n\nSee https://nvidia.custhelp.com/app/answers/detail/a_id/5408 for reasons/guide\n\nIf the issue persists please submit a report using the [official form](https://forms.gle/kJ9Bqcaicvjb82SdA). General guidance in [provide valuable feedback](http://nvidia.custhelp.com/app/answers/detail/a_id/3141) document. \n\nFor reference, I'm not a Nvidia employee, just a fellow Redditor helping the Reddit community collating information / troubleshooting / tracking issues e.g. latest at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/",
            "These may assist.\n\nUnofficial 536.99 tracking comment at https://www.reddit.com/r/nvidia/comments/15luoca/game_ready_studio_driver_53699_faqdiscussion/jvcsy5x/\n\nDriver 536.99 general discussion at https://www.reddit.com/r/nvidia/comments/15luoca/game_ready_studio_driver_53699_faqdiscussion/",
            "From here: [https://www.reddit.com/r/nvidia/comments/1d7xdbv/game\\_ready\\_studio\\_driver\\_55599\\_faqdiscussion/l72jj11/](https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/)\n\nThis user tracks the drivers and which ones gets the less bug reports or issues and considers them safe to update.",
            "We will have a solution in our next Game Ready Driver.",
            "I haven't bothered to change from the 537.58. I think it may have still flickered but it does it to lesser extent. It only did it on boot-up and randomly while watching something, the few times i've noticed. I chose 537.58 because i noticed people who run 551.86 were mainly reporting their success with 40 series cards. Where as 537.58 was 30 series mentions.\n\nIt's not unheard that newer drivers support older generations less and less. Although there's no basis that 551.86 supports 30 series less, just that 40 series was already released when that version came to be.",
            "NO",
            "Nope... I still have stuttering when opening certain applications (or at least the first time I open them). This issue was introduced by the previous driver version, 555.85. It's quite annoying, but fortunately it doesn't really impact gaming.\n\nBTW, I also have a Deathadder v2... (updated to latest firmware v1.04.01\\_r1). I really hope this issue is not specific to our mouse model or brand, cause then it'll take longer to fix.",
            "No... it's weird. But I just tried opening programs like Steam or Discord, which I clearly remember causing freezes, and didn't happen this time. It's pretty inconsistent.",
            "3.5 is Ray Reconstruction for denoising and lighting. Not an expert, but it kinda makes sense to me.",
            "Thanks bro!!",
            "how do you get these driver versions installed with a 4080Super? i tried installing 532.58 and it couldn't even find my video card",
            "Thank you. I will try 551.86 on my Rtx2070. May i ask : Is studio version more stable than gaming version ?",
            "disable hardware monitoring, close all apps like msi afterburner, rivatuner and nvidia app?\nthey may induce stutter when polling gpu voltage or access motherboard sensors.\nAnd same problem for LED softwares \n\nIcue, asus crapwares, razer synapses are known to induce stutter too.",
            "13th Gen Intel CPUs e.g. 13900K may have issues, see https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> ... check CPU clock/voltage/multipliers/core enhancement etc, particularly if system has an Intel 13th or 14th gen CPU - \n\n> https://www.radgametools.com/oodleintel.htm \n\n> and \n\n> https://community.intel.com/t5/Processors/Regarding-Reports-of-13th-14th-Gen-Unlocked-Desktop-Users/td-p/1575863\n\n> Motherboard manufacturers are releasing updated BIOSes to mitigate the issue e.g. adding **'Intel Default Settings'** profile or similar",
            "Thanks for confirming, appreciated.",
            "Games like to compile shaders after every graphic cards driver update that's why games stutter in loading new locations/areas etc.",
            "*nvgpucomp64.dll* is the CPU based compiler DLL.\n\nIntel 13th or 14th generation CPU e.g. 13900K  / 14900K?\n\nIf yes, see https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> NVGPUCOMP64.dll and/or 'out of VRAM' crashes (shader compilation), check CPU clock/voltage/multipliers etc, particularly if system has an Intel 13th or 14th gen CPU - \n\n> https://www.radgametools.com/oodleintel.htm \n\n> and \n\n> https://community.intel.com/t5/Processors/Regarding-Reports-of-13th-14th-Gen-Unlocked-Desktop-Users/td-p/1575863\n\n> Motherboard manufacturers are releasing updated BIOSes to mitigate the issue e.g. adding 'Intel Default Settings' profile or similar",
            "Yes, it may a be a driver issue, hopefully Nvidia can clarify soon. For now they've removed it from the issues list as it may also be a game developer issue.",
            "Same. Bloodborne is my personal favourite (beat it 6 times) but dont have high hopes for a good PC port if it ever happens.",
            ">none of them run well above 60fps so they just cap it there. \n\n\nStarting with Sekiro, the games can run with mods at high framerates without issues.",
            ">thank you!\n\nYou're welcome!",
            "From the unofficial 555.99 tracking comment at https://www.reddit.com/r/nvidia/comments/1d7xdbv/game_ready_studio_driver_55599_faqdiscussion/l72jj11/\n\n> The Last of Us Part 1 out of memory crash during shader compilation update: *Manuel@NVIDIA \"This is unlikely a driver issue but we are still investigating this. If after further investigation, it does point to our driver, I'll add it back\"*\n\n> 'The Last Of Us Part 1' crash during shader compile may be due to a Microsoft (not driver) issue with DirectX Shader Compiler (thanks Astyanax)",
            "Thanks, monitoring.",
            "I've just been intermittently blackscreening for the last week.",
            "Last driver was much better for me (with RTX 3080) but still did the DSC blanking thing about once an hour (which is an improvement over 5+ times an hour).  I have teammates that rely on me... I had to roll back to an old driver (532.34).",
            "No, just DDU in Safe Mode will be enough. It will remove everything",
            "What is the difference between (Insider/Developer) and (Game Ready/Studio) driver updates?",
            "Thanks alot!",
            "Great, thank youuu.",
            "Doesnt 3.5 just mean its everything including the denoiser? But yea now that you mention it that does make sense.",
            "Hey man I really appreciate you taking the time to search/sort and reply with these things for me.",
            "Wow, This is kind of shocking. I had this issue before and RMA'd the cpu.  \nTurns out to be their fault from the start. and im looking at a 20% mutli core performance loss.  \nThis stinks.\n\nThank you very much for helping me! :)",
            "So we should wait for a fix from Microsoft?",
            "Insider / Developer are usually provided for testing new features/fixes/enhancements and may have issues.\n\nGame Ready/Quadro/Studio are for general day to day use.",
            "Best thing to do is to do some extensive testing after updating your bios to the latest version and enabling the intel recommended settings. If you're still having some potential crashes, it may be a good idea to rma your chip altogether as it may have already been degraded."
        ]
    },
    "Tech Support and Question Megathread - June 2024 Edition": {
        "title": "Tech Support and Question Megathread - June 2024 Edition",
        "score": 8,
        "url": "https://www.reddit.com/r/nvidia/comments/1d5fcng/tech_support_and_question_megathread_june_2024/",
        "content": "We're consolidating **all** tech support posts and questions into this **monthly** tech support and questions megathread.\n\nIt should be noted, r/NVIDIA does not represent NVIDIA in any capacity unless specified. There's also no guarantee NVIDIA even read this subreddit, if you have an issue, criticism or complaint; it's recommended to post it on the official GeForce forum.\n\n**All Tech Support posts that do not include sufficient information will be removed without warning**\n\nBefore creating a Tech Support post, please see our additional resources section, it solves a lot of common issues.\n\nTL;DR: **DO: Use the template. DO NOT: \"i have driver issue please help not 60fps!!\"**\n\n# For Tech Support Posts\n\nPlease use this template below - posts without adequate information will be removed, we can't help you unless you provide adequate information.\n\n**Status:** UNRESOLVED/SOLVED - please update if your issue is resolved\n\n**Computer Type:** State if your computer is a Desktop or Laptop and the brand/model if possible, e.g Desktop, custom built\n\n**GPU:** Provide the model, amount of VRAM and if it has a custom overclock, e.g. GTX 1070, 8GB of VRAM, no overclock\n\n**CPU:** Provide the model and overclock information if possible, e.g. Intel Core i5 6600k, no overclock\n\n**Motherboard:** Provide the model and current BIOS version if possible, e.g. MSI Z170A GAMING M9 ACK, latest BIOS (1.8)\n\n**RAM:** Provide the model and overclock information if possible, e.g. Corsair 8GB (2x4GB) DDR4 2400MHz, XMP enabled, no overclock\n\n**PSU:** Provide the model and its rated wattage and current output if possible, e.g. EVGA 850 BQ, 850W, 70amps on the 12v rail - for laptops you can leave this blank\n\n**Operating System & Version:** State your OS and version, also please state if this is an upgrade or clean install, e.g. Windows 10 build 1607 64bit, upgrade from Windows 8.1\n\n**GPU Drivers:** Provide the current GPU driver installed and if it\u2019s clean install or upgrade, e.g. 376.33, clean install\n\n**Description of Problem:** Provide as much info about the issue as you possibly can, images and videos can be provided as well.\n\n**Troubleshooting:** Please detail all the troubleshooting techniques you\u2019ve tried previously, and if they were successful or not, e.g. tried clean install of GPU drivers, issue still occurs. Please update this as more suggestions come in\n\n# For Question & Answer Post\n\nAdditionally, this thread will be used to answer general questions that may not warrant having their own thread -- this could be questions about drivers, prices, builds, what card is the best, is this overclock good etc\u2026\n\nPlease don't downvote questions for the sake of helping others. We will also sort the post randomly so every question can be seen and answered.\n\nIf you don't have any tech support issues or questions, please contribute to the community by answering questions.\n\n# Here are some additional resources:\n\n* [Display Driver Uninstaller (DDU) tutorial](https://docs.google.com/document/d/1xRRx_3r8GgCpBAMuhT9n5kK6Zse_DYKWvjsW0rLcYQ0/edit)\n* [Repairing/Maintaining/Cleaning a Windows 8, 8.1 or 10 Image](https://docs.google.com/document/d/1Y7HOS6UiOBXAWtNFBJ0w70f14izRQcBYD9puuSw6Ghs/edit)\n* [How to repair/verify game files](https://docs.google.com/document/d/1Ba3RjEBnFcnUec0axX1ETgb1ndKvxSAX2YGQGu_leL0/edit)\n* [Malware/Virus Removal Guide](https://docs.google.com/document/d/16er6ZmSj4nUHBP-80EQLZjNgLNIDHyE93u0Vu8e6wV0/edit)\n* MemTest86 tutorial by [DigitalStorm](https://www.youtube.com/watch?v=e91hb60iPew) \\- faulty memory can cause a lot of problems, running MemTest86 will verify if your memory is faulty or not\n* You can also check the sidebar for helpful links, we update it regularly\n* You can visit our [Discord](http://discord.gg/nvidia) to chat with other NVIDIA users\n\nAgain, it should also be noted, r/NVIDIA is not a dedicated Tech Support forum and your question/issue may not be resolved. We also recommend checking out the following\n\n* r/TechSupport \\- A Subreddit dedicated entirely to answering Tech Support related questions/queries\n* [GeForce Support](http://www.geforce.com/support) \\- answers to the most common questions with a knowledgebase available 24x7x365\n* [Official GeForce Forum](https://forums.geforce.com/) \\- Posting your complaints, criticism and issues here will increase the chances an NVIDIA employee sees it.\n* [NVIDIA Support](http://nvidia.custhelp.com/app/home/) Includes live chat and email\n\nIf you think you\u2019ve discovered an issue, it\u2019s crucial you report it to NVIDIA, they can't fix an issue unless they know it exists.\n\nHere\u2019s a guide on how to submit [valuable feedback](http://nvidia.custhelp.com/app/answers/detail/a_id/3141)\n\nAnd here\u2019s where you [submit feedback](https://forms.gle/kJ9Bqcaicvjb82SdA)\n\nIf you have any questions, or think this template post could be improved for future use, please message the [/r/NVIDIA moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fnvidia)\n\nWant to see previous version of this thread? Click [here](https://www.reddit.com/r/nvidia/search?q=Tech+Support+Megathread&sort=new&restrict_sr=on)",
        "num_comments": 62,
        "comments": [
            "4070 Ti Super\n\nMSI Gaming X Slim $640\n\nMSI Ventus 2X $530\n\nGaming X Slim worth $110 more?  Worried the 2X may be noisy but I'm also not afraid of undervolting.",
            "**Status:** UNRESOLVED\n\n**Computer Type:** Desktop (with two monitors)\n\n**GPU:** ASUS RTX 3060 TUF Gaming OC GPU (PCIe 4.0, 12GB GDDR6, 3584 CUDA, 2x HDMI 2.1, 3X DP 1.4a, TUF-RTX3060-O12G-GAMING)\n\n**CPU:** AMD 3rd Gen Threadripper 3960X CPU (24-Core, 48-Thread, 3.8GHz, 280W, CPU Benchmark 47201)\n\n**Motherboard:** Unknown\n\n**RAM:** 3rd Generation Threadripper Performance Memory: 32GB (2x16GB) Corsair Vengeance DDR4 3200 CL16 (CMK32GX4M2B3200C16 kit)\n\n**PSU:** Corsair HX1200 Modular PSU\n\n**Operating System & Version:** Ubuntu 22.04.4 LTS\n\n**GPU Drivers:** Unknown\n\n**Description of Problem:**   \nI tried to update by nvidia drivers because a video game was complaining that they were out of date. After executing: **sudo ubuntu-drivers install**, and restarting, my right monitor is black for the right 60-ish percent, and normal on the left side. The other monitor works perfectly. When I unplug the monitor that works perfectly, the flawed monitor corrects itself (without rebooting or taking any other action), and is fully usable. Also, even in the black portion of the right monitor, the mouse arrow is visible.\n\n**Troubleshooting:** \n\n**>nvidia-cmi**  \n*Failed to initialize NVML: Driver/library version mismatch*  \n*NVML library version: 535.171*\n\nCommands I executed (which may have made things worse, IDK):  \n  \n**> sudo apt-get purge '\\^nvidia-.\\*'**  \n**> sudo apt-get autoremove**  \n**> sudo apt-get autoclean**  \n**> sudo add-apt-repository ppa:graphics-drivers/ppa**  \n**> sudo ubuntu-drivers autoinstall**  \n**> sudo reboot**\n\nWhen that didn't work, I tried again, but replaced the first line with:\n\n**> sudo apt-get purge 'nvidia-.\\*'**\n\nAnyway, my right monitor is still 60% black, so I didn't fix things ore make them obviously worse. However, before I started mucking about with things, the nvidia-cmi command gave a different response.",
            "**Status:**\u00a0UNRESOLVED\n\n**Computer Type:**\u00a0Custom Built Desktop\n\n**GPU:**\u00a0RTX 3080 Founders\n\n**CPU:**\u00a0Ryzen 7 5800X\n\n**Motherboard:** MSI B450 Tomahawk\n\n**RAM:**\u00a032GB DDR4 G.Skill Speed Unknown\n\n**PSU:**\u00a0Seasonic, Unknown specific Model, 850Watts\n\n**Operating System & Version:** Windows 11, fresh clean install 2 days ago\n\n**GPU Drivers:**\u00a0Current NVidia drivers\n\n**Description of Problem:** I upgraded to the new NVIDIA App (If you haven't seen it it's their replacement for GeForce Experience) and I'm having an issue with the GPU Power Maximum feature. Namely, in GeForce Experience, I could turn the power down and, as expected my GPU would use less power. Now as expected at 100% power maximum my 3080 gets hit for 150 watts. But if I turn it down to 32% maximum (its minimum) it gets hit for...150 watts, no change at all. Previously it would drop down to some 45ish watts at that maximum. Has anybody else experienced this? Is it a bug? Any advice?\n\n**Troubleshooting:** Everything is a clean install. Power Limit works if I uninstall Nvidia App and go back to GeForce Experience, no setting I could find in Nvidia App seems to have had any effect.",
            "I already fixed it but I'll post it here in case anyone else has the same issues:\n\n**Status:**\u00a0SOLVED\n\n**Computer Type:**\u00a0Laptop, Lenovo brand (2021 6th gen)\n\n**GPU:**\u00a0Laptop 3080\n\n**CPU:**\u00a0Some Ryzen 9\n\n**Motherboard:**\u00a0idk but it's not the issue anyway\n\n**RAM:**\u00a032GB (embarrassing MHz)\n\n**PSU:**\u00a0Manufacturer's laptop power brick\n\n**Operating System & Version:**\u00a0Windows 11 Home, version 22H2, last fresh install a year ago.\n\n**GPU Drivers:**\u00a0555.99, upgraded from previous versions (I might've leapfrogged an update or two this year).\n\n**Description of Problem:**\u00a0In recent weeks (~~not sure when it began~~ since turning off HDR on all displays) when the second display (an LG TV) turns off (i.e. usually due to inactivity), turning it back on would not present my expected 2nd and extended desktop display but instead would cause the primary display (laptop) to go blank, flicker, then go dead and the system would no longer respond to power button taps, blind use of WinKey + P (cycling displays) or closing the lid (sleep) requiring the laptop power button to be held to turn it off. Oh and the TV would begin cycling through its no signal generic artwork having lost signal around the time the laptop screen gave up.\n\nIn Event Viewer, among the ruckus, there would be a combo of:  \n`Warning (Yellow Triangle) | [date]: at time of problem | Source: Display | Event ID: 4101 | Task Category: None | General: Display driver nvlddmkm stopped responding and has successfully recovered.`  \n& immediately preceding that:  \n`Error (red circle) | [date]: three seconds prior to above | Source: nvlddmkm (Nvidia driver in system32) | Event ID: 153 | Task Category: None | General: The description for Event ID 153 from source nvlddmkm cannot be found. Either the component that raises this event is not installed on your local computer or the installation is corrupted. You can install or repair the component on the local computer.`\n\n`If the event originated on another computer, the display information had to be saved with the event.`\n\n`The following information was included with the event:`\n\n`\\Device\\00000122`\n\n`Restarting TDR occurred on GPUID:100`\n\n`The message resource is present but the message was not found in the message table`\n\n**Troubleshooting:**\u00a0Found this archived thread: [https://www.reddit.com/r/nvidia/comments/12l01wf/nvlddmkm\\_4090\\_crash\\_solved/](https://www.reddit.com/r/nvidia/comments/12l01wf/nvlddmkm_4090_crash_solved/) in which the top reply suggests adding permissions for User in the Properties of the troublesome driver (nvlddmkm.sys) so it can stop failing at its task for whatever reason. I did also need to take ownership of the driver first via this guide as I did not have Edit Permissions in the Security tab of the Properties: [https://www.thewindowsclub.com/take-ownership-windows-8](https://www.thewindowsclub.com/take-ownership-windows-8)\n\nOnce done I restarted the laptop and after logging in and reaching my desktop I turned off the TV, waited five seconds, turned it back on and had no issues with the laptop picking up my the 2nd extended display \ud83c\udf89",
            "**Status:**\u00a0UNRESOLVED\n\n**Computer Type:**\u00a0Desktop Custom Build\n\n**GPU:**\u00a0Geforce gtx 1080\n\n**CPU:**\u00a0intel i7-44790k 4.00 ghz\n\n**Motherboard:**\u00a0asrock\n\n**RAM:**\u00a06gb ddr3 all i remember\n\n**PSU:**\u00a0idk\n\n**Operating System & Version:**\u00a0Windows 8.1 \n\n**GPU Drivers:**\u00a0474.11\n\ni just changed my g card from 970 to 1080 I was having issues updating it b4 it would update I restarted and it deleted the icon (I have it circled in the pic) and I have no sound and it still says to install the update I just installed. the only way to fix it is to delete the driver and restart to so it puts the of bk in.. idk what to do to fix this I don't mess with computers often so I don't have a clue as what to do. i will be updating my windows to 10 or 11 next month which I think may be the issue as its causing issues with everything else too but is there a version I can dl that will work with 8.1? another issue is I can pick my had from the list when I hit f11 but I can't find it to set it in the boot order this did this when I changed cards idky.\n\nhttps://preview.redd.it/9j5n6vuoa07d1.jpeg?width=331&format=pjpg&auto=webp&s=93fa9c93590e83998071b8671d66017cf3230f4f",
            "**Status:**\u00a0UNRESOLVED\n\n**Computer Type:**\u00a0Desktop\n\n**GPU:**\u00a0Asus TUF GAMING OC GeForce RTX 4070 Ti SUPER 16 GB - no overclock\n\n**CPU:**\u00a0AMD Ryzen 7 7800X3D 4.2 GHz 8-Core - no overclock\n\n**Motherboard:**\u00a0MSI MAG X670E TOMAHAWK WIFI ATX AM5 - latest BIOS\n\n**RAM:**\u00a02 x 32 GB Corsair Vengeance DDR5-6600 - XMP, no overclock\n\n**PSU:**\u00a0Corsair HX Series HX1000, 1000W\n\n**Operating System & Version:**\u00a0Windows 11, clean install\n\n**GPU Drivers:** GeForce Game Ready Driver 555.99\n\n**Description of Problem:**\u00a0Can't create custom resolutions from Nvidia Control Panel\n\n**Troubleshooting:**\u00a0Recently bought the Zowie XL2586X but I can't seem to create custom resolutions using Nvidia's Control Panel. No matter what I do, the \"Customise...\" button is greyed out, only lighting up for a quick second after changing resolutions just to go back to being greyed out.\n\nI have tried almost everything possible in the Control Panel:\n\n- there's no DSR to be turned off\n\n- different scaling modes\n\n- using display and GPU to perfrom the scaling\n\n- different stretching modes\n\n- tried checking \"override the scaling mode set by games and programs\"\n\n- tried almost every combination of the 3D program settings\n\n- tried turning down the HZ of the monitor from 540, to 500, to 480, to 360, to 240, to 144 and even 120 HZ and still doesn't make any difference\n\n- did Windows updates\n\n- updated every possible driver\n\n- tried using CRU to create the custom resolution to no avail\n\n\n\nThe resolution I'm trying to create is 1080x1080, one which I had no problems with my previous monitors, especially with my former one - XL2546K\n\nhttps://preview.redd.it/z3rllt1j9y7d1.png?width=404&format=png&auto=webp&s=c17c22abc14c7e048560cfa9098f86149dec7241",
            "Please Help.\n\n# UNSOLVED\n\n**SUMMARY/TROUBLESHOOTING**\n\nI bought a ASUS ROG Zephyrus M16 gaming laptop about a year ago and have managed to make the 3070ti in it practically useless. Out of the box it ran amazing, running modern game titles at 165hz on high/ultra with no issues and great framerates. One day while gaming I was getting \"half\" the performance (half the frames than normal and some lag here and there). I've dealt with driver issues before on my custom desktop AMD build and thought it was that. Re-installed drivers, problem persisted, so I went ahead and downloaded DDU and did a factory reset on the laptop...only to realize while I was doing the reset and whatnot that the power supply was unplugged leading to the downgrade in performance. Since then, I haven't been able to run a game on the computer with anything higher than 20-30 frames. I've run memory diagnostics, no problems found. Multiple driver re-installs, updates, disabling and re-enabling of the display adapter to see if it was running on integrated graphics, nothing helped. Changing in-game settings(graphic quality, G-sync, render resolution, windows \"Hardware accelerated GPU scheduling\" toggled on and off, nothing has made any difference. I even bought a cooling pad to see if it was temperature-related. Below are the benchmark results I've been getting and the settings I am using on the ASUS Hardware software. Any suggestions are appreciated, I have yet to open the laptop  as I've been wanting to avoid that but can if needed, although I doubt it has accumulated enough gunk in a year and a half to cause these type of problems.\n\n**SPECS**\n\nASUS ROG Zephyrus M16 GU603ZW\n\nGPU: 3070 ti 8gb no overclock\n\nCPU: 12th Gen Intel i9-12900H 2500Mhz 14 core no overclock\n\nRAM: 16 Gb DDR5 4800mhz\n\nSMBIOS: 3.5\n\nOS: Windows 11 Home\n\nclean install of drivers\n\n\\*below are pics of ASUS hardware software and benchmark results\\*\n\nhttps://preview.redd.it/n4093jlx8y3d1.png?width=2300&format=png&auto=webp&s=6d44b119ca964c96068092f962ccc3ee98072a17",
            "Which is the most stable driver for mobile RTX4070?",
            "unsolved\n\nthe geforce experience won't connect to the internet so i can't log in to use it, already tried uninstalling and reinstalling it and restarting the pc, neither of them worked",
            "Is there a list of what games have Game Ready drivers? All I can find are the ones added by the most recent update.",
            "What makes a GPU sag or not? I just installed my 4080 super and it doesnt seem to be sagging to me\n\nhttps://preview.redd.it/gzm4e5lzj37d1.jpeg?width=4032&format=pjpg&auto=webp&s=7275b84380052adc38bbc227398c952b225668be",
            "**Status:**\u00a0UNRESOLVED\n\n**Computer Type:**\u00a0Desktop Prebuilt NZXT\n\n**GPU:**\u00a0NVIDIA GeForce RTX 2080 Super 8gb VRAM\n\n**CPU:**\u00a0Intel Core i9 9900K u/3.60GHz (16 CPUs) 3.6Ghz\n\n**Motherboard:**\u00a0ASUSTek Computer INC. ROG Maximus XI Hero (Wi-Fi) Bios 1105\n\n**RAM:**\u00a0TeamGroup T-Force Xtreem ARGB 3600MHz CL 18 32GB (2x16GB) PC4-28800 Dual Channel DDR4 DRAM\n\n**PSU:**\u00a0C850 Gold 850w\n\n**Operating System & Version:**\u00a0Windows 10 Pro 64-bit (10.0 Build 19045)\n\n**GPU Drivers:**\u00a032.0.15.5599 geforce installer\n\n**Description of Problem:**\u00a0A few months ago I noticed that every time I start to play a game or sometimes the pc is idle I get this grunting noise. At first I thought it was coming from psu so I replaced it. But today the noise remain. I looked closely at the gpu and noticed the fans would spin up and the noise would occur. The fans would slow down and speed up again and the grunting noise will occur again. It lasts for a few hours. Here is a link to a short video I made. [https://jmp.sh/G2NqgWUT](https://jmp.sh/G2NqgWUT) \n\n**Troubleshooting:**\u00a0 I replaced the PSU with a new one. (same model) thinking the noise was coming from the PSU but later found out its not.",
            "I heard bad things about the thermal pads on the 3090 Founder's Edition. I just got a 4090 Founder's Edition. Should I be thinking about replacing the thermal pads? I plan to run AI workloads, so the card will frequently be at 100%.\n\nEDIT: Did some more research. It looks like the 4000 series has excellent thermal pads according to https://www.reddit.com/r/nvidia/comments/1702pz9/4080_fe_thermal_pads_thickness/k3jtslo/ .",
            "Unresolved.\n\nDoes the NVIDIA App recording not work with older drivers such as 527.56? Whenever I start recording it instantly says saving recording and nothing gets saved.",
            "I've been using DLDSR (1080 > 1440) for a while and my other windows such as discord and resize itself after exiting a game. Is there any way to stop that from happening? I once tried to make my resolution 1440p but the fonts on my screen become blurry.  Any suggestions?",
            "Recently I am having issues with Geforce Experience Instant Replay. It's like when I press Alt-F10, it doesn't save my game play. Then when I Alt-Tab back to Windows to Alt-F10, it saves a screen of my game but it's not a video, but only audio.  \n  \nI've done a clean install of my driver/Geforce Experience but I still experience the problem.\n\nAnyone facing/faced this issue?",
            "UNSOLVED\n\nComputer type : Custom built deskstop\n\nGPU : Gigabyte RTX4070Super Windforce OC\n\nCPU: intel i5-13600KF\n\nMotherboard : MSI B760 Gaming Plus Wifi. Bios H.60 \n\nRAM : 64GB Corsair vengeance DDR5 5600MHz XMP enabled\n\nPSU : BeQuiet Pure power 12M 850W\n\nOS : Windows 11 clean install\n\nGPU driver : 555.99 upgraded, previous driver displayed same issue.\n\nDescription of Problem : I've been using NVIDIA Instant Replay quite regularly, either through GeForceNow or the NVIDIA App, in the \"capture all desktop\" mode. I have no video issues on this one but rather an audio one. Multiple times during the video there is a bit of audio missing, not silent but like you skipped a word in a sentence and kept going. This happens with both system audio and mic audio. It seems like the sound is sync with the video throughout the whole video. These clips are recorded in Variable Frame Rate and I need to convert them through Constant Frame Rate to edit them which accentuate this problem. (Both CPU and GPU encoding through Handbrake accentuate the problem). The clips are 1440p 60FPS, bitrate 50MoBps, 20min length\n\nTroubleshooting : I've tried modifiying the bitrate of the instant replay, tried removing and putting back the card in the MTB.\n\nNote : Clips are saved on my HDD (seagate Barracuda 2To) while OS is on my SSD (WD SN850X) but I've never seen the HDD nowhere near 100% usage.",
            "**Status:**\u00a0UNRESOLVED\n\n**Computer Type:**\u00a0Laptop, GIGABYTE G5 KD, bought in February 2021\n\n**GPU:**\u00a0NVIDIA\u00ae GeForce RTX\u2122 3060 Laptop GPU 6GB GDDR6, no overclock\n\n**CPU:**\u00a011th Gen Intel\u00ae Core\u2122 i5-11400H,\u00a0no overclock\n\n**Motherboard:**\u00a0GIGABYTE G5 KD, BIOS: INSYDE Corp. Version FB5, latest driver I was able to find on the GIGABYTE website, its from 05/25/2022\n\n**RAM:**\u00a0built-in RAM from GIGABYTE, DDR4, 16GB (CPU-Z says the module manufacturer is called Crucial Technology)\n\n**Operating System & Version:**\u00a0Windows 10 22H2 (OS Build 19045.4529), clean install, 64-bit\n\n**GPU Drivers:**\u00a0NVIDIA GameReady Driver, Ver. 32.0.15.5599, updated from an older driver version\n\n**Description of Problem:**\u00a0I play EAFC 24, xDefiant and Doom 2016. In every game I experience sudden FPS drops from stable FPS around 80 and higher down to 30 FPS and less. The issue appears after a short playing time, the first minutes of the games the FPS are high and stable, then they drop and it becomes unplayable. The laptop gets pretty hot I have to say. I can't tell if it would work better when cooled better. Maybe the onboard fans can't provide enough cooling power anymore?\n\n**Troubleshooting:**\u00a0lowering graphic settings of the games, adjusting settings in NVIDIA Control Panel, clean re-install of NVIDIA drivers on Windows 11, new OS install with Windows 10, clean install of the NVIDIA drivers, tried to use older NVIDIA drivers, detached my second screen and only used laptop screen",
            "can i delete content of DXCache folder in appdata/local? its 27 gigabytes and i dont think it should be THAT big",
            "Hello guys!\n\nI need some help to find out whats causing this issue since the app came out! (yes!)\n\n**Nvidia app** is crashing on startup when i am launching it..the message is: There was a problem with nvidia app, reinstall drivers and reboot.\n\nI have tried\u00a0**format the whole system**, tried many drivers with ddu, still the same problem.\n\nAll drivers updated, windows updated 23H2.\n\nDoes this new app needs something else, for example a specific Visual C++ library?\n\nI dont know what to do anymore. Everything else on my system is find, playing games, browsing whatever.\n\nOnly problem is the nvidia app.",
            "I have a really weird problem.\n\n\n\n**Status:** UNRESOLVED\n\n**Computer Type:** Custom built desktop\n\n**GPU:** I have a RTX 4070 with 12GB of VRAM. I don't think it has an overclock?\n\n**CPU:** Intel i5 9600K. I think it has an overclock but I don't use it.\n\n**Motherboard:** ASRock Z390 Phantom Gaming 4. Don't know the bios sorry.\n\n**RAM:** G.Skill Aegis 32GB\n\n**PSU:** Corsair TX-M TX550M. Gold 550W\n\n**Operating System & Version:** Windows 10 Education, clean install\n\n**GPU Drivers:** 555.99, Upgrade\n\n**Description of Problem:** I have two monitors. The main one is connected via DP, the secondary HDMI. Sometimes the second one will just lose connection and go black. This happens often when I close out of a game, but it also just sometimes happens randomly while I'm using it. It's very annoying.\n\n**Troubleshooting:** I was originally going into device manager and just disabling then re-enabling the 2nd monitor, which would get it to work. However, a way easier way I've found is just simply *looking* at GeForce Experience (????). I don't even have to open it, I just have to bring focus to it and boom, my second monitor regains its connection. I don't know how to tackle this in a more permanent way. It's not a connection issue if I'm able to re-enable it via software. So what the hell?",
            "So im just curious is DSC compression is able to be used with one screen, but 4 total screens on a 4090, i vaguely remember being told that there are only so many 'lanes' that can be used for DSC, when turning it on, it makes it so you can only use 2 of the outputs instead of 3-4 at once. I have a 4k 138hz screen with DSC, but whenever i try to use anything above 120hz for it, it just 'nopes' out and wont turn on. if i turn it back to 120hz i can use the other 3 with it as well... Ive been wanting one of the new 4k 240hz oleds, but if i cant even use 138hz with it, i have a feeling this will limit the new screens as well.   \n  \nIs there any way to get 4 screens working, with a single screen using DSC? or am i out of luck?  \n  \nThat said, is there any word if a 50 series might have support for more DSC lanes?",
            "# UNSOLVED\n\n  \nDoes anyone know what/or how the Nvidia app's \"shadowplay\" works? It works perfectly fine in one game I play, but then in another I can't get it to even open with the shortcut let alone actually save clips/the last 20 minutes. I have absolutely no idea what actually could be the problem but I have tried uninstalling and reinstalling and that did not yield any result.",
            "Is it bad to mismatch DLSS .dll versions with a different frame gen version .dll ? I downloaded the 3.7 SDK but it doesn't have the Frame Gen DLL. Also is there a difference between the /dev folder and the /rel folder? I see the same file name but different size nvngx_dlss.dll file in each folder.",
            "**Status:**\u00a0UNRESOLVED\n\n**Computer Type:**\u00a0Laptop Thinkpad\n\n**Description of Problem:**\u00a0I am still using a NVIDIA Quadro K1000M Chip and since they stopped the support for that in 2020 if my infos are correct i am having problems runnig more and more of games, even ones i could run before. Mainly Civ 6 is what bothers me the most, because i was able to play for hundreds of ours and now it is now longer possible. Is there any way i can work around this?\n\n**Troubleshooting:**\u00a0I tried to find any other source to update my driver for my chip, but could not find one. would love to get any kind of advice\n\nBelow a pic from can you run it\n\nhttps://preview.redd.it/21k7oq7mkc6d1.jpeg?width=481&format=pjpg&auto=webp&s=e372cb8c6374c830e573bc0266449896765dc17f",
            "**Status:** UNRESOLVED\n\n**Computer Type:** Desktop Custom Build\n\n**GPU:** RTX 2060 Super - slight overclock from NVIDIA overlay\n\n**CPU:** AMD Ryzen 5 5600x - no overclock\n\n**Motherboard:** B450 TOMAHAWK (MS-7C02), latest bios\n\n**RAM:** Corsair Vengeance LPX 32GB (4x8GB) DDR4 2400MHz\n\n**PSU:** EVGA 600 W\n\n**Operating System & Version:** Windows 11 Build 22631, clean install\n\n**GPU Drivers:** Upgrade, but it's been an issue for a long time even after clean installs\n\n**Description of Problem:** G-Sync activation algorithm doesn't trigger when any window/overlay that doesn't hook into the game. (AMD and Intel's VRR solutions work though). \n\nThe program \"Lossless Scaling\" creates a fullscreen, unfocused window that covers the game window which remains in focus for receiving inputs. However, this disables G-Sync as soon as it's activated. This isn't a problem with the developer's program as other VRR solutions work. The reason why \"Lossless Scaling\" creates an unfocused overlay is to not be invasive as hooking into games can cause issues with multiplayer titles. Again, there is no way for any developer to activate g-sync on an unfocused window and the app NEEDS to be unfocused. \n\n**Troubleshooting:** There is no fixing this without NVIDIA doing it themselves. Tried windowed and borderless windowed. Tried turning V-Sync off in NVCP.\n\n**Overall:** G-Sync gets disabled once Lossless Scaling is overlayed over a game due to Lossless Scaling being unfocused. Drivers need to fix this because AMD and Intel VRR solutions are working while NVIDIA's G-Sync doesn't.",
            "Status: UNRESOLVED\n\nComputer Type: Desktop but don't think it matters.\nGPU: Suprim X 4090, stock\n\nGPU Drivers: Game-Ready 555.999\n\nDescription of Problem: The NVIDIA App's recording (the new Shadowplay thingy) doesn't work at all. I can get the overlay to pop up but even when the Instant replay stuff is activated I won't get the lil recording notice. Pressing the shortcut also doesn't do anything.\n\nTroubleshooting: I have tried uninstalling and reinstalling, restarted, tried different games.",
            "Operating System: **Windows 10 Pro 64-bit 19045.4412**\n\n* CPU Type: **Intel Core i5 10400**\n* GPU/VGA Type: **NVIDIA Ge Force RTX 3060 (Gigabyte)**\n* Memory Capacity: **32 GB DDR4**\n\nNot quite sure if this is a hardware issue or some issue with the newest versions of Windows 10 now that Microsoft is neglecting it. As of about 2 months ago I'm having major issues when it comes to display and graphics. For starters certain windows have started flickering fast and frequently showing the desktop behind the window. Usually only occurs when there is video being played back and happens the most with YouTube. A more recent issue that has started arising is that games I've had no problem running for years have started crashing to the desktop which causes all 3 of my monitors to cut to black and then every app on my computer will also close (mainly discord, Firefox seems to survive).\u00a0\n\nI\u2019ve tried everything including\n\n* Disabling HAGS\u00a0\n* Lowering Graphics Quality\n* Updating Drivers\n* Updating Windows\n* Removing NVIDIA Drivers with the uninstaller tool and Re-Installing\n* Changing Display Refresh rates\n* Monitoring Temps when Crashing (Normal)\n* Monitoring Usage when Crashing (Normal)\n* Disabling G/V Sync\n\nI have not made any crazy changes or installed any new software that would lead to any issues of this nature. At the time of the crashes nothing more is running other than Discord Firefox the Game itself and Steam. I have 3 monitors, my main running at 240hz without GSYNC and 2 secondary monitors both running 60hz.\u00a0\n\nUpon crashing all 3 monitors go black for about 15 to 45 seconds before sending me back to the desktop and any running app will also crash. It's happening with most steam games now including bare minimum 500mb pixel games. It seems to become more and more unusable by the day to the point where i'm debating wiping and reinstalling windows. thank you for any help!",
            "**SOLVED**\n\nI have an HP Omen laptop and I was using it with an external monitor over HDMI. Today it just stopped working. I tried uninstalling the drivers using the driver uninstall utility and reinstall them back (both in safe mode and standard) but no luck. Monitor just displays \"No signal\" message. Laptop monitor and the GPU work normal.\n\nHowever, the monitor seems to be connected in Nvidia utility (I can see its model number so it is recognized by my laptop properly) and the laptop makes the connection noise whenever I plug or unplug the monitor. I changed monitor order, frame rate settings, resolution and various other stuff but nothing worked. I also tried the monitor and the cable with a different laptop and they work fine, so it is definitely the laptop.\n\nAny ideas?\n\nOMEN by HP Laptop ck0088TX\n\nGPU: 3080 16gb\n\nCPU: Intel\u00ae Core\u2122 i7-11800H\n\nRAM: 32 GB DDR4-3200 MHz RAM\n\nOS: Windows 11 Home\n\nlatest Nvidia drivers are installed",
            "How good is this \"Undervolting\" Method really? [https://www.youtube.com/watch?v=OkHYBp-DXQ0](https://www.youtube.com/watch?v=OkHYBp-DXQ0)\n\nIs it actually any good or just a crude way I should keep my hands off? I just got a 4080 and kinda want to undervolt a little, but with not so much trial and error testing.",
            "**Status:** UNRESOLVED\n\n**Computer Type:**  \nAMD 7600x  \nAsus B650E-F  \nAlienware AW3418DW (via DisplayPort)\n\n**GPU:**  \nAsus TUF GTX 3070 OC\n\n**GPU Drivers:**  \n555.99 w/GeForce Experience installed\n\n**Description of Problem:**  \nI am experiencing bad lag/slow down in Windows when alt-tabbed out of games. This problem is resolved by disabling G-Sync, which is enabled only for full screen mode. The problem only started when I upgraded my PC from an intel 6700k system, which used the same GPU and monitor.\n\nIt will happen even if the game I'm running is borderless windowed or windowed mode. Windows becomes laggy, actions like moving/minimising/maximising windows are sluggish. As soon as G-Sync is disabled, even while the game is running, all background operations are completely normal.\n\n**Trouble Shooting:**\n\n* Disabling the 120hz \"overclock\" on the monitor which boosts it from 100hz, as I read that can cause problems with G-Sync\n* Updating all GPU and AMD chipset drivers\n* Disabling iGPU in BIOS\n* Disconnecting secondary screen (TV) from HDMI port\n* Clean installing Windows multiple times\n* Running with as many background apps disabled as I can  \n\n\nPretty much out of ideas at this point. Thanks in advance.",
            "**Status:**\u00a0UNRESOLVED\n\n**Computer Type:** Desktop, Acer Nitro N50.\n\n**GPU:**\u00a0NVIDIA GeForce GTX 1650.\n\n**CPU:**\u00a012th Gen Intel(R) Core(TM) i5-12400F, not overclocked.\n\n**Motherboard:**\u00a0Acer Nitro N50-640\n\n**RAM:**\u00a0Corsair Vengeance LPX 32 GB DDR4 3200MHz, not overclocked.\n\n**PSU:**\u00a0500 W, can't find the model.\n\n**Operating System & Version:**\u00a0Windows 11, had since buying PC.\n\n**GPU Drivers:** NVIDIA 32.0.15.5599\n\n**Description of Problem:**\u00a0My cooling system for my GPU is broken, and I can't find out what would be a good replacement.\n\n**Troubleshooting**: I've tried everything to make the fan spin, but it refuses to. If I take it out it feels normal and is extremely easy to spin. It doesn't spin on startup, and I've tried several times to make it start through changing the fan curve with multiple services (Afterburner, etc.). Does anyone know a cooling system replacement that I could buy, that works with my gpu?",
            "Status: UNRESOLVED\n\nComputer Type: Desktop, Custom Built\n\nGPU: RTX 4090\n\nCPU: i7 13900k stock\n\nMotherboard: MSI MAG Z790 TOMAHAWK WIFI latest bios\n\nRAM: G.Skill Trident Z5 Black 32GB DDR5 6000MHz CL36 Dual Channel Kit\n\nPSU: Seasonic VERTEX GX, 80+ Gold, 1200W\n\nOperating System & Version: Windows 11 22631.3737 \n\nGPU Drivers: 555.99 install through geforce exprerience\n\nDescription of Problem: Computer crashes when recording videos using GeForce Experience in-game overlay. So far happens randomly in Elden Ring.\n\nTroubleshooting: Driver reinstall, pc reboot",
            "RTX video enhancement stopped working for me. It does not work in Edge and Firefox browser. I reinstalled the drivers, did not make any difference.\n\n  \nGPU: RTX 4070",
            "Why did nvidia release the Super versions this late?",
            "**Status:**\u00a0UNRESOLVED\n\n**Computer Type:**\u00a0Desktop, Predator\n\n**GPU:**\u00a0NVIDIA GeForce RTX 3060 Ti\n\n**CPU:**\u00a012th Gen Intel(R) Core(TM) i7-12700F   2.10 GHz\n\n**Motherboard:**\u00a064-bit operating system, x64-based processor\n\n**RAM:**\u00a016.0 GB (15.8 GB usable)\n\n**PSU:**\u00a0Not sure where this is.\n\n**Operating System & Version:**\u00a0Windows 11 Home\n\n**GPU Drivers:**\u00a0Provide the current GPU driver installed and if it\u2019s clean install or upgrade, e.g. 376.33, clean install\n\n**Description of Problem:**\u00a0My GeForce Experience app keeps loosing my games and cannot search for driver updates. Each time I reinstall the App it locates my games and driver updates, but after a day its back to being unable to find anything. Additionally, when attempting to scan for games after the reinstall, it tells me \"Scanning failed\". Its as if the GeForce Experience app is disconnected from my computer.\n\n**Troubleshooting:**\u00a0reinstalled the software 10x",
            "How exactly am I meant to output usable footage from Shadowplay when using HDR? No matter if I use GeForce Experience or the Nvidia app, and no matter if I use HEVC or AV1 output it always ends up an oversaturated mess that is impossible to clean up into usable footage in HDR, which becomes even more exaggerated trying to convert it to SDR to send on Discord or the likes",
            "Need help with the Game Filter feature. For some reason the sliders doesn't move around the way I want it to.\n\nfor example, if I want to move a slider to 10 from 0, I would expect it to move in at least 1 increment, but instead it just skips some numbers, so it would go 0 > 2 > 5 > 9 > 11. I tried moving it with arrow keys and mouse, but I can't seem to get it to the number I want. On some slider it even goes into decimals and I can't get it to go back down to 0, instead it would go to 0.5 at the closest.\n\nIs there a button to make the sliders more sensitive? I'm trying to copy another person's settings and I just can't get the sliders to the same positions.",
            "Please help me \n\nShort Version: One day the option to use Shadowplay on desktop disappeared.\n\nLong version: I can no longer use desktop capture option in my NVIDIA beta app. I didn't switch devices nor I changed settings. Prior to that the option used to be amazing and I never had any problems with it. \n\nSpecs:\n\n- NVIDIA RTX 3050\n\n- Intel Core i5-11400H\n\n- 16 GB RAM\n\n- Windows 11 \n\n- SSD 500 GB (app is installed on this disk)\n\n- HDD 1TB (files are stored on this disk)\n\nIf you need any additional info let me know, I will provide it if it will contribute into solving the problem",
            "**Short version:** If the GPU is doing the scaling, why does the display matter?\n\n\n\n**Long version:** I got a new monitor a while ago, and I noticed the options for custom resolutions and integer scaling weren't available. They were available on my previous monitor, and they're still available on the TV I have connected, as shown here (TV on left, new monitor on right):\n\nhttps://preview.redd.it/tdcedl4sq34d1.png?width=1294&format=png&auto=webp&s=3df6e38d36a6061e21a8190ce377258d0eb5b6b0\n\n  \nI reached out to Nvidia support about the issue and they said it was an issue with the monitor.\n\n>There seems to be a limitation with the monitor itself that restricts integer scaling and custom resolution options.\u00a0  \n  \nWhile the technology for integer scaling and custom resolutions exists,the LG UltraGear 45GR75DC-B might not have the necessary hardware or firmware support to enable them. This could be a cost-cutting measure or a compatibility decision by LG.\u00a0\n\nThey suggested I reach out to LG support to see if they had any suggestions, so I did. They said basically the same thing.\n\n>I have reviewed the information available on the LG website, and there do not appear to be any firmware updates that could potentially unlock these capabilities.  \n  \nWhile we strive to provide our customers with the most feature-rich and customizable products, sometimes compromises have to be made during the design and manufacturing process. In this case, it seems that the integer scaling and custom resolution options are simply not supported by your LG UltraGear 45GR75DC-B monitor.\n\nI was about to leave it at that, but then I noticed it was configured to perform scaling on the GPU. Wouldn't that mean the GPU does the scaling and outputs at a resolution the monitor could display? Why does the monitor make a difference? I went back to Nvidia to ask about this, but they just reiterated what had already been established.\n\n>So as the monitor do not support this features this is why the options are not available in Nvidia control panel itself.\u00a0This is a limitation from the monitor end itself.\n\nIt felt like I was hitting a wall with that kind of response, so I figured I'd ask here. If the scaling is being done on the GPU, why does the display make a difference?",
            "UNSOLVED\n\nPC: Desktop, RTX 4080 16 GB, Ryzen 9 5900X , 64GB ram (4x16), Windows 10 Pro 22H2 19045.4412\n\nMonitors: 2x LG Ultragear 27\" 1440p connected with DP 144Hz (tested at \"59.951\" Hz and \"143.973\" Hz) tried both  8-bit SDR and 10-bit HDR modes\n\nDrivers: 552.22 and 555.85\n\nProblem: \n\nI recently upgraded from an AMD RX 6900 XT to an RTX 4080. Or, at least I thought it was supposed to be an upgrade. \n\nI'm trying to run Premiere Pro on dual 27\" 1440p monitors with windows scaling set to 125% - But whenever I move parts of the premiere interface to the second monitor, the software performs very badly - for example trying to mute a track during video playback can take about 10 seconds to register. Or, scrubbing through the timeline by clicking/dragging jumps around at about 1FPS. It all seems to work smoothly if I limit the entire program to 1 monitor, but this isn't enough space to get work done. It also seems to work ok if I set windows scaling to 100% - but then the interface is too small to read text. \n\nEverything seemed to work great on the AMD card, so I'm getting really frustrated. Actually AMD had a compatible problem when the I was using 2 unmatched monitors which were set to different frame rates and different scaling settings, but they fixed that with a driver update. Currently, I'm using 2 copies of the same monitor with matched settings. \n\nDoes anybody know a workaround so I can get back to work? I considered buying a single ultrawide monitor, but I have some specific size limitations and I can't find one the right size that isn't too curved, or has other problems. Or a lot of monitors have a higher DPI, and I think more scaling will make the problem worse. \n\nI've tried a lot of premiere pro troubleshooting steps, but I think this is a driver problem with Nvidia since the performance is specifically limited to when I spread premiere over 2 monitors, and Windows \"scale and layout\" setting is set above 100%. \n\nDoes anybody know anything about this? I've tried the current 555.85 drivers but rolled back to the studio driver 552.22 due to some other problems in premiere - both have the same issue.",
            "Status: Unresolved\n\nPC, custom built\n\nGPU MSI RTX 3060 12gb, no overclock\n\nCPU i5-10600kf no overclock\n\nMB ASRock B460M-ITXac\n\nRAM Crucial 32GB (16x2) 3200(limited by motherboard to 2660mhz)\n\ncan't recall off the top of my head the other specs besides Windows 10.\n\nthe issue is none of the numbers in the Performance Overlay change except for CPU Utilization and all i've done is update the drivers. i've uninstalled and reinstalled GeForce Experience, turned my pc off and back on, and the problem persists.\n\n&#x200B;\n\nhttps://preview.redd.it/tfmj15ium97d1.png?width=323&format=png&auto=webp&s=2a13ccf72ea3574744a462f4276dc0451fc899d1",
            "When will RTX HDR be usable on multi display systems without any workarounds?",
            "**Status :**\u00a0UNRESOLVED\n\n**Computer :**\u00a0Desktop Custom Build\n\n**GPU :**\u00a0MSI Nvidia RTX 3060 ti Ventus X3 OC 8 Go\n\n**CPU :**\u00a0Intel Core i5-10400F 2.9 Hz\n\n**MBD :**\u00a0Gigabyte Z590 UD AC DDR4 11th\n\n**RAM:**\u00a0HyperX PCB 16 Go (2x8) 3200 Hz Fury Kingston\n\n**PSU:**\u00a0Xigmatek 750 W\n\n**OS :**\u00a0Windows 10 pro 22H2\n\n**GPU Drivers:**\u00a0regularly updated through GeForce experience\n\n**Description of Problem:**\u00a0For 2 days, RDR2 have been crashing a few minutes into the game. I have to manually restart the computer.\n\nI've been playing with this pc and this game for several months like a dream. I've had this computer for 2 or 3 years.\n\nSince I live in a hot country, I suspected the temperature, but the GPU never went over 65\u00b0C and it's not that hot outside. I still direct my fan towards the pc as soon as summer starts. I installed latest drivers for NVidia, Riva and MSI afterburner. I tried changing the power parameters of windows from regular to high performance. The crashes persist.\n\nI'm not sure if the problem is coming from Nvidia, or its latest drivers, or from Windows? \n\nI'm not very tech savvy, just enough to fix problems as they come in my work as graphic designer and in my gaming, so I would appreciate simple explanations.\n\n**Additional questions** :   \n- Is it necessary to always install the latest game ready drivers? I work a lot and I don't have much time for gaming, so I only play 2 or 3 games a year, most of the games i see in the Game ready drivers I have no intention of playing.\n\n- Is my build ok ? (personally, I'm satisfied, I tend to play games that are a bit old and not that demanding, but I wouldn't mind upgrading when I get some cash)",
            "**Status:**\u00a0UNRESOLVED\n\n**Computer Type:**\u00a0Desktop, custom built\n\n**GPU:**\u00a0Asus TUF RTX 3060 TI V2 OC stock non-oc\n\n**CPU:**\u00a0Intel Core i7 4790K non-oc stock \n\n**Motherboard:**\u00a0Asus Maximus Hero VII latest BIOS 3201 \n\n**RAM:**\u00a016 GByte G.Skill Sniper DDR3-2400 11-13-13-31 2T non-oc\n\n**PSU:**\u00a0Enermax Platimax 500W\u00a0\n\n**Operating System & Version:**\u00a0Windows 11, update from Windows 10 via inplace-upgrade, latest version\n\n**GPU Drivers:**\u00a0555.84, clean install\n\n**Description of Problem:**\u00a0After the update there are sound-hiccups when playing games (Assassin's Creed: Mirage, CoD: WW2, Prince of Persia etc). The sound stops, but there is no graphical lag, then it comes back again. Connected via hdmi 2.1-cable to a Denon AVR X1700, therefore using Nvidia High Definition Audio. \n\n**Troubleshooting:**\u00a0\n\n- Tried de- and reinstalling Dolby Access for Atmos\n\n- Turning off audio enhancements\n\n- Turning off exclusive control of audio device\n\n- Setting the speakers to 24 bit, 192000hz\n\nIs it maybe due to the official incompatibility of my cpu in Windows 11? Didn't have these problems with Windows 10.  Thanks for the help, dear redditors :)",
            " **Status:** UNRESOLVED\n\n**Computer Type:** Laptop, Lenovo \n\n**GPU:** Geforce 920m\n\n**GPU Drivers:** 425.31 updated from nvidia website.\n\n**Description of Problem:** Steam game crashes after certain point, It is suggesting me to upgrade to version 516.25 but from what I have seen it doesn't seem to exist for my version of my GPU. Hoping to confirm whether this is the case or there is a hope for me to get this version and maybe resolve this problem.   \n\n\n**Trouble Shooting:** I have tried finding the latest version of driver I need.\n\n&#x200B;\n\nhttps://preview.redd.it/3b1t02jqjq4d1.png?width=435&format=png&auto=webp&s=5798e42c8831a2e9fc03963a1a7db1f947c3e310",
            "Does anyone know how the 4060 Ti and 4070 Super scale with power limits below 100%? I'm looking for something like these:\n\nhttps://www.techpowerup.com/forums/attachments/4070-vs-4070ti-power-scaling-png.346803/\n\nhttps://www.techpowerup.com/forums/attachments/1665525656358-png.265076/\n\nI'm also looking for these same graphs (or tables) with the 4060 Ti or 4070 series cards undervolted or otherwise tuned, if you've come across them or would be willing to do your own testing -- it doesn't seem like something many are interested in so thanks to anyone who'd be willing.\n\n^(I wish reviewers would look at maximizing efficiency in addition to performance.)",
            "[deleted]",
            "[https://imgur.com/a/9eoLCWj](https://imgur.com/a/9eoLCWj)  \nnvidia screenshots in game, half of them are coming out to be blacked out like in the images attached, please help.   \nalso not specific to a game, facing this in several different ones, and its been months. updated drivers and everything.",
            " \n\n**UNRESOLVED**\n\n**Computer Type:** Desktop, custom built\n\n**GPU:** ROG Strix 4090 OC edition, 24GB VRAM\n\n**CPU:** AMD 7800X3D, no overclock\n\n**Motherboard:** Asus PRIME X670E-PRO WIFI, latest BIOS (2613)\n\n**RAM:** Corsair Dominator 32GB (2x16GB) DDR5 6000MHz, XMP enabled, no overclock\n\n**PSU:** MSI A1000G PCIE5 1000 W 80+ Gold\n\n**Operating System & Version:** Windows 11 clean install\n\n**GPU Drivers:** driver 555.85, clean install\n\n**Description of Problem:** When I am in full screen of my main monitor, gaming or in this case watching a video on discord, my screen will flicker to applications on the desktop behind the app. In the discord video, the flicker was a lot more frequent than when I am playing a game. But it will flicker to anything behind what is in Fullscreen.\n\n**Troubleshooting:** I have tried using a new DisplayPort cable for the graphics card, the only way I can make it stop is by minimizing apps behind, or moving them to another monitor.   \n\n\nLink to video of issue: [https://imgur.com/a/kL9cElF](https://imgur.com/a/kL9cElF)",
            "Oddity: When I wrote the post (and typically), my left monitor is in portrait mode and my right monitor is in desktop mode. When I switch the left monitor to desktop mode, the black area on the right monitor takes up the full screen, so I see nothing (except the mouse icon).",
            "&#x200B;\n\nhttps://preview.redd.it/d3ni8xds9y3d1.jpeg?width=2265&format=pjpg&auto=webp&s=937571c8c871cec36f0266a70a1b1586b3d6b73b",
            "https://preview.redd.it/lh0sufe3ay3d1.jpeg?width=2268&format=pjpg&auto=webp&s=82fb476025d4c62619b3d7a47565af59b7f9270c",
            "&#x200B;\n\nhttps://preview.redd.it/e95wx0ow9y3d1.jpeg?width=2258&format=pjpg&auto=webp&s=b3f6cf87babbf8fad479377b4b8a23defba94908",
            "This sorted itself out after I tried the laptop with a TV (This also worked), then took it back to monitor. I have no idea how it is fixed though... My guess is for some reason a low level setting was switched and plugging the laptop to another device reset that back.",
            "Just deshroud it and put on a regular fan. Use PWM converter for the fan connection.",
            "You must use your native resolution for it to work. No DLSDR resolutions.",
            "Your problem is very evident. Gpu is *ridiculously* throttling. It should average 1600mhz more or less, but it only manages sub 700mhz instead.",
            "I am not using any of that.",
            "Yes I noticed this problem but am pretty new to hardware tweaking through software, do you have any suggestions on how to solve this problem? I noticed my GPU temp was a bit high but after research it looks like this to be expected out of laptop GPU's. Thank you for your reply!",
            "Again, your gpu is throttling hard due to temperature. There's an *extremely* serious problem with your cooling. You have to take the laptop apart and fix it yourself, or take it to a technician. Might be paste, bad chip contact with the heatsink.\n\nCompare your unit with a perfectly [functional one](https://www.youtube.com/live/T1b_sm0EW0E&t=435s). Clock speeds, temps, power draw, performance... All as it should be."
        ]
    },
    "ASUS 4070 Ti Super Dual Oc 16GB Questions": {
        "title": "ASUS 4070 Ti Super Dual Oc 16GB Questions",
        "score": 31,
        "url": "https://www.reddit.com/gallery/1dlqpa1",
        "content": "I have this GPU myself but I need more answers to it, there's barely any reviews to it online. I'm trying to also figure out if my temps are good or not. On Most games at 1440p max settings I gather around 68-73c depending on the game with a slightly adjusted default fan curve. Case fans are artic p12 max even on the radiator at normal speed. My Cpu is 7600x. Is it causing a bottleneck?? I need a pc wizards advice because I watch YouTubers benchmarks with higher end cards with temps around 58-64 I'm over here stressing like why is mine cooking to 70's? I should also mention most of my games I don't like to use DLSS unless I have to for extra frames.",
        "num_comments": 20,
        "comments": [
            "https://preview.redd.it/o31h0uhc938d1.jpeg?width=1179&format=pjpg&auto=webp&s=66879ab7745375bd3c17544556baa9c3ae47cc2d\n\nYou\u2019re good. You have a small compact 2 fan card. Nvidia default limit is 83\\*. You can raise that to 90\\* in Afterburner if you want.",
            "No to every single question. \n\nIf you want lower temps increase the Fan speed.\nIf you dont want louder noise dont increase fan speed. \n\nNo its not a bottleneck for godsake stop watching weird yt videos benchmark videos. Watch Hardware unboxed. Im not even sure why you are watching videos about your card even though you have it installed. \n\n\nGPU load  is higher without dlss thus higher temp.",
            "OC BIOS or silent? There is a switch near the power connector on top.",
            "No 70s is fine for a 2 fan. You are fine! You are pushing alot of power through this. But the issue is some people don't realize the asus dual has 2 models. One is the evo which has sligjtly smaller fans and a smaller heatsink and the other is the none evo which has better temps due to 100mm fans instead of 90mm and has a bigger hestsink with better fins. Sneaky little move from asus it's why I always get the white one with duals because none of the whites are evos. Bit dude 70c is perfectly fine. My 4070 super dual is around 67 to 70c under full load.\n\nAlap the 7600x is a fine cpu to use. You may see a slightly bottleneck at 1080p but it's not much and in 1440p and 4k you won't really notice anything. I say it's not really a bottleneck. Enjoy man nothing wrong with your card at all. Actually the asus dual is the only dual fan card say performs at 3 fan card level. Its the top of the list for duals and the performnace is so close to 3 fans it doesn't even matter.",
            "Except for strix, Asus hardware does run a little hotter. But those temps are fine. 7600x is more than capable of handling 4070ti super.",
            ">  around 68-73c \n\ncompletely fine  \n\n> My Cpu is 7600x. Is it causing a bottleneck??   \n\njust check the GPU utilization while playing, if its near 100% then no bottleneck.",
            "73c and even a little higher are completely fine temps.Only +80c temps under load are where you should start to get worried. 5 7600x is very good for the 4070 ti super in 1440p. In 1080p the 4070 ti super is slightly bottlenecked by the 7600x.",
            "Temps are normal. Your CPU is not bottlenecking the card. \n\nIf you're concerned run a slightly more aggressive fan curve. Have it ramp up to 70% fan speed at 50'c, and 90% at 70'c. You'll find the more quickly you start cooling it off, the lower the overall temps will be.\n\nEdit. If you're not running  intake fans, or intake fans are going across a rad, your temps will be higher.",
            "Honestly the biggest problem seems to be that the gpu isn't in the top slot, I am not 100% sure",
            "One of the PC I built recently has a Asus dual 4070S paired with a 7600X on a 1440p monitor and runs 65-70C, when compared to a PC I built a while back with a MSI trio 4070 Ti with a 7700X on 3440x1440 ultra wide runs 66-70C, they perform nearly identical and run quiet and relatively cool.  The dual is tiny in comparison to the Ti.  The difference is only 50 watts between the 4070S and the Ti/Ti S, hardly enough to really matter.\n\nThere are two bios on the cards you use a switch to set.  On the dual if you're worried just use the performance one.  My testing was done on silent bios on 4070S dual.\n\nThe 7600 is not a bottleneck, but if you want to improve its performance activate PBO and use core offset in the BIOS, just set -20 all core and you should be fine, it'll basically keep the CPU at max speed forever with any half ass cooler like that.  And of course make sure your RAM speed is set properly.",
            "I have the same cpu and gpu combo. Just my card is gigabyte 3 fan card.\n\nyour temp seems normal compared to a 2 fan card.\n\nWhat's important is check your gpu hotspot and vram temp. they are crucial.",
            "Before comparing your temps to others you need to check room temperature in both scenarios. That's your starting point. If your room is 5 degrees hotter than a reviewer's \"controlled temperature room\", your GPU will also be 5 degrees hotter.\n\nRegardless of what I said, your results are absolutely fine and you won't damage or reduce your cards lifespan.\nYour CPU won't bottleneck you at 1440p.\n\nJust enjoy your machine :)",
            "This is no way near compact 2 fan regular GPU. It is 267x134x51 card and 1+ kilo. Heatsink is massive and fans are good, it is on par with some triple fans cards like gigabyte.",
            "the tuf version is also quite low on temps.",
            "is it really that easy?  i got a 5600x and also got the most time 100% on the gpu. so a 5800x would bring me more fps?",
            "I misspoke then. Tiny is relative.\n\nWe had the 4060ti Dual that uses the same cooler as OP\u2019s and it is tiny compared to my EVGA 3070 ftw3 ultra, EVGA 3080 ftw3 ultra, 4080 Super FE my friends 3090 FE.\n\nIt\u2019s more closely sized to my daughters EVGA 3060 xc3 and my girlfriends 4070 Super  FE (which also runs around 73*)",
            "Yes, it's that simple. If GPU is fully utilized then upgrading CPU won't do much, except maybe improve 0.1% lows a tiny bit.\n\nIn your case, 5800X wouldn't bring you more FPS even if you weren't GPU limited, [because they have practically the same performance in games](https://tpucdn.com/review/amd-ryzen-5-5600x/images/relative-performance-games-1920-1080.png). If you wanted actual performance uplift in CPU-limited scenarios then you'd need to upgrade to 5700X3D or 5800X3D, but even then it would only be like 20-30% at best.",
            "No it isn't, first this won't work with vsync on, second, random testing area is not a great check as it could be very non representative off the most demanding areas in the game.\n\nIt means nothing if u test in an area where there is not much CPU demand.\n\nThis works without vsync and testing on an average most CPU demanding scenario in the game.",
            "Sadly, but this is a trick on picture only. 4060Ti Dual is way smaller and less effective than 4070 Ti Super... I've noticed that in 4070 and 4070 Super duals, while they are almost identical Asus \"nerfed\" cooling solution on Super version. Size is only one parameter, the main parts are heatpipes amount and their diameter than heatsink weight. 7800XT Red Devil is on par with 4080 Noctua in terms of cooling power while being 52mm card only, they just put there 7x6mm heatpipes.\n\nIf you are curious as I am about coolers here are reviews for dual versions:\n\n[4060 Ti Dual review](https://www.techpowerup.com/review/asus-geforce-rtx-4060-ti-dual-oc/2.html)\n\n[4070 Dual review](https://www.techpowerup.com/review/asus-geforce-rtx-4070-dual/3.html)\n\n[4070 Super Dual review](https://www.techpowerup.com/review/asus-geforce-rtx-4070-super-dual/3.html)\n\nBut yes, I have 72-73C on gigabyte 3070 too.. The only way for OP will be to undervolt card.",
            "Its better to look at weight, rather than length. There are some 267 mm long cards with a weight of 550 g and only 2 heatpipes, and others that are 240~ mm long with a weight of 1 kg, with 4-5 heatpipes."
        ]
    },
    "HUB - When Are Next-Gen GPUs Launching? Buy Now or Wait? - June GPU Pricing Update": {
        "title": "HUB - When Are Next-Gen GPUs Launching? Buy Now or Wait? - June GPU Pricing Update",
        "score": 8,
        "url": "https://youtu.be/ZnjO1spYcxg",
        "content": "",
        "num_comments": 16,
        "comments": [
            "5090 will cost $1m give me upvotes pls",
            "TL DW: Wait for 5000 series if you have 3070, 3080 or 3090.",
            "The hub community is delusional af",
            "im excited for the rtx 5080xtx ti super gre",
            "I hope they learned some lessons. No price increases for 5000 series, and take care of supply. I know that\u2019s not a strong economic approach for them, but the corporate greed is insane.",
            "Gonna use my Nvidia dividends to get that 5090 when it drops.",
            "Rtx 50 series \n\n2999$ rtx 5080\n1999$ rtx 5070",
            "Upvoted",
            "It's a circlejerk for AMD fanboys. This is where they gather and give a pat on their backs on how AMD is destroying Nvidia in consumer GPU space.",
            "Haha when will you sell?",
            "5080 $999 with 4090~ level performance and 250w tdp would do me nicely",
            "5090, mega shaved down die, \u00a32,999\n\n5080, 6 memory packages 192bit bus\n\n5070, 8GB memory\n\n5060, perfect for 1080p, PCIe x4 link\n\n**The more you buy, the more you save**",
            "You mean bots.",
            "Might sell a tiny bit on the next run up but I bought it many years ago when it was like $25 a share. the dividends it pays in the meantime are small but it adds up \ud83d\ude01"
        ]
    },
    "Jensen Huang recently Hinted what DLSS 4 could bring. ": {
        "title": "Jensen Huang recently Hinted what DLSS 4 could bring. ",
        "score": 828,
        "url": "https://i.redd.it/nz2lpsanqu7d1.png",
        "content": "",
        "num_comments": 345,
        "comments": [
            "DLSS 4.0 double your VRAM for free...",
            "AMD better just put all their effort into 4-dimensional V-Cache at this point cause they ain't gonna win the arms race on the GPU side anymore",
            "This is how we\u2018ll achieve photorealistic graphics. We\u2018ll just replace so much with AI that we can put all the compute into lighting and textures.",
            "As a 3D artist I find nvidias so far ahead of the curve on things, Instant-NGP was nuts, Neuralangelo was even better, over in their Omniverse platform I couldn't believe how amazing Audio2Face was when I first started playing around with it.\n\nOmniverse in and of itself is just an absolute gem for development. One of the professors at the local college here was having an insecure bitchfit about his technical level, he was looking at my screen and gave a patronizing \"We are focusing on realtime, that looks nice but its useless\" while I was working on a small pyro sim. He lost his shit when I pointed out it was realtime. Dork.\n\nGod damn I love Omniverse.",
            "In Nvidia future, game plays you!",
            "AI does not mean DLSS. He\u2019s probably not talking about DLSS",
            "Dlss 4 is advanced ray reconstruction, 5000 series will have in built hardware for denoizer .\n\n4 may include better framegen and upscaling algorithm update .\n\nDlss 4.5 will be what you are talking about .",
            "Yeah, they've discussed AI texture decompression before.\n\nThat's one of a few things that they've been working on.",
            "\"You can use the PC as an AI assistant to help you game\"\n\nTo help... Game? People need help to play? Isn't the purpose of the game just to play it? What kind of help would be needed to play...",
            "Wake me up when there's DLSS that will generate a current gen Morrowind.",
            "9 sentences, AI word exists 8 times...\n\nFuture does not look good.",
            "How the heck can they AI increase the polygon count of objects? Is that even possible? For textures I completely understand, but they specified objects as well",
            "Seriously though if they do texture upscaling it's going to be 8GB Nvidia GPUs until like 2035",
            "Game use AI, game made using AI, hardware use AI, last step is to place AI robot to play game for you.",
            "nVidia becomes nVidAi. \ud83d\udc7e",
            "So this means in the future i don't have to download HDMOD for my Heroes of Might and Magic 3? :)",
            "Nvidia will do literally anything except put more VRAM on cards.\n\nDevelopers will do literally anything except optimise their games.",
            "Tutorial : How to reduce VRAM and convince your customers it's the way it's meant to be played.",
            "I didn\u2019t even consider GeForce being the biggest gaming brand but with the PC community being larger than any console community it makes sense",
            "If ever my 3080 needs replaced in the next year or 3, I'll buy AMD just out of spite",
            "Didn\u2019t someone already demo a game where it was almost entirely AI generated? As in, instead of making an actual level with geometry and textures, the AI just generate what you see in (near-) realtime?\n\nEdit: Demo as in \u201cdemonstrate\u201d, not any actual product being shown. It was just a proof of concept IIRC.",
            "In the future, AI will play games for you too!",
            "I just upgraded from a 3090 ti to a 4090 and finally had a chance to test out frame generation. It's significantly exceeded my expectations, which were quite low. I was really unimpressed by DLSS in Cyberpunk, and not sold on AI upscaling in any application up to that point, but after using DLSS + framegen in Horizon Forbidden West I was blown away by how smooth and clear that game runs now.",
            "I'm still waiting for raytracing to become viable tbh",
            "* Fake resolution.\n* Fake frames.\n* Fake raytracing.\n* And now fake games.\n\nThanks NVIDIA.",
            "Only available on 50XX.",
            "Only for 50 series dlss 4, frame gen 2.0, etc... Just like 40 series.",
            "I wonder if they could make old games look better with just DLSS... and you don't need anything just a GPU that supports it, enable it and play your favorite game.",
            "I just hope game will not look different for everyone at same details.\nImagine aone guide for game telling you to find something that contains some specific looking texture and it looks totally different on your side because AI hallucinated.",
            "I was hoping they would follow up ray tracing with a system to properly simulate non rigid materials like fabric and hair.",
            "\"How to justify 8 Gb VRAM\"",
            "Sick",
            "Can't wait for Intel to catch up, they've got the funds to do it.",
            "VRAM Generation!",
            "nice, looks like i wont even have to waste time playing anymore and let my pc do all the gaming in its own. win win",
            "Not liking this direction, instead of better tools for asset optimization we get realistic games that are shit in motion and ok when still. Gimme optimized assets in native resolution with smooth motion, plus is this tech a requirement for studios to make their games realistic?",
            "fixing low res textures gonna be big feature. it better work on 4090!",
            "Nvidia will make amazing software so that they can sell cheaper hardware at the same or higher pricepoint.",
            "Every time you look elsewhere the world changes",
            "Nvidia supports Pixar\u2019s USD description standard. If that was adopted by developers then it would also help with the process. It would be like feeding in PlayStation 1 games with blocky polygons and textures and churning out Alan Wake 2 or Hellblade 2 or something",
            "Great, Leroy Jenkins or is it just a long term AI character?",
            "At what point does DLSS just make the game?",
            "As someone who works in and plays around with AI a lot, this is definitely the future of video game graphics. For relatively low cost you can get really close to photorealism from a more simply rendered rasterized or raytraced image.\n\nI think what could be most exciting for DLSS n is being able to choose wildly different styles for a game. Want Elden Ring to look like a N64 game? Go ahead. Photorealism? Manga? Etc etc.",
            "I just wish AI would be used for more cool things instead of as a stop gap so devs don't need to optimize their games anymore.",
            "And it will cost $5090",
            "can they finally fix shimmering textures?",
            "Translation: We are going to shove AI down your fucking throat. \n\n  \nWhy doesnt AI just play the bloody game too.",
            "Hey u/Nms-Barry  - care to add the source?  \n[https://morethanmoore.substack.com/p/q-and-a-with-nvidia-ceo-jensen-huang](https://morethanmoore.substack.com/p/q-and-a-with-nvidia-ceo-jensen-huang)",
            "I think one of the most exciting potentials is their npc tech. Being able to converse with npcs in natural language and just talk about shit in the game will be wild.",
            "How many AI's can you shove in one sentence?",
            "Does art style mean anything to these people?   I\u2019ll take the graphics in Tears of the Kingdom any day over Cyberpunk.  And are AI NPCs spewing random crap really gonna add to the experience?  If I want that I can go outside and talk to random people on the street.  When I play a game I want a curated experience from beginning to end that came from a team of humans with creative vision.  Or if I am looking for a more unpredictable experience, thats what multiplayer is for.  On the other hand if AI can somehow help with coding / optimization / eliminating bugs then sign me up.",
            "Nanite + Lumen .... Adapting them all.",
            "Awesome seeing Nvidia push technology further and further.",
            "So... i assume with all these assists the price of games will go up?",
            "Next thing you know, they'll have the opposite of that where you'll be able to see through smoke and fog because you can just set it to ignore things like that",
            "INB4 RTX 4080 has a minimum of 720p low 24fps. These tech feats are insane by itself then again them execs will force the devs to crunch time that results in bwoken gamez.",
            "Dlss 4 is only available on 5000 series. Dlss 4.5 is only available on 5070 and above. Dlss 5 is only available on 5090 and 6000 series.",
            "Ai ai ai ai ai ai ai ai ai ai ai\n\n\n\nDo nivdia fanboys get hard when they hear about ai?",
            "Definitely, I think the texture stuff was already leaked a few months ago. I think eventually, a lot of stuff in games will become AI generated. Like if devs make a forest, they can just map this location for forest and AI will fill or replicate sparse stuff to increase density. Even vocals.",
            "Does DLSS add input delay and if yes how much?\n\nAlways playing native cuz other options always feel weird for whatever reason. 240hz addict.",
            "sounds like the next generation of AI powered games will be absolutely insane i cant wait.",
            "I just want AI companions that can play well and talk in free form dialogs",
            "I just want a raw power gpu to max 4k out with out using any stupid dlss trickery. I want full visuals not eye tricks. 5090 better have a lot of raw power",
            "Did they hire a bunch of people from the 90's/00's demo scene??",
            "Think about this for a second. We can generate textures..... These game Devs/pubs are gonna be smashing that button like crazy for those sweet microtransactions.\n\nThis shit is just going to help accelerate that death and inevitable crash of the triple A market.",
            "DLSS 4 needs to incorporate reprojection, that\u2019s the key to lag free frame generation. With good enough frame gen you could turn 60fps into 240fps without any mouse lag",
            "They can upscale the final output. I see no limitation to texture upscale before scene render, and then an extra upscale on the final output",
            "Tbh I can't wait to use G-Assist.",
            "I want this technology for old console games remasters",
            "Gotta save the vram for the professional market. If devs actually implement this could be a game changer.",
            "plz be available on rtx serie 4xxx",
            "Nah ill wait DLSS 5",
            "I\u2019d rather they focus on improving the current implementation to reduce artifacts",
            "I am really hoping the 50 series (besides the 90) is a marginal improvement because if its crazy il get bad fomo lmao.",
            "EA games: [We're sad that we're closing down studios A-Z. But We'd like to take a moment to introduce our new dev, art, writing and QA teams!](https://i.imgur.com/YOLY8pF.png)\n\nTHE MORE YOU BUY, THE MORE YOU SAVE.",
            "That's insane but I gauruntee dlss4 will be on the 4000 series.\n\nThat last statement can't wait to bring it to more people tells me we should be able to access it on at least this generation and the next one.",
            "DLSS 20 in a few decades will just Book-of-Genesis style creat a 60 Hr RPG from a single thought, write various V1 bugs, write IGN's/PC Gamer's reviews, post them online, then automate big fixes, latches the lot. Finally it will create a whole army of cynical middle-aged gamers flesh and blood, like Weird Science, except with unattractive, greasy-haired, overweight, pale, angry gamers instead of Kelly Le Brock, It's the future, it's in the game.",
            "Nvidia bread",
            "\"Geforce is the biggest gaming brand in the wordl\" ....I don't know about that.",
            "8GB 5080 incoming.",
            "I mean they showed this last year:\n\nhttps://hothardware.com/news/nvidia-neural-texture-compression\n\nWouldn't be bad if devs started working on this or even the engineers at UE5. Same quality for way less space? Given how bad is asset compression these times with devs putting basically raw assets in games.",
            "You joke, but AI texture upscale and AI resolution upscale = more vram available = essentially \"downloading\" vram",
            "You download it with the drivers.",
            "\ud83e\udd23 \n\nLess VRAM from Nvidia was my first thought upon reading this.",
            "well with lossless scaling from Steam you can download more FPS with injecting FSR X3 into everything lol",
            "Apple already did that with their magic ram",
            "Shut up and take my money (I don't have much though).",
            "More like double your vram usage for free.\n\nI mean who cares, it's not like they're selling a card for over 300 that already shits the bed in some games over vram right?",
            "Imagine how much VRAM DLSS 4.x will use when RT + FG + RR already uses a lot more.\n\nIt would seem like 5070 should start 16GB, alas it probably will still start at 12GB.",
            "The less vram you buy the more you save?",
            "So devs can get even lazier with their optimization while having performance stay the same.",
            "I fear that these improvements will lead to rapid development and more tolerance of early access quality releases rather than making the latest titles accessible to a wider audience.\n\nTake Gray Zone Warfare for example, there's two camps, people who value input latency because it's a competitive first person shooter, and people who value having a frame rate over 30 most of the time. It is CARRIED by frame generation, and the quality (and input latency) suffers as a result.\n\nIt's a great game and I'm glad frame gen (I had to use FSR because I'm using the very ancient 3080 RTX /s) is an option so I can render it smoothly, but it's concerning to say the least.\n\nEdit: added sarcasm tag for those who couldn't tell.",
            "Check their recent data leak.\n\nRdan4 2025-2026\n\nRdan5 2027\n\nBoth targetting less than 4090 performance.",
            "They're going with MCM designs and bringing that to consumers, instead of just data centers and HPC.",
            "Actually we'll also put AI into lighting and textures.\n\nSome of the real-time AI re-lighting experiments on older games already look near photoreal.  At some point, game devs will be rendering a base image for the AI to work with, that might not be particularly photo realistic, but give the AI lighting model cues it needs.  Then they will select from a series of AI lighting options to achieve the desired look.\n\nWould be pretty interesting if all the non AI compute was put into geometry again.\n\nHell, throw AI at the geo to generate expected detail too.",
            "Or we will just have less effort put into development and AI will be used to ~~do what humans did~~ compensate.",
            "Facts",
            "I honestly don't want photorealistic graphics. I think when we'll reach it it won't feel like gaming anymore so what's the point? Having that shitty distopian metaverse? Lmao.",
            "we are the NPC",
            "\ud83e\udd23",
            "Denoising can happen on the Tensor cores. Nvidia already has the hardware for it since the 2000 series.\n\nWhy are you pulling all of this out of your ass?",
            "In short, AMD fuked",
            "Digital foundry hinted in a recent episode- DLSS 4 has more than 1 frame inserted. They said 2 or even 4 might be added.",
            "And you\u2019re saying this because it\u2019s what you think or\u2026?",
            "5090 gon be 3k minimum yo!\n\nCan buy an entire desktop for the price of a video card! Yay capitalism!",
            "I personally think built-in denoising hardware is very unlikely. There's no advantage to encoding a specific algorithm into hardware, when denoiser's often need to be tweak per-game/scene, and research advances every couple of years.\n\nIf you now think, well ok, maybe they can make specialized hardware for applying filters/blurs or something. Then higher level denoiser algorithms can be written in software and use those hardware blocks. Filters are just matrix operations, which turns out, already have specialized hardware on modern GPUs (tensor cores for Nvidia). \n\nPlus, the actual bottleneck in denoising is mostly memory transfers.",
            "Blackwell gpus on the data centre side has a dedicated Decompression Engine.\n\nMemory is already compressed in vram.\n\nNow using dedicated hardware to do it far more efficiently is a strong possibility",
            "5000 series will have a special ROM with a Skyrim installer on it.",
            "It read like that monty python spam skit that coined the term",
            "It's also not AI as well",
            "It can make the tits of lara craft in the old tomb raider games look like actual tits instead of torpedo triangles.",
            "I imagine it will be something like... the AI looks at a 3d mesh object that's intended to be round or smooth, and smooths it out for you so that it has more mesh objects.\n\nAMD had a similar thing way back in the day, back when they were called ATI, but it got phased out because it distorted some models (I remember the barrel of the Colt Carbine in CS 1.6 being rounded and looking super weird) and so most people turned it off.\n\nBut with AI it could be better.",
            "I wonder if it would be similar to how they made DLSS. Training on high res image (8k or 16K I believe?) then giving the AI a 4k image and having it upscale back to the trained image resolution. Then going lower to 1080p and up scaling back to trained image resolution. So if they employed the same methodology for object training (object with 2 million triangles, then had ai upscale 1.5 million triangles back to 2 million) maybe it would just work? As someone making a game it does sound crazy tho lol",
            "Nvidia already has demos and research papers.\n16X detail, but takes more time to compute",
            "It could also be used for LOD models, scaling down and simplifying them, but still making them look super high quality, kind of like what Nanite does for Unreal Engine, but you pre-generate the LODs so that way the engine doesn't do it on the fly like nanite does. It would save CPU cycles.",
            "Probably just referring to saving artists time by having them make rough models and textures and using AI to add detail. Probably wouldn\u2019t be done in real time because there would be no purpose",
            "I can see this coming in competitive gaming. AI boosting your rank and stuff\u2026",
            "You can do that in Minecraft with mods like Baritone",
            "HD mod for HoMM 3 does a lot more than only scaling though",
            "3070- 8GB\n4070- 12GB\n\n3060ti-8GB\n4060ti-8GB\n4060ti-16GB\n\n3080- 10GB\n4080- 16GB\n\n6800XT- 16GB\n7800XT- 16GB\n\n6900XT- 16GB\n7900XTX- 24GB\n\n6700XT- 12GB\n7700XT- 12GB\n\n6600XT- 8GB\n7600- 8GB\n7600XT- 16GB\n\nJust sayin",
            "Console community is also Geforce.\n\nSwitch 1 sold more than ps5 and xbox x combined.",
            "I want it. \n\nYou know the white room in the matrix? \nWe can have that as a game where we can have it generate and combine all these ideas and existing games into a playable experience. \n\nHey game generate me Columbia from BioShock and make me solid snake. \n\nWould be insane.",
            "Funny you should bring that up, it already can. I used it as a final for a class after training it on smash till it well pretty much became a cheese machine, but it worked. 3D games may be harder as it tries to work out the where am I but 2D seems doable. I should see if it can do battletoads as a joke.",
            "Baritone GPT",
            "Just went 3070ti to 4080s. Initially I was a bit unsure, seeing I can't set everything in Cyberpunk to ultrapsychomax and have high fps on ultrawide. But then I realized that even on ultrapsychomax the low fps is actually more stable than the drops I experienced previously on some high-medium.\n\nAfter some balancing, it starts to sink in. Playing at mostly-psychomax and having around 120fps is just awesome. It's like I've been crawling through mud, and now I can dance. Similar story in Witcher 3, not having drops in Novigrad is so weird xd Also smooth driving in Test Drive SC demo was nice.",
            "True. It's only used effectively in a handful of titles.",
            "Truth is it won\u2019t until and provides these features to consoles or consoles abandon amd and go nvidia. \n\nIt sounds like amd is both more pleasant and more cost effective to go with, so I don\u2019t see consoles switching course. \n\nAnd if they do, I don\u2019t know that I see amd staying in graphics. If consoles started pushing all the advanced features nvidia can provide, amd gpu users would be left with paperweights in 2 years. \n\nOn the upside, if ps5 and Xbox went with an aggressive Nvidia solution for their gpu\u2026we\u2019d all be playing fully path traced games by 2030 as a norm with no legacy support for raster.",
            "Wait until you learn about eye saccade movements and generally about how your brain generates and extrapolates what you actually see.\n\nI'm not discussing how shitty the game industry gets, but the graphics alone can get all the AI treatment there is / will be.",
            "Jensen is",
            "I totally get what you're saying about art style but I'm still taking Cyberpunk or TotK anyway, cyberpunk is gorgeous with it's art direction combined with path tracing",
            "Have you considered that the art style might take into account being able to use AI to increase texture and model detail without murdering performance. I hardly believe an artist feels good about having to nuke texture quality and decimate model detail to make their handmade asset run well in a game.",
            "I guarantee you, if the switches hardware limitations didn't exist, the totk \"art style\" would look a lot different.",
            "Native is best yea but you get downvoted here  because DLSS is god for nividia fans. Both FSR and DLSS make image worse i like native only also.",
            "> Does DLSS add input delay and if yes how much?\n\nDLSS 2 (resolution scaling) no, maybe +1-2 ms\n\nDLSS 3 (adding 1 extra frame) doubles rendering resolution",
            "Games use a lot of eye tricks already. Even before DLSS and framegen got introduced\u2026",
            "They\u2019re already doing it. It\u2019s called \u201cskins\u201d.",
            "Considering node shrinks are not giving not much performance natively, only way up is Tensor Cores. \n\nSo, if it becomes selling point of next gen products then Jensen will make at least one aspect of new gen RTX 5000 exclusive. So, people upgrade. Just like Frame Gen of RTX 4000,",
            "I've never really known Nvidia to be that generous.\u00a0",
            "Don\u2019t understand the downvotes here. AI will be able to generate games from a few text prompts in the future.",
            "Nah nah. 4gb but the gtx 970 4gb\ud83d\ude09\ud83d\ude0e",
            "5070 3.5gb",
            "Meanwhile, I am still waiting for AMD claims of using HBM on all of their GPU skus (including lower end) since the R9 Not-So-Fury series AND no more 8GB offering from them.",
            "They need the nand for AI cards",
            "> devs putting basically raw assets in games.\n\nNot all devs are idiots like the ARK devs... or the CoD studios...",
            "FYI, nvidia paid them to put in raw assets for a bit. Too many wanted to use decompression techniques that didn't work on 700 900 or 1000 series cards. Now they haven't stopped cause who cares lol",
            "Nah man that's a compression algorithm not generation.\n\nI really do wish they use it though, heck its compatible with GPU decompression so it could be used for DirectStorage.",
            "This would be particularly great for mobile devices. I have no qualms about 200-300 GB games on my laptop or desktop, I just want the best quality possible. I\u2019d also have no qualms about that on a mobile device, but it seems that lots of people still do in this day and age, and this mobile games are still graphically far inferior, even at their max settings, no matter what device you may have.",
            "I want more quality first.",
            "I remember all the jokes back in the day, \u201chey man just go download more RAM hurr hurr durr.\u201d\n\nBitch I\u2019m about to.",
            "How is a 3080 very ancient? It's currently 1 Gen behind?",
            "My Guess is Intel will surpass them.",
            "AMD targeting lower to mid range cards would actually be a great move if they'd stop pricing their cards so stupidly.",
            "Since when was rdna5 targeting less than 4090?",
            "That doesn't even make sense.\n\nConsidering the 7900xtx equals a 4080 or beats in anything but RT and a few other scenarios, I find it very, very hard to believe that in the five years between 2022 and 2027, AMD won't catch up to a 4090.\n\nEspecially considering that 4090 likely performs between a 5070 and 5080. So assuming  2026 is the 60 series release date, by then a 4090 will be 6060ti - 6070 performance, just with more vram.\n\nThey said they aren't competing in the ultra high end segment, but I can't see how they'd still target less than 4090 performance in 3 years time, especially since they'll be making the next gen console chips?",
            "do you mind linking this? I'd like to read it.",
            "It was already rumored but the leak reinforced it. It's crazy, you're telling me that the best card you're going to release in **3 years** is targeting lower performance than the best card that's out *right now*? It's hard to take that as anything other than giving up.",
            "Evidence i cant find anything on this leak like literally nothing",
            "Any one know what is AMD's advantage is, if any?",
            "Calm down bro",
            "Why even render based on existing assets at all? Just make a game engine that spits out  generative image prompts per-frame. At best you only need to spit out a basic wireframe to the generative ai to baseline an image on\n\nGenerative images can just make any game you desire on the fly, prompted one frame at a time.",
            "It will be 50/50 as always.",
            "I think having the option of photoreal is good, doesn't mean developers have to use it though.",
            "Most of reddit is anyway.",
            "It's his wet dream.",
            "Amd knew it already, they didn't even try to fight nvidia now",
            "And supposedly that should be good?",
            "Yep what a shame",
            "This sounds downright terrible lmao, 4 faked frames per rendered frame to compensate for shit optimization",
            "/sigh\n\nThis is what VR needs, but frame generation isn't supported in VR.  VR needs 90 +FPS, but at high resolution.",
            "Like lossless scaling 3x frame gen, nice. But we need better quality, not more fps imo. Like less artifacts, better UI rendering and devs that don't fuck up everything.",
            "They're going to basically upscale the textures, for lack of a better term. A low res texture using less data and VRAM which will appear after AI reconstruction as a normal 4K texture, etc.",
            "Yup. AI is just their latest excuse to raise prices.",
            "soo nuts and bolts will become donuts like they did in the GTA san andreas remaster ? the one every hate because it was done like shit \n\nthey probably used AI to do it",
            "Nanite is not generated on the fly either. All the clusters are pregenerated ahead of time, and then selected from at runtime.",
            "4070 and 4070Super is already running out of VRAM in some games at 1440p and 2160p, there should be 16GB of VRAM just like it is in 4070 Ti Super.",
            "That's kinda stretching it considering it's been competing with the PS4, xbox one.",
            "I keep forgetting that the switch has an Nvidia chip in it.",
            "It has nothing to do with GeForce.\n\nThat chip is a Tegra chip.\n\nDifferent system, different architecture, different constraints, different games, different deals with Nintendo, different consumer base. Different everything.",
            "I am not sure if it was this concept demo by Nvidia I\u2019m thinking of:\n\nhttps://youtu.be/ayPqjPekn7g?si=DVuPE_TwuMmBy_z9\n\nIt\u2019s worth noting this was 5 YEARS ago. I can\u2019t imagine what you could do today with the MASSIVE improvements in e.g. Stable Diffusion, in generating stable, correct videos.",
            "Not just that, it still runs like shit. We're three generations into RTX and raytracing still kills performance. Can't help but wonder if we would've just been better off optimizing for raster. NVIDIA went through a lot of trouble developing DLSS and framegen JUST to make raytracing seem more viable than it ever actually was.",
            "Honestly I'm wondering if AMD would move their fat ass if Microsoft and Sony switched to Nvidia somehow",
            "My papa is the richest",
            "You can run it on a pc with mods to fix that limitation and make it look really good. Also add in multiplayer if Nintendo hasn't shut that team down yet.",
            "Yeah it will for sure, perhaps not to level of my (joke) comment earlier bit for sure that seems to be the aim, in a way it makes sense when you see how long modern games are taking to make.",
            "5070 featuring 7.5+0.5gb VRAM?",
            "Huh? I thought they go up in gb, not down. The newest one was 12GB. 3060 and 4070 were both 12GB.",
            "I remember using a texture mod for fo4 (yes bethesda, not a good example but still) that had waaay better textures than vacilla while using half the vram. This was 2015, before all the garbage late ue4 and ue5 games we see nowadays. They mostly compress like shit their assets. Sampler feedback streaming could have helped, not a single games uses it.",
            ">Not all devs are idiots like the ARK devs... or the CoD studios...\n\nThe CoD studios put huge efforts into optimizing textures and files.\n\nThe reason why the installation sizes of COD are so big, is that they calculate as much as possible while building the game and bake it into the files so it does not have to be calculated by the client and waste performance. They are also saved in a way which makes them really easy and quick to load even for weak CPUs/GPUs and make VRAM streaming super easy. Also the models and textures are really high detailed. This is why COD still looks really good on the Xbox One at 60FPS and even runs on a pc with a GTX 960 2GB.\n\nThe big disadvantage of precalculating everything and saving it in files, making streaming and loading fast and easy (part of this is basically using no compression and saving pretty similar files multiple times on the disk so mechanical hard drives don't have to search as long and caching for SSDs is easier) and having really high detail and many quality levels is that it takes up a lot of disk space.\n\nTLDR: They are optimizing the textures and files, but in a way which saves performance, but costs a lot of disk space.",
            "One of the IW devs said in an interview that there's over a terabyte of texture data they have compressed in MW2. It's compressed to shit and back.",
            "Fascinating how ark is the *only* game I have yet to expeirence that does jot reach 144+ fps on max settings 1440p\u2026 Even with dlss enabled, I have a 4090 for fuck sake",
            "Kids on Reddit are the real experts.",
            "Not idiots just lazy.",
            "A 300gb game with quality comparable to a 2018 game that was like 30gb us not \"best quality possible\" it's a waste of space. The best quality vs a good compression is basically indistinguishable, we are playing videogames not doing photography reviews so it's completely pointless. You might have no qualms but I do like a lot of people. I want more than 3 games on my pc, I don't want to buy a 24gb gpu only because devs can't optimize their assets like they should. Wasted space, resources, energy. At the end of the day 99% of people don't care about h265/av1 for movie streaming and gaming is the same, we need better compression (like this one) to get more with less, called advancement.\n\nEdit: you need to look at the whole image, and you'll understand why uncompressed is stupid, pointless and it's not \"the best quality\" if that means you need 4x the vram:\n\nhttps://hothardware.com/photo-gallery/newsitem/61558?image=big_ntc-example-1.jpg&amp;tag=&amp;p=1",
            "I'm aware, I'm throwing shade at Nvidia for not letting me use Frame Gen with it.",
            "Hopefully. While I have an AMD cpu and intent to buy a new one soon again, we can all agree that they've messed up the GPU part of the business massively. They've left Nvidia with no competition.\n\nIntel seems able and willing to compete which in the end, will benefit us as well.\n\nI hope they become better quick.",
            "Xess is already better than fsr. Yea, I can see it happening.",
            "Watching Intel and AMD competing in the GPU department is like watching Special Olympics.",
            "Yea price matching Nvidia with a 5-10 % discount doesn't really cut it when they're so behind in software features.\n\nThey need to do a Ryzen in the GPU space.",
            "Yeah, if you're going for a solid \"1440p/120hz/ultra settings\" experience, you can't be offering your cards for $50 less than the same experience from Nvidia, when Nvidia also has DLSS, ray tracing that almost works, etc etc.\n\nIf AMD's cards were half the price of Nvidia's for the same FPS then yeah, they would definitely have a market, but $50 less for a card that has no DLSS, no CUDA, same vRAM or slightly better, no other real capabilities...\n\nNah. Why would I?",
            "Even Nvidia's budget cards have features that shit all over AMD's top level cards. AMD is horrible at producing GPUs.",
            "But why would even low end gamers give up Nvidia features?\u00a0",
            "Check the data leak. Also, even if rdan5 hits 4090 performance. That is 2027.\n\n5090 is 2024\n\n6090 is 2026",
            ">4090 likely performs between a 5070 and 5080\n\nPersonally I strongly suspect 5080 will be limited to 4090D performance due to China market, but I otherwise agree with you.",
            "If you understand that node shrinks are near limit and price is too high, you would understand why even with rdan5 in 2027 they won't match 4090.\n\n7900xtx is 550mm\\~ 384 bit\n\n4080 is 380mm\\~ 256 bit\n\nsame raster\n\n30%+ faster in RT+RASTER - 4080\n\n100%+ faster in only RT - 4080\n\n100-150 watts less - 4080\n\nEven just based on gaming and watts amd can try to use 3nm to match this spec and performance. They would be big loss for them. \n\nAmd 100% will gradually leave gpu market.",
            "LMAO 5 YEARS to catch up to a 5 year old flagship. Yes I'm sure even AMD can manage that",
            "From the rumoured core count it seems unlikely that a 5080 will beat a 4090 that easily, but we'll see.",
            "Ii don't think anything other than the 5090 will surpass the 4090\n\nNvidia really values the Chinese market too much, why make a bunch of cards you can't sell there?",
            "Yep, this is even worse than Vega tbh because at least with Vega they did bring out a competitor to Pascal while Pascal was still the best NVIDIA architecture. With RDNA5 they will be two architectures behind. Thats a disaster.",
            "Amd is a joke",
            "Why? Because ai generative images are uncanny and don't have any soul. Even when they aren't uncanny they look homogenous and mass produced",
            "A serious response-- because current whole image generation methods based on prompts are still extremely slow, unpredictable, creates hallucinations, and doesn't give very precise control (in addition to all the same-y looks they tend to generate based on what it's been trained on).  And this is using massive rendering centers in the cloud stacked with GPUs.\n\nThe more specific \"scaffolding\" you give the AI to work with in terms of actual game and visuals, the faster it is and the better control you have over it.\n\nI wouldn't be surprised if more and more AI is used to create more of that scaffolding over time... and that scaffolding has to be less and less specific... until we basically have Star Trek Next Generation Holodecks.\n\nBut we're not anywhere close.  However, having AI \"filter\" a rougher game in real time to achieve photo-realism or cover the gaps in the lighting model is a lot more achievable in the near term.  In fact I bet this is going to be a feature from NVidia in the next 5-10 years if not sooner, like a mic drop feature for the 6000 or 7000 series.\n\nWhen Jensen says that NVidia isn't a graphics company but an AI company, some people lament.\n\nTo me, it actually makes a ton of sense, because the best graphics are eventually just going to be another problem set using generalized hardware designed for generative AI, and that will actually be the easiest way to solve the problem.\n\nHeck, I can even think how a photo-real lighting model might be trained-- take photo-real CGI, then train it with both the low-fidelity render and the final render.  Or take real images and videos of real world assets and scenes of a style and quality you aim to achieve, and pair that with the real-time assets you're approximating it with.  This would allow the AI to \"guess\" at what you're trying to achieve in the final look when feeding it low fidelity assets.",
            "OEMs knew it too because they stopped putting Radeon in laptops almost completely. They probably won't be back next gen.",
            "Actually I don't think 4X would be terrible. It has access to the motion vectors. DLSS 3 (and by extension FSR 3) are way better than the spatial solutions in software such as Lossless Scaling (which produces OK results at X3). That's not even accounting for the OFA and CUDA cores helping out too, in comparison to FSR 3.\n\nI'd argue that a a temporal X3 interpolation would be awesome",
            "Losseless scalling already have 2 frame interpolation, and it works like a charm.\n\n\nI own a 175hz display, and for games with locked 60fps like the crew interpolating those 2 extra frames makes the experience WAY better.\n\n\nFrame gen can be used for more than just adding \"fake performance\".\n\n\nAlso, as the other redditor mentioned, with how stupidly hard its getting to shrink manufacturing nodes, we are either about to face no performance gains at all, or new creative ways to use the existing die space.\n\n\nAt least until the new techs that intel is developing like double fet takes fly and start giving good enough shield rates to be viable for consumer space, instead of profesional space only.",
            "All frames on screen are fake. One is generated by Shader Cores and Another by RT cores and now 2 by Tensor cores.\n\nAlso, node shrinks gaining performance is becoming more and more hard.\n\nOnly way up is Tensore Cores.",
            "Myopic take.\n\nBeing able to make 60FPS into 120 is cool.\nBeing able to make 60FPS into 240 with likely the same latency, when there are 240hz OLEDs out, is just more cool.\n\nWould you rather have 60 'true frames' or 240 frames, with 60 being real, with a vanishingly small latency hit?\n\nI'd take 240 easily.  It will look better and more responsive.",
            "It will become 1000% the new scapegoat for shit optimization. Just like now they build games \"with upscale in mind\" lmao instead of having extra performance they use it to compensate bad optimization and it will be exactly like this. People might not agree, but when I said the same about dlss they didn't beliveme me and yet here we are, just wait until frame gen is the same. They'll target 30fps again since you can triple your fps with frame gen with an horrible input lag. Games will run at 720p 30fps internally.",
            "Frame generation doesn't make sense for VR until you're so perfectly predicting the future that there's no artifacts even in fast and unpredictable rates of motion.\n\nFast and unpredictable rates of motion describes any action VR game to a T, and even worse, it describes peripheral vision in VR *even more* accurately... which is exactly the area of vision where you're going to notice shit moving just slightly wrong. (Which is exactly why people turn off the current VR frame interpolation stuff in any fast-paced game)",
            "They sort of already do this by doubling the frames to make it appear smoother, but I agree, true frame generation would be a game changer for VR.",
            "VR uses reprojection, frame gen won't work",
            "You can't really do frame interpolation in VR because the head movement is non-predictable and added latency will be far more noticeable. VR has better tricks for this instead like async reprojection and frame extrapolation to avoid the 1 frame minimum hold time for interpolation.\n\nWhat NVIDIA should invest in is somehow getting the tensor cores / OFA to handle frame extrapolation in steamvr or something.",
            "I think it's also an excuse to force upgrades as well. I fear they're trying to kill the concept of having a card for a long time just because it performs well. Now they're trying to get you to upgrade because only the latest cards support DLSS 4 or 4.5 or the new AI feature.\n\nAnd of course this model will also be more akin to a monthly type of payment model as well. It's all kind of very scary. It's weird because in any normal circumstances I'd expect to be excited by news like this, but we're dealing with one of the worlds most commercially successful companies and their monetization policy is **brutal** on consumers.\n\nThey've completely out priced the casual mid range consumer from top tier cards. In the past a flagship high end card was an investment for the average person, it was expensive but they could buy one. Now it's genuinely impossible for them",
            "This one was using older methods, which would basically smooth everything.\n\nA well trained generative AI that outputs 3d models would yield accurate results. But yeah not before 3-4 years IMO",
            "Yeah the original technology I mentioned (forgot what it was called) really kinda made everything look worse, like things that were supposed to be cylinders got squeezed at the end, things that were meant to be square became rounded, etc.",
            "extreme minority of games at the highest/ ultra settings. \n\nits fud. not a realistic concern",
            "Guess you'd have to compare to PS4+PS5+XBONE+XBSX/S",
            "I mean the PC community is bigger than all of it. Steam is in way more countries with way more users. Too bad Sony is being so restrictive now.",
            "It has the benefit of helping with marketing. AMD could keep up with raster, they are struggling more with RT, and look at the marketshare.  DLSS is good, but I bet the majority of people that bought the card for RT might have only played one or two games with RT, if at all.",
            "What does optimizing for raster even mean? You can't just pour more money into research and expect results. Raster is fundamentally limited. It's great for rendering a vast amount of geometric detail, and absolutely terrible at lighting.\n\nThere's a reason a ton of game studios have been trying to use raytraced lighting more and more, even before hardware RT acceleration. Baked lighting sucks.",
            "That's what I'm saying! The emulated version is crazy good.",
            "7.5 Gb of ddr2 you mean right? And .5 of gddr5 trust. It allowed them to bring down cost from 699 to 650. Always trying to help the consumer out\ud83d\ude01",
            "I hope they go up to 16gb, they aren\u2019t increasing memory bandwidth tho for the upcoming gen supposedly tho so if you want 16 you\u2019ll get a 128 bit bus which is smaller than the 3060 at 192 bit. It\u2019ll probably be  2gb modules which will be good cause the 4060ti double sided memory situation won\u2019t happen but the bandwidth detriment will still be bad.",
            "> They mostly compress like shit their assets.\n\nGotta remember the more compression the more overhead, and things like GPU decompression still isn't exactly leveraged a ton to my knowledge even now. \n\nSome things can be deliberately compressed poorly for speed of loading on weaker systems.",
            "You're downvoted but you're not wrong. \n\nPeople haven't figured out that you can have great graphics, lower system overheads, or smaller file sizes. Developers only really get to pick two out of the three. \n\nIf they downgrade the graphics they get flogged for how bad rock textures look.\n\nIf they lean too hard on compression and don't structure things to make loads quicker and easier they get flogged for \"unoptimized performance\" and system overheads.",
            "One terabyte? Is it even necessary? Lmao. I mean I really hope that nvidia neural compression will be used or we'll pass 500gb games pretty soon lol.",
            "Well if CoD devs said it, it must be true.",
            "... says another kid on Reddit...",
            "Absolutely. We don\u2019t just need more powerful GPUs on the market, we need desperately need more, stronger competitors.",
            "For me the real differentiator is that Nvidia identified the market for AI much sooner than their counterparts and built the CUDA framework then opened it up to the scientific world for adoption. Nvidia recently shutdown a few adaptations that would allow the CUDA framework to run on AMD GPUs. At this point it would take a lot for data scientists to switch to another framework for generating their models. I think ASICS / FGPAs will take more market share from NVDIA than AMD at this point.",
            "> Intel seems able and willing to compete\n\nThey have plenty to prove in product and dedication.",
            "> They need to do a Ryzen in the GPU space.\n\nOne reason Ryzen has worked out so well is CPUs don't need as much of a software stack. If it did AMD would be floundering there too. Their software dept. is just terrible and has been terrible for ages.",
            "[deleted]",
            "If you don't want to pay more for a GPU then a console, but still want to play games",
            "Only leaks I've seen is that RDNA4 is basically a refresh and RDNA5 is where they are bringing their true multi chip design. I'll do some searching I haven't heard these rumours.",
            "Isn't 6090 going to be 2025? Nvidia said they were going to move to yearly architecture updates. At least they said so for their AI chips, but that implies the same for gaming, unless gaming is going to be a generation behind.",
            "Recent leaks of the 5080's heavily cut down specs relative to the 5090 strongly suggest they're trying to get it to hit right at the 4090D performance target for sale in China, just like you suggest. \n\nKind of a bummer since it also suggests the 5080 *could've* been more powerful than it is projected to be, but I guess that theoretical card will just end up being released as the 5080 Ti (and then Nvidia can charge even more for what it should've been, yay! \ud83d\ude11)",
            "RemindMe! 3 years",
            "Maybe they can split GPU's off, I have the perfect name\n\n**A**dvanced **T**echnolgy **I**nstitute",
            "Node shrinks no longer give big boost + node prices are absurdly high.\n\nHow many years amd took to beat 2080 ti?\n\nHow many to beat 1080 Ti?\n\nNow, even more to beat 4090.",
            "A 5080 that doesn't surpass the 4090 will be a flop.\n\nNvidia will gimp the vram so it's trash for AI purposes, so it going to china won't be an issue",
            "The Chinese market is closed down for the AI and computing in general, you need to lack all awareness to invest in that market today.",
            "Hey man, i didn't say it would be *good*, but the basis of the idea already works. \n\nhttps://www.youtube.com/watch?v=udPY5rQVoW0\n\nThat was 3 years ago on shitty NN models. It would be 10x better today.\n\n>  Even when they aren't uncanny they look homogenous and mass produced\n\nThat is the fault of those who prompt them. High level prompt engineering can produce unique works of art, and getting coherent video game gameplay out of it would def require extremely large complex prompting.",
            "Lossless scaling? O_o \n\nHave I missed something? How is that mathematically possible",
            "I think the only problem is how it affects latency, I have a 3080 so I never tried dlss framegen but I did try fsr3 framegen + nvidia reflex on immortals of aveum and according to afterburner the rendering latency even went down and the game was running much better from something like 60fps to over 120 but the latency made it unplayable, it felt like I was playing a game in streaming",
            "RT cores generate frames? Since when?\n\nAlso node shrinks are getting harder, but things like AMDs chiplets and Nvidia new interconnect is what will ultimately scale true performance over the next decade not faking frames. AMD have already shown with their CPUs the stuff you can and scale with chiplets and both Nvidia and AMD have interconnects that can merge multiple chips together and act as one. This is where scaling will come from. Faking it til you make it will only get you so far.",
            "https://i.imgur.com/pT9qUOD.png",
            "Except the fake frames *still* look like shit. The only reason you don't notice it is because they're not on screen very long. If you suddenly have 3 or 4 of them in a row, however...",
            "I think this is quite different, it sounds like they will start with high res assets and then decrease the res of the asset as far as the AI model can still recreate the model accurately. Starting with a low res model gives no comparison as to what the output should be. Therefore the significant variable won\u2019t be higher accuracy but higher compression.",
            "It was AMD's version of tessellation, something that was originally supposed to be in DirectX 10, but got delayed to version 11 at the last minute. AMD tried to implement it in their drivers since they already had the hardware, but you really need support in game engines or otherwise it makes things wonky.\n\nE: why the downvote? You can look this up on Google.",
            "Those cards are just one year old and launched for $599, how is this not an issue? Things will only get worse.",
            "No doubt, but as a consumer I have no use for marketing lol",
            "I don't even know where to begin. It's like you just took NVIDIA's marketing at face value and invented reasons to justify it.\n\nI don't feel like getting in an argument today. But I'll just say that rasterization is an awesome rendering technique. If you don't think it is, you should study it's history. Baked lighting is awesome. Raytracing still has years to go before it can even hope to replace raster.\n\n>What does optimizing for raster even mean?\n\nIt means investing R&D into making rasterization better instead of shoehorning raytracing.\nRaytracing is gorgeous. But the hardware just isn't good enough. People got dazzled by promises of the future. Three gens later and it still sucks. It's so grossly inefficient.",
            "The more VRAM, the better, I suppose?",
            "Gpu decompression isn't supported on some cards that nvidia supports. They'll continue to embed engineers that encourage uncompressed assets to various companies.",
            ">If they downgrade the graphics they get flogged for how bad rock textures look.\n\nI still feel like a vast majority of gamers prefer a playable game over an unplayable, hyperreal imax feature.",
            "> One terabyte? Is it even necessary?\n\nRage back in 2008 also had over a terabyte of data when uncompressed. A lot of stuff is massive without compression, far bigger than most people realize. Even just a basic music album or a video can be of obscene size if captured with zero compression in a non-lossy format.",
            "Is it necessary? No. In the old days you used to have someone making textures and they were artists, real artists in the sense they made a texture look real and fit an art style from nothing and they used to be so tricky and smart, they could make 32KB look like a 10MB texture just by being crafty as an artist. It's a skill to be a texture artist and a computer automated process of scanning and compressing can't match it currently (maybe AI will in future though).\n\nNow days game studios they just scan stuff from the real world into assets for the game and thats why the games now have bloat, photogrammetry is supposed to be a tool to help texture artists, but instead it's put them out of business basically and the textures are unoptimised. I am not knocking photogrammetry because it can be very useful if you use it sparingly or in tandem with texture artists. More often than not though, the texture artists are pushed out the door in favor of just scanning everything in and then compressing the textures for the assets via an automated algorithm. It's trash because a texture artist could make those textures maybe look 95% as good for 1/10th the size.",
            "Carmack in 2008 said Rage had over a terabyte of data uncompressed. It wouldn't be that unthinkable that more modern stuff with higher fidelity is that huge now.\n\nEdit: Don't believe me? https://www.shacknews.com/article/53976/rage-will-look-worse-on",
            "I believe it, they scan everything for the games, like everything, even a small scarf on a character model. Thats terabytes of texture data and realistically they're compressing it to shit to try and fit it in 50-100 GB.",
            "Takes one to know one!",
            "I wish!",
            "We need cheaper gpus more than we need faster ones.",
            "Nvidia GPUs are more power efficient though?\n4070S is faster and uses less power than the 7800XT for example.\n\nUnless you mean CPUs they are way more efficient than Intel's.",
            "Thats just conjecture by people based off patents and stuff. Truth be told RDNA5 is so far out now that maybe it also won't be ready for multi-chip GPU either, seeing as these types of comments were also made about RDNA4 when RDNA2 was out. RDNA4 was supposed to be the revolution, it got downgraded when the approach did not work or ready to work either because of poor scaling or some other issue.\n\nPlus if RDNA3 is anything to go by, the whole multi-chip approach isn't even that much better anyway. The 7900 XTX matched the 4080 in terms of performance, but it used more power on a similar node and it also was priced not that much lower. mGPU isn't so promising because it requires all these new packaging services and extra power budget that it basically offsets the cost and power savings you're supposed to get by going multi-GPU. Maybe in 10 years time it will be more feasible, but not right now. I'm sure if anyone could do it right now, NVIDIA would have for the consumer market if they could, they're ahead of AMD by two years and ahead of Intel by four years in the industry and the only product they've done it for is a compute AI chip where probably it scales on those sorts of workloads because it's not latency sensitive and you can offset the costs with a high margin product. But for consumer stuff, you're not going to see it for a while.",
            "Ah yes the true \"multi chip design\" where AMD will finally power match Nvidia. Like I haven't been hearing that since the Vega launch where the cards weren't underpowered guys. They can improve them and make them more powerful over time. I've been PC gaming for 15 years and not once has AMD ever had the GPU advantage.",
            "I will be messaging you in 3 years on [**2027-06-21 19:07:06 UTC**](http://www.wolframalpha.com/input/?i=2027-06-21%2019:07:06%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/nvidia/comments/1dkvcze/jensen_huang_recently_hinted_what_dlss_4_could/l9noe9g/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fnvidia%2Fcomments%2F1dkvcze%2Fjensen_huang_recently_hinted_what_dlss_4_could%2Fl9noe9g%2F%5D%0A%0ARemindMe%21%202027-06-21%2019%3A07%3A06%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201dkvcze)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
            "AMD is alright if you don't care for the highest fidelity experience at every price point. Otherwise it's all Nvidia",
            "They released a card faster than a 2080ti 2 years after the ti came out....the 7900xtx is >30% slower than a 4090 at 4k....so in 2 gpu generations you don't think they surpass 30% ? \n\nPerformance is more than node size.....",
            "Well it can't be better than the 4090 if they want to sell it in China....",
            "Cool I'm just saying the 5080 won't beat the 4090 because Nvidia wants to sell to consumers... it's not all AI",
            "I'm not some AI bro, but I get tired of hearing \"AI can't do that\" or \"AI won't look right\" and shit. The explosion in AI tech we've had in the last couple of years, let alone the last decade, is insane. [This thread](https://www.reddit.com/r/StableDiffusion/comments/y9zxj1/you_think_thats_insane_heres_what_an_aigenerated/) shows an AI generated cow from 2014. An AI generated cow photo today is about indistinguishable from a photo of a real cow.",
            "Thats the name of the program that does this 2 frame generation, it's called \"Lossless Scaling\".",
            "The difference between FSR3 and Nvidia's FG was night and day in Aveum. I tried both and it was very obvious when you used FSR3. But I honestly didn't feel any issues with Nvidia's FG.",
            "I've used the DLSS FG to FSR3 mod in a few games and they all felt great with reflex on at 100+ FPS.\n\nI also tried the Forspoken demo with FSR3 and it felt absolutely unplayable, so it really depends on the implementation.",
            "I'm not convinced that UE5 games are going to running at 200+ FPS on any CPU in the next 5 years. Meanwhile monitors are pushing higher and higher refresh rates. I'll take the fluidity of 240 or even 480 fps with frame gen if it's possible.",
            "IF what you said was true then Amd would have taken the performance crown as their gpus do not waste space on dedicated tensor and rt cores like Nvidia.",
            "If you suddenly see each fake frame, which is better than older fake frames, for 1/3 or 1/4 of the time...\n\nIt will look better.",
            "> But I'll just say that rasterization is an awesome rendering technique. If you don't think it is, you should study it's history. \n\nIt is great - for determining primary visibility. It absolutely sucks at lighting.\n\nI'm well aware of the limitations of different techniques, _especially_ rasterization. I'm a graphics developer, have implemented a raytraced GI system, and recently reimplemented a large chunk of Nanite from scratch. \n\nNanite is pretty much the peak of rasterization, and is great for getting large amounts of geometric detail... but not lighting.\n\n> It means investing R&D into making rasterization better instead of shoehorning raytracing.\n\nMaking rasterization better, how? What's there to improve? Even a 100x speed improvement would not be enough to make it viable for path traced lighting. It's way too slow and memory intensive to render a full view at every light bounce for every pixel. Plus, transparency is extremely limited in rasterization. Rasterization is a dead end for lighting. Not so for geometry, but lighting yeah.\n\n> Three gens later and it still sucks. It's so grossly inefficient.\n\nAgain, sucks how? Inefficient? Compared to what? Games can hit 60fps with real time fully dynamic lighting, without having to rely on screen space reflections, AO, shadow maps, or semi-baked lighting. People are playing those games right now, and enjoying them. That's a massive success.",
            "Yeah but having a reduced bandwidth basically prevents assets from being properly presented on screen at high resolutions and it also decreased performance because it can be a major bottleneck.",
            "There's no conspiracy like that. Developers regardless are just going to target the lowest common denominator as a baseline. Whenever developers do use newer techniques with no fallbacks they get pilloried online for it. \n\nWhat encourages less compression is devs get harassed and bad reviews when system overheads are high. Add in storage space being cheap and most gaming platforms allowing redownloading whenever and yeah there isn't a whole lot of incentive to push things that either rely on newer \"bleeding edge\" stuff or just cripple weaker CPUs (hint whenever games are CPU heavy everyone loses their minds and bombards them with bad reviews).",
            "Maybe the silent majority, but the ones that post reviews, hang out on community topics, and such? Not a chance. Otherwise they'd actually tweak settings. The \"ultraaaaaa\" or bust crowd kind of stands defiant to that mindset. The people that won't turn down any settings, and then will whine how things run on a laptop or a 1050ti.",
            "Kind of a tangent but most PS3 games actually include 4k textures on disk, and it down samples them when installing to the PS3 HDD..... they did this probably thinking forward to future PS3 Pro that never existed.\n\nPS3 Pro probably never existed due to Nvidia also being crap to work with.\n\nThe emulators these days can use the BD textures directly at higher res than PS3.",
            "Eh, HD resolutions and no longer tiling a handful of textures put more of an end to traditional \"texture work\" more than technology did. A sizable number of textures are still drawn up artwork by artists. Still need various types of artists as well to compose a scene as well. \n\n>It's trash because a texture artist could make those textures maybe look 95% as good for 1/10th the size.\n\nSure if they had a large amount of time to spend on each and every object manually modifying it instead of flipping their artwork right into game and moving on. How many unique textures do you think are in even smaller mid-budget games anymore? There's also a reason pixel art 2D games are all but gone or using clever tricks to try and recreate the effects.",
            "No, you!",
            "> We need cheaper gpus more than we need faster ones.\n\nIf there were competition in the market we'd get both. Unfortunately in GPUs AMD has zero ambition and is happy with table scraps. Nvidia won't compete super hard either because if they beat AMD any harder on that front there will be anti-trust inquiries.",
            "No, that's what amd is doing, flooding the market with cheap garbage GPUs, while Nvidia sits comfortably knowing they are kings of this realm of faster and powerful GPUs.",
            "I mean you can just buy last gen or used? There's really little incentive to make fast and cheap cards that cannibalize their own product stack, or worse, use up precious fab output",
            "Multichip is the future, Nvidia showed this with their new server chips where they fused chips together to act as one. This is the approach I'm taking about that all companies will go down. That and stacked memory once they get it working with nee HBM.\n\nRDNA3 uses more power because it's multichip but these aren't fused together they use a larger interconnect similar to Zen CPUs. You are transferring the same data multiple times to try and speed it up but transferring data costs energy. This led to RDNA3 not hitting clock frequencies which was aiming for 3GHz+. Then factor in that rumours pointed towards 12k shaders which would have obliterated the 4090 (AMDs shaders have always been individually stronger than Nvidias). But this rumour was taken the wrong way and without context. It was 6K shaders will double execution, which we're meant to act like 12K shaders but this didn't work properly and never has done.\n\nRDNA4 rumours point towards a refresh of RDNA3, with some slight changes and better overall RT. These are going to come in at very aggressive prices, they have too or AMD are seriously delusional.\n\nRDBA5 rumours I have seen point to it being a new ground up architecture which has been in the works for nearly 4 years already. Resources were taken away from RDNA4 once they found out the issues were not worth fixing and they moved tons of manpower onto RDNA5.\n\nI still hold onto AMD making a comeback, we need more competition. Intel is great and doing good things so far in the GPU space but we don't want a true monopoly.",
            "In fairness, the vega56 does match the 1080Ti now.\n\nhttps://www.techspot.com/articles-info/2339/bench/Ultra_1080p.png\n\nNot that it counts for much in 2024 given how far down the stack a 1080Ti is, but hey.",
            "Amd is too much behind. For same raster 7900xtx needs to cost half of 4080 super. \n\nThe gap is too big in overeall performance and features.",
            "That's assuming AMD releases more x900 XTX tier cards. If Nvidia's top GPU was the 4060 this gen it wouldn't be faster than a 2080 ti. \n\nI'm not convinced it's true this time, but the RX 580 wasn't much faster than the 290X which came out 4 years earlier.",
            "3 Years + to beat 1080 Ti\n\n2 years + to beat 2080 Ti\n\nSo, it is very easy to understand 4090 won't be beat in 3 years minimum.\n\nI recommend you check node shrink gains. \n\nThere is not much room left for big gains.\n\nFor example - tsmc 5nm to 3nm is very small gain but node price is absurd.",
            "They can make a separate version for China",
            "it's all AI baby, we be living in the rise of the AI century.",
            "Yeah. generative AI is gaining quality at a breakneck pace. even 2023 vs 2024 models are insane.\n\nIt's hard to quantify but someone should coin a law like moore's law but for how fast AI develops.",
            "Oh, thanks. I fail to comprehend why would anyone name the application in such way",
            "[removed]",
            "I really think the fundamentals of your arguments comes down to you not understanding why people play games.\n\nExtremely few people give a shit about raster being worse at lighting than raytracing. They want games to be fun and they want them to run well. Every single successful 3D game that has ever been released is rasterized, and chief complaints are either due to bugs or performance, not that the lighting isn't accurate enough.\n\nRaytracing can't make games more fun, and it makes them run worse. It is useful for *computer graphics*, but for games it's little more than a hat trick.\n\nThere might be a day where raytracing is just as performant as rasterization. The GeForce 20-series came out 6 years ago... so maybe in 10 years RT will be there?\n\nTo clarify, I'm not trying poo-poo raytracing as a *graphics technology*. There's obviously no argument that it looks better. But the limitation of its usefulness for games was and still is performance. Raster is faster, and when that day changes we can have this discussion again. Cheers.",
            "It's not a conspiracy. They straight up have embedded engineers at quite a few companies. Not all nvidia gpus that have driver support have the features many started using for compression. Engineers at nvidia will absolutely encourage you to leave less compressed assets for this reason.\n\nAlso it's not bleeding edge, dx12 allowed for many new forms of compression with essentially no performance hit for gpu decompression... except nvidia only supported dx12 is name only and not all it's features.  Using it absolutely mollywhopped performance.  It's almost as bad as mesh shades on a gpu with no support now.",
            "even now people are decrying not enough vram as a huge boogeyman, but that\u2019s not the actual case.",
            ">Kind of a tangent but most PS3 games actually include 4k textures on disk, and it down samples them when installing to the PS3 HDD..... they did this probably thinking forward to future PS3 Pro that never existed.\n\nThat's interesting. Probably also helped that the optical media they were using had enough space that hypotheticals were easy to entertain. \n\n> PS3 Pro probably never existed due to Nvidia also being crap to work with.\n\nProbably some other factors too. The cell was hugely unpopular among developers and it took the console awhile to gain proper traction because of difficulties on that end. By the time it gained traction it made more sense to do completely new hardware.",
            "> Eh, HD resolutions and no longer tiling a handful of textures put more of an end to traditional \"texture work\" more than technology did. A sizable number of textures are still drawn up artwork by artists. \n\nSure maybe if you work on a game like Valorant which has a more cartoony sort of art style. But not for a game like CoD.\n\n> Still need various types of artists as well to compose a scene as well.\n\nSure but with a game like CoD with a photorealistic art style, it's basically drag and drop a bunch of photogramm assets under crunch for these artists. They don't have the time or resources to sit there and optimise these textures and assets. Think about all the crap CoD has to ship these days, with all the bundles, the large maps, the character models etc and imagine each artist sitting there working maticulously on every texture. It's impossible with the amount of devs they have working on the game. So it's all automated by a computer to compress these things down. If you've ever seen a long range LOD for a building model on Warzone it looks like absolute junk, a square building can look round because a computer has just made a choice via an algorithm, rather than a human making a decent looking LOD that resembles the higher LOD asset.\n\n> Sure if they had a large amount of time to spend on each and every object manually modifying it instead of flipping their artwork right into game and moving on.\n\nThats right thats what being a texture artist used to be and thats why games back in the old days were better optimised and looked better too, you had to be an artist. They looked better because there was a unified art style crafted by people who had skill. \n\nNow it's \"scan this object in and drag and drop it in Unity/Unreal engine editor\", there's no skill in these things anymore and sometimes you have assets that look out of place because they were scanned in poorly or they simply do not fit the overall art style of the game or they have a mix of super high resolution assets and low resolution assets to save data. So in the same game you have a rock that looks like it has a 500 KB texture, it's blurry and looks terrible and then a car that has a 30MB texture and looks super detailed and sharp. But because it's drop and drag and done under crunch, it looks like trash.\n\n> How many unique textures do you think are in even smaller mid-budget games anymore?\n\nIn smaller or mid budget games, probably not many, because they don't have access to state of the art photogramm equipment to use. They have to hand build everything or use asset packs to speed up dev time. They also don't usually make very complex games because they know that they're small devs and they dont have the time or resources to make a large AAA like game. Maybe a few thousand textures at most.\n\nBut for AAA dev games like CoD, you could have hundreds of thousands of textures because there's so many assets in the game and you have to accomodate more platforms and customers.\n\n> There's also a reason pixel art 2D games are all but gone or using clever tricks to try and recreate the effects.\n\nThey're not gone per se they're just unpopular because in 2024 people expect more from games graphically. Plus there's only so many 2D sprite platform games that can be made before everyone tried every new or interesting design choice, the gameplay gets stale and old quickly.",
            ":'(",
            "Nono I think you missed the point. We need the higher performance level at lower prices, I really don't care about who provides that. Amd gpus are not that bad per se, yes they consume more etc, but they work pretty well. Issue is pricing, just like nvidia. We are stuck at the same performance per dollar.",
            "I mean like any sensible business they're manufacturing what is selling. Why aim for the top if customers at that price bracket prefer your competitor no matter what.",
            "Seeing as that is where 95% of all gamers are, at least according to steam surveys, I don\u2019t think that such a bad strategy. The top end cards this gen are selling extremely well, but iirc the top 5 or 6 fastest cards from both AMD and NVIDIA make up about 10% of the steam survey, at the most. Just looked it up. From the top end, the leader is the 4070ti with 1.3% and then the 4090 with 1%. 7900xtx sits at 0,4%. Only slightly above 5% even habe directx12 capable cards. \ud83e\udd37\ud83c\udffb\u200d\u2642\ufe0f\n\nSo if I was AMD I\u2019d rather sell 4 500$ cards then 1 for 1500. \n\nAnd that\u2019s why I find the whole \u201efutureproofing\u201c discussion so insane, too. Developers look at those surveys, too. And they see that over 80% of their customers sit at 8gb vram or lower. And since a lot of that is 30- and 40- series cards, as well as 5000/6000 series AMD, that likely won\u2019t change for the next 3-4 years. Normal people keep their machines for longer the we do. \nSo if you want to sell your game, you\u2019d better make sure it runs decently at 8Gb VRAM for a while. At least until the next console gen hits. Those have more VRAM, but it\u2019s shared. \ud83e\udd37\ud83c\udffb\u200d\u2642\ufe0f",
            "Not every country has the priviledge that usa has in terms of prices and used cards. Here prices basically never go down even for last gen and there are only a few used cards on the market.\nI get what you are saying though, obv they won't do it, doesn't mean it's not what we'd need.",
            "When people talk about \"competition\", it is generally in the context of AMDs performance being more equal to Nvidias, in which AMD has already showed that they only lower prices when they aren't competitive performance wise, or feature set wise. Since the parrots sqawk \"performance competition\" between the two, no one has actually said what they assume would occur as a benefit between performance competition. Perhaps they assume AMD will keep prices low so Nvidia will have to lower theirs. Fking doubtful. AMD is more likely to raise their prices being more performance competitive, not lower them, especially with their Radeon division down so low. Fans of course want \"performance competition\" because well, they love AMD. Whats the alternative to these two? If Intel becomes a major competitor, theres no evidence to suggest performance competition would do anything aside from driving up pricing to Nvidias level. None of the companies currently competing have showed they aren't chasing $$$ in the highest possible per unit, but will lower if not competing well enough. So I'm curious what the dream is? What do people assume will happen if AMD suddenly shrinks the performance gap with Nvidia?",
            "Exactly.",
            "Yeah they could do the 4090d thing where they make a gimmped version but from what I'm hearing that's not the plan, they intend to launch the 5080 before the 5090 so it can be international launch, then the 5090 later",
            "The more you buy the more you save!",
            "It was originally designed to upscale games hence the name. Frame generation was later added.",
            ">It's not a conspiracy. They straight up have embedded engineers at quite a few companies. Not all nvidia gpus that have driver support have the features many started using for compression. Engineers at nvidia will absolutely encourage you to leave less compressed assets for this reason.\n\nIf there were such a push it'd be to sell their newer GPUs not appease the people that haven't bought hardware since Pascal.\n\n>except nvidia only supported dx12 is name only\n\nIt's not 2016 anymore.",
            "The cell is entirely tangential to texture quality on PS3.... due to the fact that the cell itself does very little rendering in most games, it sometimes does 2D particle etc.... the real issue is non shared very small vram.\n\nAlso Cell just isn't \\*that\\* complicated. It's basically 1 normal core + 6 cores you can task with small tasks. All of them sharing the same instruction set + some extensions differing. Xbox 360 has most of the same programming difficulties, and less hardware to work with (only 3 cores instead of total of usable). The difficulty both consoles have is prior consoles only had 1 CPU + coprocessors in some cases.\n\nWhere xbox is probably easier is it has unified memory like modern consoles, were PS3 is more like a PC and has to upload to its GPU.\n\nAlso hugely unpopular is not true either.... PS3 game library is quite good. It was a slight learning curve from past generations that is all.",
            "Oh yeah you are right, my bad \ud83e\udd19",
            "if it is on a discount, you earn money!",
            "I understand, I mean specifically the \"lossless\" word. It is very disingenuous",
            "My dude, they still sell brand new gpus that don't support it lol.",
            "Early on a number of studios complained heavily about the cell and the general approach they had to take with the console. It very much inhibited things early on. \n\n>Also hugely unpopular is not true either....\n\nThe cell was very derided in the earlier years of the consoles lifespan. It did take awhile to gain traction and for developers to turn around on it. It very much had a slow start, but strong later years."
        ]
    }
}