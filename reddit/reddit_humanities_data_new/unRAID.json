{
    "Unraid 6.12.10 Now Available": {
        "title": "Unraid 6.12.10 Now Available",
        "score": 150,
        "url": "https://docs.unraid.net/unraid-os/release-notes/6.12.10/",
        "content": "This release reverts to an earlier version of the Linux kernel to resolve two issues being reported in 6.12.9. It also includes a 'curl' security update and corner case bug fix.\n\nNote: Unraid OS v6.12.x and all earlier versions are not vulnerable to the xz backdoor CVE-2024-3094.",
        "num_comments": 123,
        "comments": [
            "Just did the update from 6.12.9. No problem.",
            "https://forums.unraid.net/topic/160471-unraid-os-version-61210-available/",
            "Upgraded from 6.12.4 without issue.",
            "Works well for me. I was having some UI weirdness with 6.12.9. This is much better.",
            "This fixed some weirdness I had after the 6.12.9 upgrade where Plex library scans would result in movies and tv episodes disappearing and new stuff wouldn't show up even doing the \"Plex dance\" to try and get them to show. After updating Unraid to 6.12.10 and running another scan, 50 \"new\" movies came back.",
            "Lol. So many drama queens in this thread. Upgrade asap on patch releases, use caution on major releases.",
            "Split level manual no longer works with this upgrade",
            "Does anyone knows if it fix the macvlan related kernel panics ? Thx\nI'll upgrade in any cases btw",
            "It seems like there's a lot of confusion on the versioning terminology. As u/Solverz said: [semantic versioning ](https://www.geeksforgeeks.org/introduction-semantic-versioning/)structures updates as Major.Minor.Patch.\n\n  \nMajor updates are for breaking changes. Minor updates are for new features/changes that are backwards compatible. And patches are for fixes and stuff that is also backwards compatible with that major version.\n\n  \nSo appropriately this is a *patch* update which should have the lowest \"risk\" of breaking anything out of the version types. Annoying that I just upgraded from 6.12.8 the other day, yeah, because I would have otherwise skipped a rollback but I appreciate the prompt fixes.",
            "Worth doing the update if on 6.12.8?",
            "Upgraded from 6.12.9 no probs",
            "They must have just seen me update two days ago and are like ya now we can release a new version.",
            "Did this fix the unassigned devices SMB remote share issue?",
            "Gets me nervous when the first item in the changelog is to rollback something.  Guess I'll just stick to 6.12.6 for until something major gets added.",
            "After I click \"confirm and start update\" on the related pop-up the \"command line\" style pop up opens and shows this:\n\nplugin: installing: unRAIDServer.plg\n\nExecuting hook script: pre_plugin_checks\n\nplugin: downloading: unRAIDServer.plg ... done\n\nplugin: not installing older version\n\nExecuting hook script: post_plugin_checks\n\nAnd it's just stuck there... Does nothing else. Any suggestions? \ud83d\ude05\n\nI'm on 6.12.9.",
            "Does this resolve issue with drives missing on 6.12.9 update ?",
            "6.12.9 to 6.12.10 and now having issues with recognizing drives. When it first started, one of my NVMe drives in my cache pool was not detected. I restarted and now I've also lost one of my HDDs from my array. I've never had a drive or controller issue on this machine for the last 3 years. Seems too coincidental after the upgrade and with different controller interfaces (NVMe and SATA). Anyone else experience anything similar? I've seen a post on the unRAID FB Group that another user is experiencing issues with loosing their parity drive after the upgrade.\n\nEdit - Also posted this to the support forum.\n\nEdit 2 - Restarted again and all is normal with all drives showing. Odd....",
            "I\u2019m on 6.11.5 still. Would this be a good version to jump to",
            "And this is why I don't update right away.  I give it a few weeks.",
            "No problems here",
            "Update: I rebooted from the main dashboard instead of top right menu, everything is fine\n\nI\u2019m on a trial key with 25 days remaining, I went through the update steps and now I think I have to buy a registration key\u2026is this true? To attempt a reboot I am getting a prompt to activate with a key\n\nA little ambiguity here because the dialog says the trial has all the support and capability of a pro license (doesn\u2019t say anything about no updates for trial users)\n\nAm I missing something?",
            "6.12.8 to 6.12.10 - my Home Assistant VM is no longer fully booting, and reverting didn't help, fml",
            "Installed from 6.12.9. All good for now.",
            "I hope this update fixes the issue I am having with drive bays showing warnings because of data storage limits and then self resolving. They started after 6.12.8...didnt upgrade to .9 because of other operations but its annoying getting 120 emails a day thT 30 drives are approaching full capacity and then 30 drives are cleared as ok",
            "I tried uograding to 6.12.9 yesterday and received an error during the update.  Either my USB stick took a crap or the update jacked up my USB stick, causing it to not reboot.  I wound up having to create a new USB boot drive and replacing the key.  When I got it up and running 6.12.10 was available and I updated without any problem.\n\nThankfully, all is good and everything is working as it should.",
            "Dumb question but how do I pull this update? I just only got the 6.12.09 to show up on unraid for upgrade just 2 days ago.  After upgrading, it's not showing any other new updates to 6.12.10.",
            "Glad this went well for so many. 2 out of 3 of mine went way wrong, so YMMV. And Unraid's support is more miss than his, so have a Plan B!",
            "I updated directly from 6.12.8 to 6.12.10 and my Windows 11 VM stopped working.  \nWhen I look at the monitor (it has GPU passthrough) it's stuck on the Unraid UEFI logo.",
            "Can the pool limit be increased to 100 drives?",
            "finally an update",
            "\"Oh you had data going to cache first before the array? What a use of the name! Tell yah what, how about we ALSO have all the data from the array go to the cache?\" \n\nWhy does this glitch exist? My screw-up prior to the update? \n\nIs the only solution manually re-configuring all cache settings for each share?",
            "Am on 6.11.5\n\nCan i upgrade directly ?\n\nAlso what should i do as precaution before upgrading ?",
            "they're going the wrong direction. someone send them an uno reverse card.",
            "Ya know, a few updates ago I commented here about how I would NOT immediately update and was downvoted. Here we go again, I never trust an update until it's been live at least a few weeks. Will y'all down ote me this time, or do we now have consensus that day of update releases may not be wise...",
            "LTS branch when?",
            "I thought we were starting to turn a corner on bad releases being the norm, but no.",
            "I hate unraid no checksum or file integrity to solve sync issues",
            "6.12.10 huh?  7 days after 6.12.9?\n\nI'll just stay quiet and sip my tea. :-)",
            "Yes a rollback is really bad news",
            "I\u2019m on 6.12.9 and it seems more stable\u2026 gonna stay until 6.13",
            "Good thing you got lifetime free updates or their bad rushed update could be your last one and take down your server forever.  Rip unraid.",
            "Same, no issues so far.",
            "Updated this morning.",
            "Ugh, this screwed me as my library \u201ctrash\u201d was emptied after a scan on 6.12.9 and removed those \u201cmissing\u201d files from my library. New rescan shows a ton of old files as \u201crecently added\u201d now",
            "I had the problem with files stored on a network share - my Unraid machine would simply not list certain files and directories when mounted as SMB share. Hence also Plex did not pick those file up. Interim solution was to mount the share as NFS.  \nI thought that it had something to do with my NAS, but now that you mention that problem, I remember that it occurred after the .9 update.  \nGood to know, that .10 is fixing this behaviour.",
            "Same issue, I had to revert to 6.12.8.  So to confirm, this SMB issue is now fixed and gone with 6.12.10?",
            "As someone who just over the past 10 days or so has been swapping from Synology to unraid (6.12.10) and who found this thread when researching this error...it might not be fixed?  \nStill looking into it but basically a lot of shows have the trash can icon but files are fully accessible by the server.\n\nI am trying to avoid emptying the trash because I don't want to lose history/progress of shows but is that what ended up fixing it for you?",
            "> Upgrade asap on patch releases\n\nDid you miss the part where bugs were introduced in the last patch release? It's prudent to wait a couple of weeks for updates, even on patch versions, unless there's a critical security patch or you're having problems with the previous patch.",
            "Yeah like if you don't want to frequently upgrade, then don't? \n\nI just upgrade when I know I have free time to read the change log and (potentially) troubleshoot, because I assume i'll have problems.  It's a media server not a bank lol, I think I can survive being a couple minor releases behind latest.",
            "Just to be super clear, by major release you're meaning if we went to 6.13.1 right?",
            "You new here? Unraid track record for updates is in the shitter. That's why you see so many paranoid posts.\n\n\nDownvote all you want guys but all you have to do is scroll this sub for about 30 seconds before you find a post asking if an update is safe. Can't deny what's in front of you\ud83e\udd37",
            "Got the answer in the release notes, it does. Thx anyway",
            "Do you like security?",
            "Yep.",
            "What issue was this?",
            "Read the changelog",
            "as I remember, after hook script post\\_plugins\\_checks, it'S done",
            "Use forced flag",
            "I'm on the same boat\n\n    plugin: installing: unRAIDServer.plg\n    Executing hook script: pre_plugin_checks\n    plugin: downloading: unRAIDServer.plg ... done\n    \n    plugin: not installing older version\n    Executing hook script: post_plugin_checks\n\nAnd then nothing,  \nI'm on 6.12.8",
            "I lost one of my two parity drives after the upgrade also.  Restarting didn't bring it back so I had to remove it and readd/resync.  I upgraded from a few versions behind.  My system also did a parity check the night before without an issue.",
            "Probably, I\u2019d say it\u2019s one of the last versions before we see the next major (6.13)",
            "u/TheRealSeeThruHead  did you end up upgrading? did you have any issues?",
            "On the same version. I am afraid incase i skip these upgrades , some major upgrades will break many stuff.\n\nLet me know if you did upgrade or sticking to this version.. and also for how long ?\n\nAlready had issues with auto-upgrade for apps , now all apps are fixed and auto-updates disabled.",
            "As of v 6.9, you can have up to 35 named pools with 30 storage devices per pool: [https://docs.unraid.net/unraid-os/manual/storage-management/#multiple-pools](https://docs.unraid.net/unraid-os/manual/storage-management/#multiple-pools)",
            "Just like your down votes?",
            "Lol, I see you're at -1. While I think the downvotes are undeserved... If everyone followed your advice and waited a few weeks then the bugs wouldn't be found and addressed in that timeframe and you'd be advocating for a few weeks more. \n\nI think most people know of the potential for bugs in new releases. Some are willing to take the chance, others aren't. I think most of your downvotes are people who know that if no one updates, then there are effectively no updates, stable and bug free or not. \n\nBut that's just like, my opinion man.",
            "Well, I trust this update more since it fixes two things that were broken with the previous update.",
            "[hmm](https://media1.tenor.com/m/8_rf9sCPLp8AAAAd/sup-daily-twitch.gif)",
            "I'm happy to let someone else be the guinea pig. I'll update in a week or two if nobody's reporting any major issues.\n\nMost of the updates have gone smoothly for me, but there have been a few that have really messed up active directory integration, which doesn't get much love to begin with.",
            "I\u2019m downvoting you because you\u2019re bitching about votes.  I\u2019d be more willing to discuss the other points of your post but now I don\u2019t care\u2026Grow up and stop worrying about worthless internet points.",
            "I took away a down vote for ya.",
            "Is there LTS for unraid?  Usually people recommend waiting a few months after the major releases.",
            "It\u2019s coming soon more than likely. With their new update mechanism it\u2019ll be easier to maintain multiple branches.",
            "Won't happen. Stop asking.",
            "I'm just curious why you are here then?",
            "There is a major bug if you use any smb or nfs shares or unassigned devices plugin with 6.12.9.  It will spam your logs and random files may not show up.  6.12.10 fixes it.",
            "Ahh, that explains what I saw too then! Part of the \"Plex dance\" involves emptying the trash \ud83e\udd26\u200d\u2642\ufe0f",
            "Sorry yeah, I should have said, I'm using SMB too.",
            "Seems to be, for me at least! Ever since the upgrade I've not run into any Plex library related issues anyway.",
            "Major.Minor.Patch",
            "Yes. While technically /u/Solverz is right about the version digit designiations, in unRAID's case (and many projects that don't necessarily follow semantic versioning) a minor release is also like a major release. So a jump from 6.12 to 6.13 would necessitate caution IMO.",
            "* 6.13 = major \n* 6.13.1 = minor",
            "There are loads of people asking if an update is safe and very few people saying no. \n\nJust because loads of people are asking if it\u2019s safe, doesn\u2019t mean it\u2019s not.",
            "Elaborate?",
            "Basically random files on a mounted SMB share became unavailable.\nhttps://forums.unraid.net/bug-reports/stable-releases/6129-cifs-vfs-directory-entry-name-would-overflow-frame-end-of-buf-r2916/",
            "Not sure why, but even after rebooting, I'm still with 6.12.9 :((",
            "Where/how can I do that?\n\nThanks a lot!",
            "I found the culprit! There were some plugins I didn't update, which were holding back the OS-level update \ud83d\ude01",
            "ROFL at people correcting you like 6.0 didn't release literally 9 years ago.",
            "The next major release will be 7.0",
            "6.13 is not a major release it's a minor release.\n\n7.0 would be a major release.",
            "Yes I uograded. Haven\u2019t had any issues at all",
            "I need a 96 drive zfs pool, the current drive limit for a pool is 60 drives.",
            "Yeah, I'm pretty sure with something like Unraid, we're a user base who all have various preferences and understand whatever pros and cons our preferences carry.",
            "Also, it\u2019s not some secret knowledge to not upgrade immediately if you can\u2019t deal with potential issues. \n\nEvery release has multiple people saying the same thing \u201cI TOLD YOU GUYS NOT TO UPGRADE IMMEDIATELY\u201d as if anyone is disagreeing.",
            "Good point about if nobody updated. I never looked at it that way because there are always folks who like the bleeding edge and figured enough of them out there to catch the elusive stuff that QA didn't. It's easy enough to roll back if necessary.",
            "If I was worried about the votes, I would not have made the comment.",
            "No, just Current and Next branches. I have been asking for some time and have not received a response yet.  \n\nI don't need bleeding edge, I want stable.  Give me the same kernel for 4 years but still push critical/vuln patches.  I went from 6.11.5 to 6.12.6 and now every month a new patch is coming out.  \n\nI do appreciate the research, progress, and love that goes in to this and I don't want that to stop.  Just give the ultra cautious & lazy some LTS reassurance please. <3",
            "do let it be known unraid sucks and so does the so called devs on the discord channel. Stay far away and just invest in true nas scale",
            "because even though i have 2 unraid servers, and one true nas core server , the file system sucks on unraid and there is no information to tell you why you get sync errors\n\nThe only good reason to buy unraid is to upgrade your drives if you need more data, which i do 24 bays filled and constantly requiring more storage. Other than that the devs on the discord channel can suck a big one they are trash talking pieces of S\\*\\*\\*\\* with no help!  I am deleting my parity drives now just to do another sync check LOL. Its not hardware related but the crappy software",
            "Oh! I.. did not know this XD Thanks for letting me know!",
            "All my SMB shares disappeared when I updated. The SMB Exports were reset to No and I had to enable them again.\n\nArray write speed also seems a little degraded. I would hold off on upgrading.",
            "This is incorrect.",
            "You clearly don't do any digging. Just because most comments say yes doesn't mean it's automatically safe. Give the update a few days and sure enough you still got those same posts but now the comments are mixed results.",
            "read the change logs",
            "Thanks. This might be the reason my mounted backups are corrupted. What a pain in the arse.",
            "That's not normal. I would check log",
            "you can use cli command: plugin install PLUGIN-FILE \\[forced\\]",
            "That was it. Thanks!!",
            "6.13 is slated to have some major updates. It all depends on your perspective, a major update to me is any time features are added or something under the hood changes (e.g. the upcoming Linux kernel update). These generally cause enough instability or bugs that it can affect my work, so I\u2019d consider that a major update",
            "Thank you!",
            "I hear that!",
            "Any updates on if this was corrected? been waiting and watching what I can on when to upgrade from 6.12.8",
            "Please don\u2019t tell me you went to 6.12.9 lol smb shares don\u2019t work on 6.12.9, they quickly had to patch to 6.12.10.\n\nOtherwise, it might be an issue with the Realtek drivers?  I am not sure how that stuff works.  I have never needed to do anything like that.",
            "Can you help us with the correct way then?",
            "Super newbie here \ud83d\ude2c how do I check the log, and make sense of it?",
            "Well it\u2019s not really perspective. It\u2019s just how versioning is\n\n`Major.minor.patch`",
            "My shares were working just fine on 6.12.9 but when I updated to 6.12.10 the SMB Export value got reset to No. \n\nI am too lazy to troubleshoot the array speed issue because I am upgrading my hardware soon so I will look into it then.",
            "I have already commented with this.\n\nNonetheless, Major.Minor.Patch",
            "Sure, but only programmers care about those definitions. This person is likely interested if this version will be stable, the wording used to describe an update makes no difference.",
            "The issue with 9 was that it spams your logs every second or so about the smb and nfs shares.  A lot of people also had random missing files with unassigned device shares.\n\nSeems like you figured out the issue.  Hopefully this is more stable for you.",
            "Thanks",
            "Yes, but if Unraid\u2019s developers thought it was a major release, they would have bumped the version number accordingly. The obviously consider this a minor release."
        ]
    },
    "Unraid April Digest": {
        "title": "Unraid April Digest",
        "score": 30,
        "url": "https://newsletter.unraid.net/p/unraid-april-digest-014e",
        "content": "All Things Unraid for April 2024",
        "num_comments": 32,
        "comments": [
            "Anyone have thoughts on the integrated proxy manager? I'm new to unraid and just got Nginx working, would be curious to see if the integrated one is worth the switch.",
            "Looking forward to first party support for backing up VMs.\n\nIt would also be nice if they improved the GUI so that it doesn't override custom changes in the VM XML.",
            "When will 6.13 come? They said soon this month. Its almost end of the month.",
            "Today watched May Digest, so many improvements will be in Unraid 7 for ZFS when I decided to use only xfs for array and btrfs for cache \ud83e\udd23",
            "I wonder if it will be similar to Synology's which was pretty barebones.\n\nI ended up going with traefik on my unRAID and have it all integrated with my other containers, LetsEncrypt, and Cloudflare subdomains.",
            "I can\u2019t get Nginx working. Hopefully the integrated one works for me. I\u2019m going to set up OPNsense and run ACME and HAproxy that way but if the integrated one works I\u2019ll try that",
            "I have a good feeling about next week.",
            "HAProxy kinda sucks compared to Nginx Proxy Manager. What couldn't you get working?",
            "any updates? its june already?",
            "What about next week? Still good feelings?",
            "Can you please confirm which month and which year? because its already been 2 weeks, Thanks!",
            "looking forward to it :D",
            "When?**\u2122** :D",
            "any Info?",
            "What is wrong with HAProxy? I've been using it for years without issue.",
            "Anything. It just didn\u2019t work. It set up a DNS challenge with cloudflare and it showed as active or whatever but none of the domains worked",
            "still waiting?",
            "Some of the backends I was testing just refused to work. They would give a 503 Error and say there was no server to fulfill the request, when that was not the case.",
            "In cloud flare you need to turn off \u2018auto forward everything into ssl\u2019 as this causes an infinite 503 loop.",
            "I haven't done anything with DNS challenges not sure there.",
            "Yep still waiting :/",
            "Fair enough. I had a little trouble setting it up initially. Now, I just copy/paste and modify from my working config and never have problems.",
            "Yeah I have no interest in exposing anything externally to the internet so 0% chance I\u2019m doing anything other than dns Challenges. I followed multiple tutorials. They all were basically the same instructions anyway, and not one worked. \n\n\nI\u2019m hopeful that it\u2019ll work on OPNsense",
            "Can you show me your config (with any keys redacted, obviously) \n\n\nI\u2019m a super noob and having a baseline to go off would be helpful. \n\n\nNo worries if not though",
            "You'd still have to allow ports 80/443 on the WAN address if you want them to be accessible remotely.",
            "Edit: Sorry, I had to figure out how to format this!\n\nSure, here is my config. I use pfSense, and it generates the config for me. There is a shared-http and shared-https that listen on 3 public ip\u2019s. I have 7 front ends defined. All of them use ssl offloading. The lua script is for acme cert renewals. Some of the frontends do redirect if the www is missing. They all but one do redirect to https if the request is over http. Some of the well known stuff was for nextcloud. Same with the ciphers. There are 7 back ends defined. For 2 of them, I use forward for option to pass through the ip of the requests.\n\n    # Automaticaly generated, dont edit manually.\n    # Generated on: 2024-04-07 02:29\n    global\n    \tmaxconn\t\t\t100\n    \tstats socket /tmp/haproxy.socket level admin  expose-fd listeners\n    \tuid\t\t\t80\n    \tgid\t\t\t80\n    \tnbthread\t\t\t1\n    \thard-stop-after\t\t15m\n    \tchroot\t\t\t\t/tmp/haproxy_chroot\n    \tdaemon\n    \ttune.ssl.default-dh-param\t2048\n    \tserver-state-file /tmp/haproxy_server_state\n    \tlua-load\t\t/var/etc/haproxy/luascript_acme-http01-webroot.lua\n    \n    listen HAProxyLocalStats\n    \tbind 127.0.0.1:2200 name localstats\n    \tmode http\n    \tstats enable\n    \tstats admin if TRUE\n    \tstats show-legends\n    \tstats uri /haproxy/haproxy_stats.php?haproxystats=1\n    \ttimeout client 5000\n    \ttimeout connect 5000\n    \ttimeout server 5000\n    \n    frontend shared-http\n    \tbind\t\t\tfirst_ip_here:80 name first_ip_here:80   \n    \tbind\t\t\tsecond_ip_here:80 name second_ip_here:80   \n    \tbind\t\t\tthird_ip_here:80 name third_ip_here:80   \n    \tmode\t\t\thttp\n    \tlog\t\t\tglobal\n    \toption\t\t\thttp-keep-alive\n    \toption\t\t\tforwardfor\n    \tacl https ssl_fc\n    \thttp-request set-header\t\tX-Forwarded-Proto http if !https\n    \thttp-request set-header\t\tX-Forwarded-Proto https if https\n    \ttimeout client\t\t30000\n    \tacl\t\t\tno-redirect\tvar(txn.txnhost) -m str -i www.example2.com\n    \tacl\t\t\tno-redirect\tvar(txn.txnhost) -m str -i example2.com\n    \tacl\t\t\turl_acme_http01\tvar(txn.txnpath) -m beg -i /.well-known/acme-challenge\n    \tacl\t\t\tredirect-to-https\tvar(txn.txnhost) -m str -i www.example.com\n    \tacl\t\t\tredirect-missing-www\tvar(txn.txnhost) -m str -i example.com\n    \tacl\t\t\tstage.api.example.com-acl\tvar(txn.txnhost) -m str -i stage.api.example.com\n    \tacl\t\t\tredirect-request.example-acl\tvar(txn.txnhost) -m str -i request.example.com\n    \tacl\t\t\tredirect-to-https\tvar(txn.txnhost) -m str -i cloud.example.com\n    \tacl\t\t\tredirect-to-https\tvar(txn.txnhost) -m str -i api.example.com\n    \thttp-request set-var(txn.txnhost) hdr(host)\n    \thttp-request set-var(txn.txnpath) path\n    \thttp-request use-service lua.acme-http01  if  METH_GET url_acme_http01 \n    \thttp-request redirect scheme https  if  redirect-to-https \n    \thttp-request redirect code 301 location https://www.%[hdr(host)]%[path]  if  redirect-missing-www \n    \thttp-request redirect code 301 location https://%[hdr(host)]%[path]  if  redirect-request.example-acl \n    \tuse_backend example-dev_ipvANY  if  no-redirect \n    \tuse_backend stage.api.example.com_ipvANY  if  stage.api.example.com-acl \n    \n    frontend shared-https-merged\n    \tbind\t\t\tfirst_ip_here:443 name first_ip_here:443  no-sslv3 no-tlsv10 no-tlsv11 no-tls-tickets ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384 ssl crt-list /var/etc/haproxy/shared-https.crt_list  \n    \tbind\t\t\tsecond_ip_here:443 name second_ip_here:443  no-sslv3 no-tlsv10 no-tlsv11 no-tls-tickets ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384 ssl crt-list /var/etc/haproxy/shared-https.crt_list  \n    \tbind\t\t\tthird_ip_here:443 name third_ip_here:443  no-sslv3 no-tlsv10 no-tlsv11 no-tls-tickets ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384 ssl crt-list /var/etc/haproxy/shared-https.crt_list  \n    \tmode\t\t\thttp\n    \tlog\t\t\tglobal\n    \toption\t\t\thttp-keep-alive\n    \toption\t\t\tforwardfor\n    \tacl https ssl_fc\n    \thttp-request set-header\t\tX-Forwarded-Proto http if !https\n    \thttp-request set-header\t\tX-Forwarded-Proto https if https\n    \ttimeout client\t\t30000\n    \tacl\t\t\taclcrt_shared-https\tvar(txn.txnhost) -m reg -i ^www\\.example\\.com(:([0-9]){1,5})?$\n    \tacl\t\t\texample2-acl\tvar(txn.txnhost) -m str -i www.example2.com\n    \tacl\t\t\texample2-acl\tvar(txn.txnhost) -m str -i example2.com\n    \tacl\t\t\texample-acl\tvar(txn.txnhost) -m str -i www.example.com\n    \tacl\t\t\texample-www-redirect-acl\tvar(txn.txnhost) -m str -i example.com\n    \tacl\t\t\texample-caldav-acl\tvar(txn.txnpath) -m beg -i /.well-known/caldav\n    \tacl\t\t\texample-carddav-acl\tvar(txn.txnpath) -m beg -i /.well-known/carddav\n    \tacl\t\t\texample-wellknown-acl\tvar(txn.txnpath) -m beg -i /.well-known\n    \tacl\t\t\tstageapiexamplecom-acl\tvar(txn.txnhost) -m str -i stage.api.example.com\n    \tacl\t\t\trequest.example-acl\tvar(txn.txnhost) -m str -i request.example.com\n    \tacl\t\t\tcloud.example.com-acl\tvar(txn.txnhost) -m str -i cloud.example.com\n    \tacl\t\t\tapi.example.com-acl\tvar(txn.txnhost) -m str -i api.example.com\n    \thttp-request set-var(txn.txnhost) hdr(host)\n    \thttp-request set-var(txn.txnpath) path\n    \thttp-request redirect code 301 location https://www.%[hdr(host)]%[path]  if  example-www-redirect-acl \n    \thttp-request redirect code 301 location /remote.php/dav/  if  example-caldav-acl \n    \thttp-request redirect code 301 location /remote.php/dav/  if  example-carddav-acl \n    \thttp-request redirect code 301 location /index.php%[path]  if  example-wellknown-acl \n    \thttp-response del-header X-Robots-Tag  if  example-acl \n    \thttp-response add-header X-Robots-Tag \"noindex, nofollow\"  if  example-acl \n    \tuse_backend example-dev_ipvANY  if  example2-acl \n    \tuse_backend example.com_ipvANY  if  example-acl \n    \tuse_backend stage.api.example.com_ipvANY  if  stageapiexamplecom-acl \n    \tuse_backend ombi_ipvANY  if  request.example-acl \n    \tuse_backend cloud.example.com_ipvANY  if  cloud.example.com-acl \n    \tuse_backend api.example.com_ipvANY  if  api.example.com-acl \n    \n    frontend office\n    \tbind\t\t\tfirst_ip_here:9980 name first_ip_here:9980  no-sslv3 no-tlsv10 no-tlsv11 no-tls-tickets ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384 ssl crt-list /var/etc/haproxy/office.crt_list  \n    \tmode\t\t\thttp\n    \tlog\t\t\tglobal\n    \toption\t\t\thttp-keep-alive\n    \toption\t\t\tforwardfor\n    \tacl https ssl_fc\n    \thttp-request set-header\t\tX-Forwarded-Proto http if !https\n    \thttp-request set-header\t\tX-Forwarded-Proto https if https\n    \ttimeout client\t\t30000\n    \tacl\t\t\toffice-acl\tvar(txn.txnhost) -m str -i www.example.com\n    \thttp-request set-var(txn.txnhost) hdr(host)\n    \tdefault_backend office_ipvANY\n    \n    backend example-dev_ipvANY\n    \tmode\t\t\thttp\n    \tid\t\t\t101\n    \tlog\t\t\tglobal\n    \toption\t\t\tlog-health-checks\n    \thttp-check\t\tsend meth GET uri /service/api/healthchk ver HTTP/1.0\\r\\nHost:\\ www.example2.com\\r\\nAccept:\\ */*\n    \ttimeout connect\t\t30000\n    \ttimeout server\t\t30000\n    \tretries\t\t\t3\n    \tload-server-state-from-file\tglobal\n    \toption\t\t\thttpchk\n    \tserver\t\t\texample-dev 10.7.3.14:80 id 102 check inter 30000  \n    \n    backend stage.api.example.com_ipvANY\n    \tmode\t\t\thttp\n    \tid\t\t\t106\n    \tlog\t\t\tglobal\n    \ttimeout connect\t\t30000\n    \ttimeout server\t\t30000\n    \tretries\t\t\t3\n\tload-server-state-from-file\tglobal\n\tserver\t\t\tstage.api.example.com 10.7.3.14:808 id 102  \n\n    backend example.com_ipvANY\n    \tmode\t\t\thttp\n    \tid\t\t\t100\n    \tlog\t\t\tglobal\n    \thttp-response set-header Strict-Transport-Security max-age=15552000;\n    \ttimeout connect\t\t30000\n    \ttimeout server\t\t30000\n    \tretries\t\t\t3\n    \tload-server-state-from-file\tglobal\n    \tserver\t\t\texample.com 10.7.3.15:80 id 102  \n    \n    backend ombi_ipvANY\n    \tmode\t\t\thttp\n    \tid\t\t\t107\n    \tlog\t\t\tglobal\n    \thttp-response set-header Strict-Transport-Security max-age=15552000;\n\ttimeout connect\t\t30000\n\ttimeout server\t\t30000\n\tretries\t\t\t3\n\tload-server-state-from-file\tglobal\n\tserver\t\t\tombi 10.7.3.10:3579 id 108  \n\n    backend cloud.example.com_ipvANY\n    \tmode\t\t\thttp\n    \tid\t\t\t105\n    \tlog\t\t\tglobal\n    \thttp-response set-header Strict-Transport-Security max-age=15552000;\n    \ttimeout connect\t\t30000\n    \ttimeout server\t\t30000\n    \tretries\t\t\t3\n    \tload-server-state-from-file\tglobal\n    \toption forwardfor\n    \tserver\t\t\tcloud.example.com 10.24.1.11:80 id 102  \n    \n    backend api.example.com_ipvANY\n    \tmode\t\t\thttp\n    \tid\t\t\t109\n    \tlog\t\t\tglobal\n    \thttp-response set-header Strict-Transport-Security max-age=15552000;\n    \ttimeout connect\t\t30000\n    \ttimeout server\t\t30000\n    \tretries\t\t\t3\n    \tload-server-state-from-file\tglobal\n    \toption forwardfor\n    \tserver\t\t\tapi.example.com 10.24.1.10:80 id 102  \n\n    backend office_ipvANY\n    \tmode\t\t\thttp\n    \tid\t\t\t103\n    \tlog\t\t\tglobal\n    \ttimeout connect\t\t30000\n    \ttimeout server\t\t30000\n    \tretries\t\t\t3\n    \tload-server-state-from-file\tglobal\n    \tserver\t\t\toffice 10.7.3.16:9980 id 104 ssl  verify none",
            "I don\u2019t want them accessible remotely.",
            "Oh well you don't need Cloudlflare to do that. Just create local DNS entries that point to your reverse proxy for the services you need.\n\nSay your local domain is whatever.home\n\nCreate DNS entry for service.whatever.home pointed at the IP of your reverse proxy\n\nThen on the reverse proxy create service.whatever.home pointed at the IP/port of the service on unRAID\n\nIt gets a little more complex if you want SSL certificates but you don't really need that if it's all local",
            "That doesn\u2019t give me HTTPS I\u2019m pretty sure. Which I need for some services."
        ]
    },
    "Unraid vs Windows Server": {
        "title": "Unraid vs Windows Server",
        "score": 3,
        "url": "https://www.reddit.com/r/unRAID/comments/1dls829/unraid_vs_windows_server/",
        "content": "I have an Unraid server with 4 20tb drives and a 2TB cache drive, 16gb of memory, i9-9900k.\n\nPlanning on installing a 10G nic in the unraid server and create a peer to peer network with my workstation- TR 7090. ",
        "num_comments": 7,
        "comments": [
            "Don't go with Windows server. Aside from licensing, it's very unlikely to be the best choice for you. The thing that makes Windows good are the domain services. If you have AD DS setup, then yeah, use Windows Server for your file server. \n\nSince I'm guessing that's not the case, you should probably be looking at TrueNAS or Unraid. Or, if you want to dive deeper, setting up shares on a headless Linux distro (Debian, Ubuntu, etc), setting up Proxmox, etc.",
            "Forgot to mention, I\u2019m looking for some guidance. If I go with the 10g nic< should I do rj45 or melanocyte connection. I will be using the unraid server for production type work",
            "What are you doing, videos? Might want to consider bigger or array cache drives... That way you can use the full 10g, otherwise you'll fill up your cache and the spinning drives will max out super fast.",
            "RJ45 is good for 30m with cat5e/cat6 as long as you have a decent cable. \nHowever. It's best to buy a network card with SFP+ as you can change the connection type as you please. Plenty of second hand network cards available",
            "Honestly, you\u2019re using spinning drives. Unless you\u2019re pulling to/from the nvme drive at max capacity that 10g isn\u2019t going to help you any. But to keep things simpler, heat way down, I went with the qnap Qhora 322 router as an all-in-one and kept it rj45.  I\u2019d definitely jump for more than 1g but going for 10g, at least in my case, doesn\u2019t serve any purpose when I\u2019m WAY limited by IO of the drives.",
            "Thanks for info.  If I went with a sub $300 setup - 10t nic with sfp+, do you have any recommendations/. I have a threadripper TRX50 motherboard that has 2.5 and 10G networking. Does it make sense to go with SFP_",
            "Oh, if you have it built in then you're fine. \nI was only recommending SFP+ if you needed to buy an add in card. \nJust use what you have unless you need to do over 30m cable run off course, then you will need fibre."
        ]
    },
    "Turning old PC into server (noob here)": {
        "title": "Turning old PC into server (noob here)",
        "score": 2,
        "url": "https://www.reddit.com/r/unRAID/comments/1dltrqu/turning_old_pc_into_server_noob_here/",
        "content": "Hi everyone,\n\nI want to turn my old PC into a server for Plex, storage and a Minecraft (private) server. The parts I already own are:\n\n* Intel Core i7 4790K\n* MSI Z87-G45 Gaming motherboard\n* Kingston HyperX Fury 2x8GB DDR3 1866 MHz\n* 16 TB Seagate Ironwolf PRO ST16000NT001\n* 2x8 TB Seagate Ironwolf NAS ST8000VN004\n* 3 TB Seagate Barracuda ST3000DM001\n* GPU ASUS R9 290X 4GB (if needed)\n* Corsair Carbide Series 200R case\n\nWhat I plan to buy:\n\n* Samsung 970 Evo plus (to use with an ASUS Hyper M.2 expansion card) (BIOS flashed to work with NVME)\n* PSU (haven't decided yet probably a Seasonic \\~500W)\n* Noctua NH-L9i CPU cooler\n\nDoes this setup make sense? How would you configure the harddrives? I need one HDD for parity and backup, right? I've never build a server before so any suggestions are very welcome. ",
        "num_comments": 5,
        "comments": [
            "Yes, you need one HDD for parity, but not backup. Backup should be somewhere else. You must choose 16tb as parity because other disks cannot be larger than your parity disk.",
            "Yes those parts are all fine. I'd suggest reading some Unraid guides or the many videos for installing fresh, esp from SpaceInvaderOne channel, just to get an idea.\n\nUnraid uses the largest drive as parity - the thing is any extra space on it will go to waste.\n\ne.g. if you want to use your current drives you must use the 16TB one as parity but then 8TB of it goes to waste. If you happen to buy another 16TB drive (maybe a refurb one on sale) then you can use that as a data drive, have it fully protected as well. \n\nMake sure your motherboard and case have enough SATA ports and slots to fit all the drives you need.",
            "Thanks for the feedback. Does that leave the other 3 HDDs for storage then?",
            "Thanks for the comment and suggestion. I'll definitely be expanding with a 16 TB drive at some point but that will also mean a new case. Checking out SpaceInvaderOne right now and it's just what I need!",
            "Yes"
        ]
    },
    "Where is unRaid 7/6.13?  ": {
        "title": "Where is unRaid 7/6.13?  ",
        "score": 29,
        "url": "https://www.reddit.com/r/unRAID/comments/1dlcjh9/where_is_unraid_7613/",
        "content": "Does anyone know what is going on with the release of unRaid 7/6.13?  u/unraidOfficial ?\n\nThere hasn't been much talk since the April Digest thread about when this release is coming out (which turned out to not be the case).   There was no May Digest thread, even though June is 3/4s over now.\n\nI hope the silence is just them being heads down trying to iron out all the bugs, but I worry that something else might be happening.   I've seen this happen when a merger or sale occurs and the company goes silent until it's announced.\n\nI just want Intel Arc support!   ZFS stuff would be cool too, I guess.",
        "num_comments": 95,
        "comments": [
            "I want VM snapshots",
            "Of course they could have a public roadmap, but I'm fine for them being silent as long: 1. Security problems are taken care ASAP and 2. The overall quality of releases is good.\n\nThe only wish I have is if they could release the latest stable [kernel](https://kernel.org/) once in a while that is not bound to a fixed/frozen release, so that people can at least use newer hardware on the spot.",
            "Unraid 7 has been teased by spaceinvader one over the last while. And has been in closed beta for a while now. \n\nI have no issue with the wait. It'll be the same people who are rushing the release that will complain about it being rushed when it has bugs",
            "Two weeks. (TM)",
            "They won\u2019t admit it, but they\u2019ve been dropping the ball for a while now on communication. \n\nThe marketplace Unraid operates in is getting more contested and Unraid needs to do better, much better. \n\n-TrueNAS scale, a free and open source platform, is switching to docker at its core. \n\n-CasaOS is getting traction and attention, and is again, free and open source. \n\n-HexOS, which is apparently made by some ex Unraid devs, partially funded by Linus from LinusTechTips fame is working to have a low barrier to entry middleware based upon TrueNAS.\n\nUnraid is the \u201cpremium\u201d product from a price perspective but is let down by a poor community management team, which is sad to see happen to an app built on the considerable effort of it\u2019s community developers.",
            "As far as I know, they don't commit to release dates. It'll be out when it's ready.",
            "I'd rather Unraid wait until it's ready than push it out in a rush and end up causing issues. There are a lot of ZFS changes coming, so not really a surprise they are taking their time.",
            "What I need is a NAS that will support \"Intel ARC A380\", I don't care who they are at this point. I bought the parts for a DIY NAS so I can move my Plex server over to it, so I can do transcoding on the fly.",
            "Where are the promised 'faster updates and new features' ??\n\nThey doubled the price, got millions in sales, and its been complete silence. \n\nthe last interview with the founders promised only ZFS features. I don't want ZFS, its completely pointless for home users. I want them to fix the slow speeds, fix bugs and keep it simple.",
            "It's kinda comical how many times they've said beta is \"coming soon\" at this point",
            "Isn\u2019t it on beta?",
            "I'm fine with delaying stuff to solve bugs (obviously) but this feels to me like they don't want to commit to any features. If you don't formally announce and communicate things, there is also nothing you have to take back if you can't deliver.",
            "https://newsletter.unraid.net/p/unraid-may-digest-719f from May31 they say they are working overtime to get some last minute bugs fixed before a public beta. This is the May digest?\n\n\nNot a lot of talk, but at least something past April. And I guess that's how bugs are? They shouldn't be there, so it's hard to put exact timing on when you find the root and how hard it's to fix.",
            "For comparison, here\u2019s TrueNAS communication on their next release:\n\n[https://forums.truenas.com/t/the-future-of-electric-eel-and-apps/5409](https://forums.truenas.com/t/the-future-of-electric-eel-and-apps/5409)\n\nThe Future of Electric Eel and Apps\nThere\u2019s been some great progress on Apps in SCALE. Kubernetes (K3S) has been integrated. A catalog system has been built to enable easy web UI management of apps. ZFS has been modified to support Docker OverlayFS to better handle snapshots from containerized applications, and custom Docker containers can be downloaded and run directly via the web UI.\n\nHowever, one thing has been missing until now: Docker Compose scripts cannot be used for importing applications.\n\nThe TrueNAS community has been forced to build Apps using Helm charts. Helm charts are powerful, but they are more complex and less portable. There\u2019s been lots of feedback from the community that native Docker and Docker Compose is preferred, and that K3s and Helm charts have been too complex and less reliable. It\u2019s clear that Docker is the industry standard for building containerized applications. TrueNAS users have been vocal.\n\n\u201cAdd native Docker support. It\u2019s the most requested feature for TrueNAS.\u201d (Quentin J.)\n\n\u201cInclude easier or more streamlined Docker support out of the box Docker.\u201d (Primoz R.)\n\n\u201cI really do wish TrueNAS had a clean option for more pure Docker/Compose use.\u201c (James P.)\n\niX Listens\nOne of TrueNAS\u2019s primary goals is to create an environment that is easy to use and innovate in. We want it to be easy to import and export Apps and their data. This is part of our promise of \u201cTrue Data Freedom.\u201d\n\nOver the last six months, we have been working to meet the Community demand for native Docker Compose while minimizing the disruption of existing App deployments. We now have a solution and a plan for Electric Eel, which we\u2019re ready to share now.\n\nElectric Eel\nThe current version of TrueNAS SCALE is Dragonfish (24.04). The first update, 24.04.1, has achieved the quality needed for it to be our most successful release ever, with over 35,000 users already running on Dragonfish.\n\nThe next version of TrueNAS SCALE is Electric Eel (24.10), due in Q4 this year. It\u2019s been in active development for many months and will be BETA in Q3 this year. It\u2019s a very exciting and feature-packed release that\u2019s set to include:\n\nOpenZFS 2.3 with RAIDZ expansion and Fast Dedup\nWeb UI improvements including a Global Search, and better dashboards\nWeb-based installation of TrueNAS\nIntegrated Cloud backup of TrueNAS\nAdditional security features and dozens of other improvements\nNew iX platforms and capabilities\nAnd now \u2026.Native Docker and Docker Compose support for Apps\nWe expect that users will be very excited by the complete package. As we progress to BETA there will be a more complete write-up of what is included.\n\nDocker Compose Introduction\nWe\u2019ve thought long and hard about how to introduce Docker Compose support and have developed a plan that minimizes disruption. It will include moving the Apps infrastructure from K3S to Docker. Docker and Compose will simplify portability and innovation of Apps.\n\nAll of the TrueNAS Apps catalog will migrate to Docker Compose without requiring users to take any manual actions; all current users will need to do is update the system to Electric Eel when it launches. Each TrueNAS installation will identify the Apps running and automatically migrate to using the new Docker Compose App from the Catalog. Those updated Apps will use exactly the same datasets and configuration options as used previously.\n\nAfter this update, new Apps can come from the TrueNAS Catalogs or can be installed as traditional Docker Compose applications using standard YAML config files. TrueNAS will fully support the gamut of industry standard Docker and Compose application deployments. With this process, it will be vastly easier for TrueNAS users to build or import more complex Apps from upstream sources.\n\nLong-Term Kubernetes Support\nFor anyone who wants to continue using Kubernetes (K3s or K8s), we recommend using TrueNAS Sandboxes 474. This transition can happen in Dragonfish before updating to Electric Eel. Kubernetes instances within a Sandbox have full APIs available and can also be configured for clustering if the user so chooses\n\nThird-party catalogs will have an option of continuing to maintain and support their Helm chart Catalogs via deployment of K3s in a sandbox. Contact us if you need guidance on this transition.\n\nNext Steps\nThe engineering team will be merging code and updating the catalogs over the next two months. There will be no impact on existing Dragonfish Apps. When the new Apps system in Electric Eel is functional, we will alert the developer community and provide links to a nightly image. However, open testing will likely begin with a BETA version, expected in August.\n\n:warning: Do not update your system to a Nightly developer image now and expect your Apps to work. Please wait until iX provides an all-clear message. :warning:\n\nWe\u2019ll help the community establish best practices for transitioning Apps as we progress through Nightly and BETA testing. This process should be easy for TrueNAS Apps, but will likely require some examples and thought for private and third-party Apps.\n\nUsers can continue deploying TrueNAS Apps on Cobia or Dragonfish and expect a straightforward migration to Electric Eel. For those that want Docker Compose applications now, a native Docker runtime can be run in a Linux sandbox today. If you like this new direction, please cheer us on and let us know how we can improve it.",
            "We have subscription now.",
            "Odd release naming, extended release cycles and poor communications has been SOP for limetech in the 10+ years I've been using unraid. Why would they change that now?",
            "I personally don't care about zfs. But new kernel, updated qemu will be great.",
            "Sorry guys. This one\u2019s on me. I just extended my thirty day trial. They\u2019re waiting for me to pull the trigger before release.",
            "Three weeks ago they said \"We're resolving a few last-minute issues before go public.\" I think that was the last update.  They're not good at communication.\n\nI don't really have a problem with them not announcing when it's coming out, that's their prerogative.  But it's preposterous for them to not have some of these features ready by now.  Not having Intel GPU support at this point is insane; that should have been built into a more minor release if it was going to take this long for the full Unraid 7.  Or, just expand the beta and let people who want to try the features be able to try them and ignore complaints as it's beta.",
            "They definitely have been teasing things too much for how long it is taking to release. The latest beta for 6.13 was released at the end of April. \n\nFigured they would have at least done a point release since changing the license model since I'm sure they sold a ton of old licenses before the switch.",
            "The hilarity of people defending Unraid as... it's always been like that is just preposterous. \n\nIt is my opinion that all is not well. A recent licence model change with massive price increase...\n\nShoddy comms since April which promised updates soon but haven't delivered.\n\nSomething is afoot. The licence model change looks like an attempt at a cash grab and to demonstrate growth and recurring income to a potential investor/buyer.\n\nI honestly wouldn't be surprised if we're about to get boned in a really bad way.\n\nRemember...a lifetime licence is only as long as they keep operating or can be bothered.\n\nThe thoughts above are a major contribution to why I've not bought it yet. I'll keep operating a trial until I see improvement.",
            "6.13 is in beta I know for sure, they should have an rc \"soon\"",
            "For those wanting intel arc support, there's a github with modified kernel for the current release of unraid. I've been running it for a while now, all you do is backup your thumb drive, overwrite the files on your thumb drive with the files from the zip and then boot back up again. \n\nhttps://github.com/thor2002ro/unraid_kernel/releases",
            "I've played with upgrading the kernel...  And it works on the Ugreen NAS device I've been playing with.  I put an Arc 310 and it works just fine their (plex, haven't tried tdarr, yet)\n\nHaven't tried on my main unraid server,  but will probably try this weekend.\n\nhttps://github.com/thor2002ro/unraid_kernel/releases\n\nBack up you usb then copy the file from this over and use your ARC card.",
            "There's an unofficial kernel version that has provided Intel Arc drivers that's been maintained for like three years now. I think it was released by an alias like Thor or Zeus or something. Sorry, too lazy to Google for you on my phone.",
            "Believe the big issue is the switch to the new Linux kernel.",
            "If something is wanted. I got the information We all should start forum posts/topics\u2026 nothing will happen right now\u2026.",
            "Who\u2019s going to skip 6.13 cuz of the good luck number?",
            "for a company that is now charging an absurd price (for developing country users), unraid surely is droping the ball",
            "[deleted]",
            "Not updating the kernel frequently is very annoying.",
            "security updates are not taken care of. Latest 6.1 kernel is **6.1.95**. Unraid is **6.1.79.**",
            "Is there a way to get into these \"Closed beta's\" or are they internal only?",
            "It's the famous 90-90 rule of programming:\n\n*\"The first 90 percent of the code accounts for the first 90 percent of the development time. The remaining 10 percent of the code accounts for the other 90 percent of the development time.\"*",
            "As soon as another system with as easy to use docker support and the ability to go \u201cI\u2019d like more storage. Let me go buy a hard drive, chuck it in, and let my system worry about it\u201d comes along, I\u2019ll probably leave unraid.",
            "What OS does have good communication about releases? I only follow unraid properly. But who should unraid aspire to be like in terms of communication about releases?\n\nAs far as I can tell there aren't any that are amazing with release communication. Again I don't follow any nearly as much as I follow unraid so I'd like to be corrected.\n\nYou just have to look at the gaming industry the last few years with the release dates being communicated so early. Then they can have massive delays or worse still releases of games that's not even close to finished. Sometimes less communication in that regard means less expectations and as a result less disappointment.",
            "TrueNAS is expensive since it can only use zfs and you can\\`t as of today just add another drive of any size when you feel like it tho.  \n  \nHexOS seems to become just an beginner-friendly \"skin\" of TrueNAS, kinda disappointing, was expecting something more custom, so it will have all the drawbacks for TrueNAS.",
            "Oo the OS Linus funded is public now? Gotta check that out, didn\u2019t know anything was announced",
            "I\u2019ve been on unraid for about 14 years now, I tested truenas scale when it came out but the lack of docker compose really killed it for me, if they do switch the foundation to docker it will be hard to say no.",
            "Yeah, I mean that's pretty much a given for any software company and doesn't really address my post per se.   Also, they did comment and give a hint in the April digest threat when asked and they said \"they are feeling good about next week\" or something like that.   Obviously, that didn't happen and it's been very silent since then.   I'm more concerned that there hasn't been a May Digest thread and we are almost done with June.",
            "That isn\u2019t how good tech companies work. Good products get updates daily. These slow batched releases are the death of innovation and quality.",
            "All servers can do transcoding on the fly. It just won't do hardware transcoding. If you don't need high numbers of concurrent transcodes you should go ahead and build the server. If you still need the a380 you can follow any of the guides to use the updated kernal. Then when the update comes out just upgrade as normal. This shouldn't hold you up",
            "+Mobile UI",
            "100%",
            "Just because you don\u2019t want ZFS doesn\u2019t mean other people don\u2019t. I use Unraid because it has ZFS, I was using TrueNAS before. I would 100% drop Unraid if it weren\u2019t for ZFS and I\u2019m a home user.",
            "Zfs is nice man",
            "Yes and the CEO is nearing retirement age too, so it might really be going down like this. They've basically gone radio silent since pushing the new licensing model, you'd expect a company that just shoved a new financial model down their customer's throats to try to make their customers feel it was at least worth it, unless... they just don't care because they have got another iron in the fire.",
            "Everyone.  Pretty sure they've already communicated that the next version will be 7.0 not 6.13.",
            "that\u2019s the equivalent of saying you can make your car more fuel efficient by engine swapping it. not everyone is running ZFS bruh",
            "Not using ZFS",
            "Do you know any related CVE that doesn't need physical access affecting those kernels?",
            "You'd need to be close with developers and/or a contributer/developer of apps at this stage I'd imagine. \n\nYou could have a good look through their discord there might be something on there but I'm nearly sure it's very selective on the closed betas.",
            "No, or it would be an open beta.",
            "You can register and request access with the expectation you provide feedback and test the OS.",
            "Yeah even the community side is likely to drop away now with the new pricing model making it less attractive to just pick up the cheapest option\n\nRight now the main things holding me on unRAID are\n\n- My existing perpetual licence\n- It's still the easiest way to use Docker I've found\n- \"Throw a disk in\" storage\n- The community\n\nBut honestly I've realised \"throw a disk in\" storage is nice but isn't as important to me as I thought, especially as I've gotten older and tend to invest more in newer, bigger disks rather than using whatever I had lying around. Don't get me wrong, \"I have old disks from my PC lying around\" was a great way to get into the home server world - but I think that was somewhat unique to the late 2010s when we all had 1-2TB drives lying around from the late 200s and early 2010s, and won't really be repeated for new users. At this point, for me, I'm more limited by the number of drive bays I have than anything\n\nOther communities are going, and a license is a sunk cost not a reason to stick with a platform - So really it just comes down to Docker management for the most part. And lack of decent SSD support is starting to tempt me *away* from unRAID\n\nAt this point I could be tempted to move my home server to another platform with better SSD support and then use my unRAID license as a dedicated Docker host",
            "docker support is in plenty of systems, even plain debian its trivial, install portainer/cockpit and you're done.\n\nthe only real benefit of Unraid is the file storage (non striped parity) and the community.",
            "Why?",
            "Same!!",
            "Me too.  I\u2019m already switching one of my NAS\u2019 to TrueNAS and will leave the apps on UNRAID and have been slowly building them up on Peoxmox.  As soon as I have a workable alternative I\u2019ll move the second one off Unraid.  The support is terrible, especially for a paid product, it\u2019s too slow, and it\u2019s not stable, even for a consumer-grade NAS.",
            "If memory serves right, the next rendition of zfs is supposed to support expanding pools. Once that is finalized, zfs will be the goat imo",
            "lol the unraid zealots downvote. Unraid is a corporation. End of story.",
            "> , I mean that's pretty much a given for any software company \n\nNot really... Some software projects have public roadmaps listing all their planned releases, sometimes as long as 2 years into the future. Some projects have a set release cadence, for example Home Assistant releases a new major version on the first Wednesday of every month, and Microsoft release patches on the second Tuesday of every month (\"Patch Tuesday\"). Some just release \"when it's ready\". That's especially the case if there's a small dev team.",
            "Daily updates are a pain. I'm not going to read release notes every day. Way better to batch them. Once a month is more than sufficient.",
            "If you want ZFS just use TrueNas or any of a million other options. Are you just paying for unraid so you can use community apps?\n\nZFS is an enterprise solution which has nothing to offer home users except complexity. I've used it and know it well. I'm pretty sure you are also well versed in the tech and it matches your needs, e.g. snapshots, bitrot etc etc. None of that means its suitable for home users.",
            "For a simple file server it is a redundant feature.",
            "As this thread has discussed, they communicate things that haven't come to fruition.   I tend to agree that it is going to 7.0, but until it exists, we don't know for sure what they are going to do (if they do).",
            "Throw a disk in is still as important to me as it\u2019s always been so that\u2019s the main thing honestly. Easy docker support is a plus, but the deal breaker elsewhere is the storage management",
            "> But honestly I've realised \"throw a disk in\" storage is nice but isn't as important to me as I thought, especially as I've gotten older and tend to invest more in newer, bigger disks rather than using whatever I had lying around.\n\nStill if you want to add more space to a ZFS setup, you need to buy multiple disks of the same size and make a huge deal out of it. And I'm not sure how ZFS splits data among the drives - I kinda like the fact that I can have a bunch of different disks in UnRAID and have them share the same space.\n\nBut I can still use those two SUPER LOUD (but they were really cheap) drives just for /archive and they'll never start up unless I or some script actually accesses /archive.",
            "I Agree , While I Yet to go Beyond windows for my needs , the momment a better docker ect is here i can make a better decision . At this Point I would rather wait and see then even commit to a basic lic",
            "Portainer and Cockpit etc are good but they're more work to set up and IMO are still more complex than most casual home server users need\n\nunRAID (especially with Community Apps) strikes a VERY good balance of ease of setup/management vs flexibility etc\n\nI've been using Portainer a while and it's still a little fiddly compared to just finding the app in the Community Apps store and hitting install, *maybe* adjusting a setting or two, and hitting go\n\nChrist, I give unRAID a lot of points just for giving me a \"WebUI\" button to hit rather than have to figure out which port is the bloody webUI and which it's using for other purposes",
            "Thanks for saying exactly what I said?",
            "The only think stopping me from switching to Proxmox is the ability to just pop a hard drive in. I think I can do everything on Proxmox. Only think I\u2019m not sure about is Time Machine backups but Plex, all my VMs, all my dockers should easily go over.",
            "This has been talked about for ages, let\u2019s see when it\u2019s actually released.  But yes if they come out with an effective solution then it will change the calculus for many.",
            "It still doesn't rebalance the load across the drives and IS NOT advised or recommended for frequent use, it's an emergency use thing, or a once off. It is not the advised or recommend way to expand a zfs pool.",
            "Hopefully that would be enough to get me to switch to something like Proxmox probably (I know Proxmox isn\u2019t a NAS but I don\u2019t use unraid as a NAS. I use it for Plex and then dockers and VMs. Which Proxmox can do fine.)",
            "They\u2019re suggesting maybe even by Zoey and definitely next year.",
            "I have to say, that\u2019s a pretty nice feature, but you give up a lot to get it.  I\u2019m moving one of my Unraid servers to TrueNAS and there is a performance hit at 80% capacity and I\u2019m at 70% with Unraid and RAIDz2 will take up more storage in redundancy so I need to get it to 50%.  When your server is already at 110TB and you need to buy twice the storage to move over and have enough capacity, it stings, not to mention quadrupling the RAM for ARC.",
            "Yeah, running docker on VMs, in my case on Proxmox is significantly more time consuming for me.",
            "You can chuck in a new hdd in a Qnap/Synology too. Doesn't mean they have the same benefits as unRAID filesystem. It's not the same thing at all",
            "The VMs were so much easier in Proxmox, but the containers are a lot easier in Unraid, especially since your data is on the same server.  I\u2019m still working on it and it\u2019s been a couple months.  Proxmox isn\u2019t nearly as easy as VMware was/is.  I\u2019m planning to use Unraid as a backup server, for apps until I can get everything consistently and reliably working with Proxmox, use TrueNAS for primary storage, and have TrueNAS for Proxmox VMs as iSCSI target (desktop and server VMs as well as gaming PCs virtual and bare metal)",
            "Even with expanding pools, you still \"need\" all disks in a pool to be the same size, right?",
            "The feature is officially part of the TrueNAS nightly build so it might actually be part of 24.10 later in the year.",
            "As a Plex server, right now I only have 3 drives. 8 TB parity, and 2 * 4 TB data drives. I will need to add more later without a doubt. My case supports 13 drives. 8 TB drives are about $200 where I am. So if I switched to ZFS, I\u2019d need to buy at least 5 8TB drives for $1000 today to ensure I\u2019d never need to deal with expanding again. (2 pools of 6/7 drives to mimick my plan with unraid of dual parity). \n\n\nOr, I can stay on unraid, and just expand over time. My money will be worth more later. And hard drives will be cheaper later. So it\u2019ll cost me effectively less money. In fact, it\u2019ll cost so much less I bet the unraid license fee pays for itself. \n\n\nBut, if I could expand ZFS in the same way, then I\u2019d be able to switch",
            "Ok. I\u2019m not here to argue. Goodbye.",
            "If ZFS could work the way I\u2019d want, I\u2019d switch fully to Proxmox, and have my data on the Proxmox server too. I wouldn\u2019t use 2 machines",
            "I don't believe so based on the comments here.\n\nhttps://github.com/openzfs/zfs/pull/12225\n\nMost up to date link is\n\nhttps://github.com/openzfs/zfs/pull/15022",
            "Have you tried this feature on any of the nightlies yet?",
            "zpool expansion is going to be released next year.  Still, Unraid will be cheaper., especially for you.  I just hate how slow and unstable at scale Unraid is Where do you live that drives are so expensive?",
            "Not for me.  I guess if you don\u2019t have a lot of data you could come up with something, but Proxmox isn\u2019t designed for data management.",
            "Maybe I'm missing something, but in neither of those links do I see anything about pools of varying disk sizes.",
            "No, I'm still on 23.10. I always lag behind on purpose. But I do try to keep up with what's happening to at least know what's in the pipe.",
            "I mean\u2026 my server stores 1) app data. 2) Time Machine backups and 3) plex media. \n\n\nProxmos should be more than capable of that",
            "\"section and it might help to do a little example here:\n\nIf I have a 8x1TB RAIDZ2 and load 2 TB into it, it would be 33% full.\n\nIn comparison, if I start with 4x1TB RAIDZ2 which is full and expand it to 8x1TB RAIDZ2, it would be 50% full.\n\nIs that understanding correct? If so, it would mean that one should always start an expansion with as empty a vdev as possible. As this will not always be an option, is there any possibility (planned) to rewrite the old data into the new parity? Would moving the data off the vdev and then back again do the job? \"\n\nResponse:\n\n\" @cornim I think that math is right - that's one of the worst cases (you'd be better off starting with mirrors than a 4-wide RAIDZ2; and if you're doubling your number of drives you might be better off adding a new RAIDZ group). To take another example, if you have a 5-wide RAIDZ1, and add a disk, you'll still be using 1/5th (20%) of the space as parity, whereas newly written blocks will use 1/6th (17%) of the space as parity - a difference of 3% \"\n\nFrom Matt Ahrens the co-creator of ZFS.\n\nIn the comment of first link unless I'm misunderstanding what the poster means",
            "I will have to power on my TN scale box and update to nightly to test this out.",
            "Every example there is using the same drive size in the pool though? \"Load 2 TB into it\" means they are writing 2TB of data, not adding a new 2TB drive to a pool of 1TB drives.",
            "That's where I wasn't sure. As I know you can have 1 drive striped pools, so I wasn't sure if you could just add in additional single drives but now thinking about it, I guess it doesn't work off the current application of raidz"
        ]
    }
}