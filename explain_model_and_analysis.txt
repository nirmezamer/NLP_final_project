To further evaluate the usability of our dataset, we employed a BERT classifier, a transformer model developed and pre-trained by Google. This model is specifically designed for natural language processing tasks and provides a robust framework for text classification.
Our dataset consists of two columns: 'data' and 'label'. The 'data' column includes the textual data, while the 'label' column contains binary labels indicating the origin of the text (0 for human-generated and 1 for AI-generated). To adapt BERT for our classification task, we added a fully connected layer with a single output neuron. By applying the sigmoid activation function to this output, we obtained the probability of each text being human or AI-generated.
We utilized the Binary Cross Entropy Loss (BCELoss) as our loss function, which is suitable for binary classification tasks. The model was trained for 10 epochs with a batch size of 32. The accompanying graph illustrates the training and validation accuracy over the epochs.
From the graph, it is evident that the model effectively distinguishes between human and AI-generated texts. The training accuracy rapidly increases, approaching near-perfect performance within a few epochs. However, the validation accuracy exhibits a different trend, initially improving but then fluctuating and showing signs of decline as the epochs progress. This divergence between training and validation accuracy indicates overfitting, a common issue when the model learns to perform exceptionally well on the training data but fails to generalize to unseen data.
The overfitting observed here is likely due to the relatively small size of our dataset. With limited data, the model can easily memorize the training examples, leading to high training accuracy but poor generalization. This emphasizes the need for larger and more diverse datasets to train models that can generalize well to new, unseen texts.
In conclusion, while the BERT classifier demonstrates high performance on the training data, the validation results highlight the importance of addressing overfitting. This analysis underscores the dataset's utility in distinguishing between human and AI-generated texts but also points to the need for further data augmentation and regularization techniques to improve generalization.


In our analysis, we utilized several classic techniques to examine and compare AI-generated and human-generated texts across different datasets. We began by visualizing the distribution of the number of words, characters, unique words, and sentences using histograms. These visualizations reveal a clear distinction between human and AI-generated texts. In the Wikipedia and newspaper datasets, human-generated texts exhibit a broader distribution in terms of the number of words, characters, unique words, and sentences compared to AI-generated texts. This indicates that human writing in these contexts tends to vary more in length and complexity. Conversely, in the Reddit comments dataset, AI-generated texts show a wider distribution than human-generated texts. This can be attributed to the informal and typically shorter nature of Reddit comments, which contrasts with the more formal and extended format of Wikipedia and newspaper articles.

Furthermore, we analyzed the most frequent n-grams (combinations of words) within the datasets. This analysis highlights significant differences between human and AI-generated n-grams. Human-generated texts tend to have n-grams that are used more frequently than those in AI-generated texts. This suggests that human writing often relies on common phrases and constructs, while AI-generated texts may exhibit a more diverse range of expressions. The n-gram analysis thus provides insights into the distinctive patterns of language use between human and AI texts.

The word clouds generated from the datasets further illustrate these differences. In word clouds, the size of a word corresponds to its frequency in the text. Human-generated texts typically result in larger word clouds, reflecting a higher repetition of certain words or phrases. On the other hand, AI-generated texts produce more diverse word clouds, indicating a broader vocabulary and varied word usage. This diversity in AI-generated word clouds underscores the algorithmic attempts to create varied and less repetitive content.

Overall, our analysis reveals that human-generated texts are more varied and nuanced, especially in formal contexts like Wikipedia and newspapers. In contrast, AI-generated texts, while potentially more diverse in vocabulary, tend to be more consistent in length and structure, particularly in informal contexts like Reddit comments. These findings highlight the intrinsic differences in content generation between humans and AI, offering valuable insights for understanding and improving AI text generation technologies.
