{
    "Announcing the Open Release of Stable Diffusion 3 Medium": {
        "title": "Announcing the Open Release of Stable Diffusion 3 Medium",
        "score": 710,
        "url": "https://www.reddit.com/r/StableDiffusion/comments/1de2qne/announcing_the_open_release_of_stable_diffusion_3/",
        "content": "**Key Takeaways**\n\n* Stable Diffusion 3 Medium is Stability AI\u2019s most advanced text-to-image open model yet, comprising two billion parameters.\n* The smaller size of this model makes it perfect for running on consumer PCs and laptops as well as enterprise-tier GPUs. It is suitably sized to become the next standard in text-to-image models.\n* The weights are now available under an\u00a0open\u00a0[non-commercial license](https://huggingface.co/stabilityai/stable-diffusion-3-medium)\u00a0and a low-cost\u00a0[Creator License](https://stability.ai/membership). For large-scale commercial use, please\u00a0[contact us](https://stability.ai/enterprise)\u00a0for licensing details.\n* To try Stable Diffusion 3 models, try using the API on the\u00a0[Stability Platform](https://platform.stability.ai/), sign up for a free three-day trial on\u00a0[Stable Assistant](https://stability.ai/stable-assistant), and try\u00a0[Stable Artisan](https://stability.ai/stable-artisan)\u00a0via Discord.\n\nWe are excited to announce the launch of\u00a0**Stable Diffusion 3 Medium**, the latest and most advanced text-to-image AI model in our\u00a0[Stable Diffusion 3 series](https://stability.ai/news/stable-diffusion-3). Released today, Stable Diffusion 3 Medium represents a major milestone in the evolution of generative AI, continuing our commitment to democratising this powerful technology.\n\n**What Makes SD3 Medium Stand Out?**\n\nSD3 Medium is a 2 billion parameter\u00a0[SD3 model](https://stability.ai/news/stable-diffusion-3)\u00a0that offers some notable features:\n\n* **Photorealism:**\u00a0Overcomes common artifacts in hands and faces, delivering high-quality images without the need for complex workflows.\n* **Prompt Adherence:**\u00a0Comprehends complex prompts involving spatial relationships, compositional elements, actions, and styles.\n* **Typography:**\u00a0Achieves unprecedented results in generating text without artifacting and spelling errors with the assistance of our\u00a0[Diffusion Transformer architecture.](https://stability.ai/news/stable-diffusion-3-research-paper)\n* **Resource-efficient:**\u00a0Ideal for running on standard consumer GPUs without performance-degradation, thanks to its low VRAM footprint.\n* **Fine-Tuning:**\u00a0Capable of absorbing nuanced details from small datasets, making it perfect for customisation.\n\nhttps://preview.redd.it/ts9c9o60446d1.jpg?width=1000&format=pjpg&auto=webp&s=3ce4f3567d1a1099d989d0b22c281a8ea65c2944\n\n**Our collaboration with NVIDIA**\n\nWe collaborated with NVIDIA to enhance the performance of all Stable Diffusion models, including Stable Diffusion 3 Medium, by leveraging NVIDIA\u00ae RTX\u2122 GPUs and TensorRT\u2122. The TensorRT- optimised versions will provide best-in-class performance, yielding 50% increase in performance.\n\nStay tuned for a TensorRT-optimised version of Stable Diffusion 3 Medium.\n\n**Our collaboration with AMD**\n\nAMD has optimized inference for SD3 Medium for various AMD devices including AMD\u2019s latest APUs, consumer GPUs and MI-300X Enterprise GPUs.\n\n**Open and Accessible**\n\nOur commitment to open generative AI remains unwavering. Stable Diffusion 3 Medium is released under the\u00a0*Stability Non-Commercial Research Community License*. We encourage professional artists, designers, developers, and AI enthusiasts to use our new[\u00a0](https://stability.ai/membership)[*Creator License*](https://stability.ai/membership)\u00a0for commercial purposes. For large-scale commercial use, please[\u00a0contact us](https://stability.ai/enterprise)\u00a0for licensing details.\n\n**Try Stable Diffusion 3 via our API and Applications**\n\nAlongside the open release, Stable Diffusion 3 Medium is available on our\u00a0[API](https://platform.stability.ai/). Other versions of Stable Diffusion 3 such as the SD3 Large model and SD3 Ultra are also available to try on our friendly chatbot,\u00a0[Stable Assistant](https://stability.ai/stable-assistant)\u00a0and on Discord via\u00a0[Stable Artisan](https://stability.ai/stable-artisan). Get started with a three-day free trial.\n\n**How to Get Started**\n\n* [**Download the weights**](https://huggingface.co/stabilityai/stable-diffusion-3-medium)\u00a0of Stable Diffusion 3 Medium\n* **Commercial Inquiries:**\u00a0[Contact us](https://stability.ai/contact)\u00a0for licensing details.\u00a0\n* **FAQs**: Have a question about Stable Diffusion 3 Medium? Check out our\u00a0[detailed FAQs.](https://stability.ai/sd3-faq)\n\n**Safety**\u00a0\n\nWe believe in safe, responsible AI practices. This means we have taken and continue to take reasonable steps to prevent the misuse of Stable Diffusion 3 Medium by bad actors. Safety starts when we begin training our model and continues throughout testing, evaluation, and deployment. We have conducted extensive internal and external testing of this model and have developed and implemented numerous safeguards to prevent harms.\u00a0\u00a0\u00a0\n\nBy continually collaborating with researchers, experts, and our community, we expect to innovate further with integrity as we continue to improve the model. For more information about our approach to Safety please visit our\u00a0[Stable Safety](https://stability.ai/safety)\u00a0page.  \n**Licensing**\n\nWhile Stable Diffusion 3 Medium is open for personal and research use, we have introduced the new\u00a0[*Creator License*](https://stability.ai/membership)\u00a0to enable professional users to leverage Stable Diffusion 3 while supporting Stability in its mission to democratize AI and maintain its commitment to open AI.\n\nLarge-scale commercial users and enterprises are requested to\u00a0[contact us](https://stability.ai/enterprise). This ensures that businesses can leverage the full potential of our model while adhering to our usage guidelines.\n\n**Future Plans**\n\nWe plan to continuously improve Stable Diffusion 3 Medium based on user feedback, expand its features, and enhance its performance. Our goal is to set a new standard for creativity in AI-generated art and make Stable Diffusion 3 Medium a vital tool for professionals and hobbyists alike.\n\nWe are excited to see what you create with the new model and look forward to your feedback. Together, we can shape the future of generative AI.\n\nTo stay updated on our progress follow us on[\u00a0Twitter](https://twitter.com/stabilityai),[\u00a0Instagram](https://www.instagram.com/stability.ai/),[\u00a0LinkedIn](https://www.linkedin.com/company/stability-ai),\u00a0and join our[\u00a0Discord Community](https://discord.gg/stablediffusion).",
        "num_comments": 659,
        "comments": [
            "Damn, from SD2 they skipped all the way to SD404.",
            "https://preview.redd.it/iplbpw7tj86d1.png?width=1024&format=png&auto=webp&s=320a30a6b011b17a7af284c50883cbe40fb90240\n\ni love being able to use the most advanced model to make the best images anyone has ever seen /j",
            "> Photorealism: Overcomes common artifacts in hands and faces, delivering high-quality images without the need for complex workflows.  \nPrompt Adherence: Comprehends complex prompts involving spatial relationships, compositional elements, actions, and styles.  \nTypography: Achieves unprecedented results in generating text without artifacting and spelling errors with the assistance of our Diffusion Transformer architecture.\n\nI love how literally everything here turned out to be a lie.",
            "Confirmed: SD3 Cannot do hands. Worse than SDXL hands. Worse than SD 1.5 hands.",
            "What platform will the weights work on when they\u2019re available?",
            "https://preview.redd.it/2fosu6c9k56d1.png?width=593&format=png&auto=webp&s=6f84b59881d6a8de526fc26c7e76e560a0a7f29d\n\nLooks like stability learned nothing from SD 2. Its comically horrible at anatomy, straight up refuses to generate anything with a bit of skin normally. See you in a month when it got fixed by us.",
            "Well after testing a bit, for now I have SD2 vibes.\n\nI hope I'm wrong",
            "Cool, let me know when a model that isn\u2019t complete ass comes out though",
            "5 months of leading us on and hyping it up btw",
            "Is it just me or can SD3 not generate Anthropomorphic characters at all.",
            "I was here \n\n\nwhile it wasnt",
            "https://preview.redd.it/jsw6f9v8b46d1.png?width=3840&format=png&auto=webp&s=8ab4a793acafea5be85f7282f59738610b5baf1c",
            "They really want full openai on us. Wow",
            "And how can pictures harm me?",
            "So let me be the first to officially kick things off....SD4 WHEN? \ud83d\ude00",
            "Doesn't seem that much better tbh. Noticeably better at doing text and maybe marginally better with composition, but it still can't do hands. Anatomy in general seems to be below a lot of the models that exist for XL.",
            "First reviews on SD3 = hands are mayor crap. Can you provide examples on how to get proper hands? In theory the announced said that body and hands are now much improved. But then look HORRIBLE. Way worse than all previous models.",
            "I hope you guys stand with your promise on releasing the 8b version",
            "The license is NOT low cost. Midjourney and OpenAI are charging the same price but providing compute, whereas Stability expects their licensees to provide their own compute.  You're torching the good will of the community and making the model unattractive to fine-tuners and extension builders.\n\nI get that Stability needs a path towards profitability but this is not the way to do it. It will be virtually impossible for Stability to collect rent on the average user selling their gens commercially and they are not adding any value for people who decide to pay. It makes no sense.",
            "Tested, heavily censored, extra or missing limbs everywhere. Generally crappy. I am sticking to 1.5 models.\n\nEdit: 1.5 models have matured a lot. The anatomy problems in that model are known and have known fixes/workarounds. It generates much faster too. So except if SD3 would have been much much better, I am not changing my workflow. I also think it could be difficult to make fixes to SD3 because the code itself contains censorship.",
            "Stability AI is fucking kidding us? all this time to release a goddamn censored model that creates cronnenberg aberrations instead human beings because the censorship is so hard that can't depict a human body",
            "Fuck Safety\n\n\nFuck Censorship\n\n\nNo new locally used models or techniques will ever be used in the future, unless the users get the ability to take the censorship and meddling down, thankfully it seems it will be possible for SD3, so it will probably take off.",
            "Weights are live!! https://huggingface.co/stabilityai/stable-diffusion-3-medium",
            "They lobotomized it before the release. Why would they destroy their reputation rather than trying to promote themselves and their success with a decent product,\n\n\nOh well potential squandered. Sd3 currently doesn't stand a chance against the competition in it's current state",
            "Looks like shit. Why did you release this? Who is it for?",
            "I will wait for AUTOMATIC 1111, I can't wait for it! :D",
            "Sd3 is so bad. 0/10.",
            "Very interested in the safety feature aka censorship. Pretty sure any prompts including \"Taylor Swift\" will be completely blacked out.",
            "Is government forcing them to believe in safe and responsible ai research? Why all ai researcher repeat  same crap always.",
            "What are recommended settings (sampler,sheduler, cfg..) ?",
            "sighs... Yea, just played a bit with it in comfy UI and doesn't feel any better than 1.5 tbh... We'll have to wait for the community to make it better. Midjourney, which I use everyday, is still king for now...",
            "Extremely bad... Another model that will die before it is born.....",
            "It's live now.\n\n[https://huggingface.co/stabilityai/stable-diffusion-3-medium](https://huggingface.co/stabilityai/stable-diffusion-3-medium)",
            "It seems to be shit at generating humans in specific poses, like just laying down. Maybe lack of nudes in training materials is making anatomy bad?",
            "is it me or is it terrible it cant do anything good",
            "Does the lack of nudity training also make the anatomy worse?",
            "Lot of work for making something of no value. I bet Batwoman was a better movie than this release is a generative art model. And we'll never see that.",
            "You know, I do not understand why SAI doesn't make a non-crippled NSFW model and just *sell* it to people, making them fill out legal disclaimers that they won't post pics of Taylor Swift doing the nasty on the internet, or whatever your \"safety\" requirements demand.  Offer some sort of trial that could be evaluated locally (since I'm never ever using your cloud based stuff) and then just let people give you money for it.  Maybe this won't be a popular suggestion since everyone wants free stuff, but I'd be willing to pay for an actually good model to be used for personal use.  As it is, SD3 is looking like it's going to suffer the same fate as SD2.  \nedit: to clarify, a *one time* license fee: I'm not doing any subscription crap.",
            "Its up guys",
            "What are the recommended system requirements? Similar to SD 1.5 and SDXL?",
            "I cannot wait to see the first benchmarks on local GPUs and comparisons between models.\n\nI'm a bit afraid that it's again a heavily censored model that skipped training any kind of human anatomy, making it completely useless compared to Dall.E or other models.\n\nI'm also not a fan of the licensing part. A huge pro was the use of SD for small content creators. Removing the rights to use their images makes the whole thing less attractive (especially when there are already great models to pay for like firefly or Dall.E).\nAlso, I wonder what the licensing implications are when it comes to user created checkpoints of these models.",
            "I just get black outputs with the basic workflow on comfy",
            "so sad",
            "HYPE HYPE HYPE HYPE HYPE\n\nTHANK YOU SO MUCH STABILITYAI <3",
            "Model options:  \n  \nWe have prepared three packaging variants of the SD3 Medium model, each equipped with the same set of MMDiT & VAE weights, for user convenience.\n\n* **sd3\\_medium.safetensors** includes the MMDiT and VAE weights but does not include any text encoders.\n* **sd3\\_medium\\_incl\\_clips\\_t5xxlfp8.safetensors** contains all necessary weights, including fp8 version of the T5XXL text encoder, offering a balance between quality and resource requirements.\n* **sd3\\_medium\\_incl\\_clips.safetensors** includes all necessary weights except for the T5XXL text encoder. It requires minimal resources, but the model's performance will differ without the T5XXL text encoder.",
            "~~404 on Hugging Face link for now:~~ Aaand weights are here now:\nhttps://huggingface.co/stabilityai/stable-diffusion-3-medium\n\n~~Same for the FAQ link to:~~ FAQs are working now: \n\nhttps://stability.ai/sd3-faq\n\n\nSeems like a TOO-early access... : D",
            "Do you work for them, orrrr?",
            "Lmao even their example images didn\u2019t conform to their own prompts. And like why do the 3 dogs all have the same fur pattern?",
            "The commercial license on fine-tunes on a model that can't pose humans sort of kills it.",
            "https://preview.redd.it/ihtn4jms4d6d1.png?width=866&format=png&auto=webp&s=840b305b691d164e1e1a3a1cba6e58c11c01bad3",
            "Kohya support when????",
            "404 on links",
            "hell yeah.",
            "Last year on a LLM kick it was so exciting, something new every week, a 7b model from May felt antiquated by July. You never knew what was coming around the corner.\n\nThis year it's news, wait, no news, wait, half ass api release with blurry censorship, wait, announcement, wait, date set, wait, release day, turns out to be a massive puddle of dog water, wait.\n\nI'm okay with that, I don't need SD3, I'm just kind of embarrassed at their effort. bartsimpsonyoutried.jpg",
            "They claim it's good for anything that doesn't contain a human, but I can't generate a single image of a room from the victorian era. I see nonsensical things everywhere. How much pollution did they generate to create this piece of shit of a model?",
            "A1111 Work with It?",
            "THANK YOU!!!!",
            "Cool ! Now i am waiting for SD3 Inpainting model ...\n\nbtw - **how much VRAM require the SD3 ?**",
            "AWESOME! New generation of local image generation kicks off, hopefully not the last! \n\nPlease keep us updated on the fate of Stability in general. We (most of us) really do care and want to see you guys succeed.",
            "Anyone knows what are the very minimal specs to run it?\nAnd with all the bazinga like controlnet, warpers etc?",
            "I don't believe you",
            "My favorite portrait so far is one where it put 3 mouths on a face, two where the eyes should be and one in the normal spot. \n\nThis model - you guys are not seeing the big picture. This is going to be great for SCP art.",
            "I can't make a human female with any fidelity.\n\nBut I can spread misinformation!\n\nhttps://preview.redd.it/aig15nznff6d1.png?width=1024&format=png&auto=webp&s=976c6d3afc2f6e646a710da1cbf076548629ba5e",
            "No thanks. The \"Creator License\" is crap and I'm no longer interested.",
            "\"You need to agree to share your contact information to access this model\"\n\nHow about no. I can deal with SDXL's limitations, thanks.\n\nEdit: I'm getting downvoted for not wanting to share my personal info with a company just to use their public AI model on my local system? OK.",
            "Thank you so much for the release !!!!",
            "Cant wait to try it out \ud83d\ude01",
            "\n\nWe believe safety starts at the time we are training our models. Stable Diffusion 3 Medium was trained on filtered data sets in order to help ensure we are starting with safe data, which makes it harder for the model to generate harmful content downstream.  We have also added embedded safeguards that help prevent harmful images from being generated.\n\n\"We have also added embedded safeguards that help prevent harmful images from being generated.\"\n\ngg wp unless lie",
            "what am I downloading? And ComfyUI workflow? \n\nhttps://preview.redd.it/01pe5bbm956d1.png?width=786&format=png&auto=webp&s=a97fa475c6d9046506e388b141f0c77077d56e31",
            " Been using Forge and love it but if it's been discontinued as I keep hearing will it be able to use SD3?",
            "Well, time to ignore it completely until auto version comes out... :)",
            "It's Available Now \ud83d\udd25\ud83d\udd25\ud83d\udd25",
            "Censored trash",
            "I had a feeling that small-scale local commercial use will require paid license this time, like SDXL Turbo or Cascade.\n\nUntil there's great NSFW anime model I'd stay with SDXL :P\n\nEDIT: I'm not complaining, why are you jumping to conclusions? It will take a while for checkpoints based on SD3.",
            "This is the moment we've all been waiting for!",
            "This is a sign that my job will be easier to do, so let's all wait for this historic moment!",
            "Lets Gone! Ahh I mean Lets Go!",
            "https://preview.redd.it/j9avk4xeo46d1.png?width=3840&format=png&auto=webp&s=d6d0a9e73d2329ce92fff7b87142c2d4e7442a13",
            "umm its available now",
            "Doesn't seem to work out of the box with Krita Diffusion. Have to wait until the plugin is updated.",
            "It's there... 404 was removed.",
            "which file do i want??",
            "https://preview.redd.it/de2ty0mye56d1.jpeg?width=1024&format=pjpg&auto=webp&s=8f4d5b1464947763f83e1670910ae46f1ecf01a1\n\nLooks great!",
            "You said you'd update the first line when it was out :(",
            "Now for the unstable diffusion~",
            "About the non-commercial license, is my understanding right?\n\nI can train a Lora on top of SD3 and possibly benefit from it alone but I can't use the outputs of the Lora + SD3 for any commercial use.\n\nI can't use a finetuned model for any commercial use. If I finetune an SD3 model and want to post it somewhere, I have to ask stability AI and include their non-commercial license in a notice file.",
            "Where do the text encoders go in the ComfyUI models directory? I placed them in the clip folder but they're not showing up",
            "I can already break through the heavy censoring.",
            "Presuming smart lawyers are involved...  \nWhat does \"non-consensual nudity\" mean in the README?  \nSo does this mean that high quality non-real person nudity can be produced as long as it isn't sexually explicit?  Otherwise that clause would not have been necessary.\n\nSo what is: A women standing nude on a mountain top at sunset  \ngoing to produce?",
            "Do not release the semi-finished products.",
            "So much for innovation",
            "most advanced ... pfff (no)",
            "i tried their sample here and it doesn't even give me nice result",
            "Trash",
            "Let's go. Thank you for all your hard work.",
            "i want sd3 nowwwwwwwwwwwww",
            "[deleted]",
            "LFG!! Can't wait to play around with this later.",
            "Thanks Stability! <3",
            "Thank you \ud83d\ude0d\ud83d\ude0d\ud83d\ude0d",
            "How do we actually use it? i hear Comfy has support but where do I get the workflow? **thanks!**",
            "Will the large model also be released in the future?",
            "[deleted]",
            "it's up ;) https://huggingface.co/stabilityai/stable-diffusion-3-medium",
            "It's here! \n\n[https://huggingface.co/stabilityai/stable-diffusion-3-medium](https://huggingface.co/stabilityai/stable-diffusion-3-medium)",
            "https://preview.redd.it/ho5jtxb2856d1.png?width=826&format=png&auto=webp&s=62a03570be890bac719f59d6e07f3b5fbc601425\n\nWell, I've waited all this time. What's another three days?",
            "fyi this is just a fake announcement by some guy unaffiliated with stability, as far as i can tell (edit: irrelevant update, but it's the real announcement posted by an unrelated guy who scooped stability by posting their blog post before they had actually released the model)",
            "Can't wait to remote my phone into my PC running this for bespoke memes on the go",
            "Creator's license at the same cost as ChatGPT hardly seems like a \"low cost\" for something that used to be free to use. It's one thing if you are a \"creator\" earning a few thousand a month sure- but if you are just getting started and earn $5-10 a month as a hobbyist; that seems crazy.\n\nThe last model with the paid license option was basically like \"you don't have to pay UNTIL you are over 1 mil\" - this one is \"pay this until you go over 1 mil\". They really should put some lower bands on that price, especially if they are claiming that this is a \"low cost\" option.",
            "How can you have a commercial license on a derivative of something Runway invented, and a data set that isn't wholly yours either?\n\nSerious question. Any lawyers here?\n\nOpen source works this way?",
            "If someone here is kinda bored and want to explain something to me then id appreciate it :D\n\nWhat is SD3 compared to SD 1.5? I feel like with sd 1.5 i can create already very good and realistic images in a bunch of different situations, objects, people etc, ESPECIALLY with Lora's etc.\n\nIt says that SD3 is 2 billion parameters, and ive read that 1.5 is 983M, but even though its more than twice the parameters, it cant be twice as good right? Or am i just really not comprehending the massive difference between 983m vs 2b? It seems like everyone is talking about SD3 like SD 1.5 is a toyota corrolla while SD3 is a lamborghini. Like SD 1.5 is mcdonalds while SD3 is a michelin star restaurant, especially when Lora's and customised models of sd3 comes out.\n\nHow big of a step from SD 1.5 is this new model in regards to AI image generation?",
            "Great stuff many thanks!  Or not as the case may be..",
            "So if I subscribe to the creator license I can use the models in all the tools like comfy/etc? And I keep rights to my images forever?",
            "Why isn't there a set release time?",
            "It's time!\n\nThank you!",
            "I have been waiting for a long time. I am looking forward to it.",
            "Looking forward very much, want to download.",
            "Bravo! Marhabah! From 'stanboul with love!",
            "Will the safe tensor include the clip? Or we need to download the clip separately.\n\nDoes SD3 use its own clip or T5... Or both??? Damn I can't make sense of all this.",
            "2 hours. Release is 10:00 est.",
            "But we\u2019re going to have to wait for controlnets, and ipadapters to be released, right?",
            "Were controlnet models going to be included with this release or am I misremembering?",
            "[deleted]",
            "Now to just wait and see if whether the next version of pony will be trained on this or sdxl",
            "RELEASE already! jeeze the wait is killing us.",
            "Still 404. Did they switch to dialup to cut costs?",
            "https://preview.redd.it/fvsqvxh7356d1.png?width=3091&format=png&auto=webp&s=166d6052d011c407da285fee67931a237666e2f6\n\nVERY SOON!",
            "https://preview.redd.it/ulmqwsgk356d1.png?width=454&format=png&auto=webp&s=bacf9796f1ec455cf40d168fbe135206dd78c54a\n\n:D",
            "10 gb file",
            "Yo, people sorting by new, which file should we download?",
            "https://preview.redd.it/0phjswju456d1.png?width=1283&format=png&auto=webp&s=029d2ba8a11ef1e6e9d72ca5714d6ac740778024\n\nIt\u00b4s here!!",
            "So...  Is the 10 Gig version the one we want for Comfy?",
            "Can I use any loras, embeddings, models or whatnot with this?\n\n\nOr is it safe to throw all that away, I dont plan to use older models anymore",
            "Congrats on the launch!",
            "Using the multiprompt example workflow first to test it out. Super impressed by the pixel art results just from adding \"pixel art anime\" to the front of the 3rd positive prompt",
            "\"SD3 Medium is a 2 billion\"  \n\n\nSo it's more than half the size of SDXL?",
            "First impressions?",
            "Well. waiting for Kohya\\_SS now.",
            "Hmm. I can't get it to work on my WebUI set up.",
            "Does it work on forge?",
            "Soon we will get to the point that it will depend on what people do with models instead of how much stuff they have hoovered up.",
            "https://preview.redd.it/kte55t5xl56d1.jpeg?width=1406&format=pjpg&auto=webp&s=a2478eebca54b248bc6755a2bf764e9d2cf65c46\n\nwhere do we get these from?",
            "Still trying to create my pen&paper character. Results got way better, but umbrellas and snakes seem to be a problem. Big improvement on the umbrella part though:\n\n\"a rpg fantasy illustration of a beautiful friendly witch with curly red hair in medieval black pirate clothing and a skirt. she is holding an umbrella in her hand and leaning it over her shoulder. the umbrella has asian patterns. she is standing at a medieval dock with ships in the background. next to her sits her pet cobra snake\"\n\nhttps://preview.redd.it/r968gwv6s56d1.png?width=1024&format=png&auto=webp&s=17ebe2fe277317501abb0a0c3018010f7043dc41\n\nany recommendations to improve the prompt? and this is cherrypicked, best out of 32 images, most umbrellas and/or snakes are broken",
            "Can anyone share the sha or md5 checksums of the various files from huggingface?",
            "Any links that don't ask for my ID? I honestly don't know why I care, I'm sure the NSA is watching me type this through my phone, but I do.",
            "Will I need to use lowvram, medvram if I have only 6gb of vram? Where will be clip files stored, into system ram or vram as well?",
            "Does the license allow for fine tuned models? What license does the fine tuned model need to have?",
            "Where can i find the SD3 nodes for ComfyUI?\n\n  \nEdit: Nevermind. The nodes is in the recent ComfyUI update.",
            "Thank you",
            "AMD support!!",
            "As someone who avoided SDXL like the plague until recently due to the onerous time and resources needed for ~100K images compared to SD1.5, the timing of this release couldn't have been any better as I was just about to spend a ridiculous amount of credits to retrain one of my own custom models on SDXL and now have it available for once SD3 training has been figured out.",
            "Why did they added a Safety thing? Are they stupid? Jailbreak is inevitable.",
            "Why does the complex comfy thing seem to be dominating more and more every day locking us into that infrastructure?  Up until this I'd usually see a simply python example loading a model and gen'ing an image in the README file for a model.  Just using low level diffusers stuff.  Thus allowing running experiments.  What's going on?",
            "Will we have a GitHub Repo for research use?",
            "macOS?",
            "Is there anywhere that you're able to try this model online?",
            "Can I run it on gtx 1650 4gb? I Could run sd 1.5 and 2.0. (xl was to much for this gpu) And which file should i download, 4,3gb one?",
            "its available!",
            "https://preview.redd.it/vmn5qjor186d1.png?width=1024&format=png&auto=webp&s=dee96e921c0c98edccd07789e66fc206cb918436\n\nIt works! lol what language is that plate?",
            "So need train again ?",
            "I've never tried stable diffusion... I was waiting for this release before diving in. I take it I should just setup SDXL? \n\nI plan to generate images on my 2070 super PC. I've used midjourney and dalle through chatGPT, but never tried stable diffusion. I'm pretty good with computers overall... so I should be able to figure this all out.\n\nWhere do I start? Anyone want to point me in the right direction?",
            "If I were building a PC  on a budget, with the goal of running this locally, what would I be looking at? And how long would it take to generate images?\n\nI'm really interested in running things locally instead of through a 3rd party. I'm willing to pay the license fee. Any help is appreciated.",
            "has anyone been able to get it working on llamacpp or koboldcpp?",
            "Hopefully, other open-sourced companies will learn something useful from the Stability AI history. And with this release, after all that hype, looks like its story is basically finished.",
            "If enough people ask\n\n(at) stabilityAI  \nIs there any plans to make Stable Audio Open and Stable Diffusion 3 Open Source? (fully free use without use restrictions including commercial use  \nopensource DOT org/osd  \n 1. Free Redistribution\n\nPerhaps we can move things in a positive direction by sheer publicity.  \nEither not called open source or license changes.  \nIf thousands of people asked, then perhaps something.",
            "I'd rather have the ability to zoom in when trying to fix things with inpainting in A1111. Sounds like a much easier feature to implement.",
            "after reading, steps out of a chat, exits room, the hall, and the building. Exists the street down the street, and into the next town. Watches from a distance, smoke rising and rises himself above the earth, seeing the smoke rising high into the sky and turns into a space butterfly and explodes.",
            "Do you work for them, or....?",
            "I'm trying to use the fp8 model on forge, and I get \"stable diffusion model failed to load\". Is it normal?",
            "It's a hard thing for me to write to not come out as a pedo. So this is like 3rd attempt. I understand concerns of StableAI for their model not being able to generate any porn that is resembling any human. I do not know how this would reduce rape amount on children, actually I think it does nothing. While it could reduce it if 1 potential rapist stayed at home and generated images instead of going out. But I digress.\n\nWhat making unable to do any NSFW imagery did - made people angy. Just that. Nothing interesting that goomers get angy. Normal for them anyways. It did nothing to anatomy coherence. IT WOULD DO NOTHING to anatomy if it was just nsfw stuff. But the shit they use to censor it was brutal. Too brutal. SD3 doesn't understand what a breast is. SD3 doesn't understand what recline, lounge, sit, lie, walk, jog, etc. is; also it completely doesn't understand any androgynous term.\n\nJust so I am not unfounded. Simple jog. Of a man (prompt: man jogs, no negs). I do not even want him barechested. Want to create a simple jogging man for an article (SD3 was advertised to be on par with DALL-E 3): [https://imgur.com/a/ZAm0aYu](https://imgur.com/a/ZAm0aYu) Okay, none of them is anatomically correct. Do we need to post SD1.5 shit in it? We were told it won't be a thing in this iteration. SDXL doesn't need it a lot of the time. At least I can generate 1 in 5 images that are okay-ish in it. Not in SD3. I can run it through night and it will generate shit all the time. Why? Why though?\n\nIt took like 3 months (or two?) to put SD3 to the public. They anounced they are working with some weird anti-pedo guys. Not like with actual agency. Just some dudes. Organisation created by Demi Moore (nothing against her) and Ashton Kutcher (let's not delve into his awareness of 15yo being ... groomed in that show or his proclivities with Danny). Their organisation's AI is KNOWN for misinterpreting stuff on the net. It uses AI for a long time now. A lot of incorrect readings.\n\nThis wouldn't be a problem if their thingie worked. Reduce amount of children in database? Easy fix with a nicely done LORA - nicely done 30K pics of safe images of kids and their parents? EZ in comparison to what we have. Nobody complains. Some people complain, but a fixable problem. But what we have now is having so many pics deleted from the database incorrectly tagged as nsfw to have NO human anatomy coherence when they are clothed. We cannot generate any pic that has no human in the center of the image that has an actual coherent anatomy.\n\nWhen SD3 first came on API - I played with it, pushing it as much as my accumulated credits allowed me to. I tried simple things like 'young girl on edge of a pool, water droplets, looking back at viewer' - it tended to generate nude one without me prompting for it. And it was blured. Understandable. But I remember those pics had nice colors - very cinematic. I cannot make in SD Medium that kind of feel anymore. That nice coloring. Thing they used to reduce NSFW actually affected adults too because THORN IS CRAPSHOOT, I think. It's a personal opinion from what I've seen and read. This is not a legally liable thing because I do not state it as a fact. Only things I state as facts are things that are widely known.\n\nPeace!",
            "When I try to load this SD3 Checkpoint it goes back to the previous checkpoint selected, does not let me select SD3 Checkpoint? am I doing something wrong?",
            "The NSFW bot deleted my image generated by the new Safe SD 3.0 model, only took me 5 prompts and maybe 25 images generated.   Now to free up 17GB of disk space.",
            "SD 0.01 beta release",
            "https://preview.redd.it/1bhpyc0d7d7d1.jpeg?width=1024&format=pjpg&auto=webp&s=5f0d13ac0221ad5d0713d3f43fe7c944d811c128\n\nI love how much freedom it gives me!",
            "I think civitai today restricted sd3 for some reason over some licensing BS",
            "Why are you heading towards a licence to humiliate your creators with slavery? Do you feel better if you eat more than 10 bowls of rice a day?",
            "\"open\"",
            "How did this trash get over 700 upvotes? Who upvotes this failed experiment and a \"f\\*\\*\\* you\\* to the community from SAI?",
            "Performance is good ... it can generate text ... what else ... hmm ... a yea - you have to pay 20$/month and delete everything you generated when you stop.\n\nNothing to see here ...",
            "The new corp-controlled and parenting soon-to-fail version of SD 2.1... for those of you not paying attention yet.",
            "*Scouter blows up*",
            "That's one way to make sure you have the first post for some news: just lie and say the thing already happened before it did.",
            "Did you hear?  sd4 will be out soon, any day now, not too long, next month, next week, tomorrow, next week, next month, some further fixing up(aka censoring), ...",
            "This joke became even funnier in retrospect",
            "Consequences of making AI \"safe\".",
            "https://i.redd.it/skttavwmfa6d1.gif",
            "but at least the text works D:",
            "Literally every single thing that they claimed was false",
            "I\u2019m genuinely baffled, feels like we\u2019re being trolled",
            "So which model do hands better? SDXL or SD 1.5?",
            "Comfy",
            "A1111?",
            "StableSwarmUI should work (never know 100% until you try it)",
            "Hopefully InvokeAI will add support pretty quickly.",
            "I'm a huge noob, but arent .safetensors models the same for all platforms? Im really confused",
            "What reason does anyone have to fix anything when the license is so suffocating? Why would anyone do that?",
            "It's going to be more than a month if ever.",
            "This is my fetish.",
            "looks good to me",
            "its pretty horrible for everything with a woman so far, feels like early 1.5 again\n\nhttps://preview.redd.it/gqwexsxida6d1.png?width=1024&format=png&auto=webp&s=c349b8ed0ad0fa167e98c2a23f7049eae9cef7c8",
            "Agreed.   I could at least get some decent anatomy (yes, even nudes, which isn't the same thing as porn despite what seems to have gone into this model) out of the original SDXL releases.  I still don't use them because by the time it finally came out the SD 1.5 finetunes could generate at 1024x1024 and followed the prompt better.  Had people trained corresponding refiners for their finetunes so detailing was significantly better it might be different but as of now 1.5 is still far more entertaining coming from someone who uses actual oil paints when they want to make cool insane looking art and screws around with prompts in stable diffusion when they want semi-predictable insanity or weird hidden QR codes for their wifi network or whatever. :P",
            "Not just you",
            "Can't do transformations either. Makes a human magazine photo cover of clothing models instead. SDXL 0.9 had more to it.",
            "I WANT IT NOW. GIMME GIMME GIMME.",
            "nice",
            "You might see a nipple!",
            "It could generate a photo of SCP-096.",
            "What if you see a skinny white woman with blue eyes and white hair ? i mean it can cripple you for life...",
            "Somebody made naked pictures of Taylor Swift.  Do  you know how unhappy she was about this?  She lay on her huge pile of money and cried.  She was so sad that only her $1.3 billion could console her.  I don't want to live in a world where billionaires can be mocked in such a fashion.",
            "Well if you don\u2019t pay them $20 a month for the Creator License, and you want to make thumbnails for your YouTube that brings in a couple bucks, they will sic the government after you to take your money or throw you in jail.\n\nWonderful.",
            "Same way words can, I suppose.",
            "After ww3 then comes sd4",
            "I'd be more interested in SD3 Large.\n\nIf there is a SD3 Medium there will be a Large, right?",
            "Probably another year.",
            "When Valve releases a new TF2 update that is not localization files.",
            "S o o n",
            "Hi, is SDXL better than SD 1.5 in anatomy and hand?\u00a0",
            "Yeah, sadly the writing was on the wall with all their announcements about \"making AI (((safe)))\"",
            "I have a 4060Ti (8GB) and want to modify it based on my own set of images as training data.  \nCan you please tell me which one of the 3 should I go for?",
            "100%. Now, even if someday community will somehow make it useful for something, huge damage to their reputation is done, and well, looks like they are done as the company as well, with such damage at their current financial status.",
            "probably gonna be a while...",
            "How long did it take for Auto1111 to support SDXL? I'm assuming the wait will be similar this time.",
            "it's lora training! time!",
            "What if I want a picture of \u201ca swift tailor, sewing a dress on stage during music performance, fast worker, measuring tape entangled around her, singing to herself, blonde hair, blue eyes\u201d\n\nOnly one way to find out.",
            "Its local weights. Nothing will be \"blacked out\". Worse case stuff like celebrities or nudity wont be in the dataset, and usual user refined checkpoints will fix that easily.\n\nFor that matter, this generic description doesnt read like censorship to begin with. It reads as \"we want to cover our asses from morons starting internet drama and/or suing us, so we'll claim that our AI is 'safe' \".",
            ">blacked\n\nA most unfortunate choice of words",
            "Gotta wait some fine tunes to help with that lol",
            "Apparently AI is going to cause humanity to go extinct. How, you might ask? Probably the same way the Y2K bug caused us to go back into the stone age.\n\nYou should always believe everything \"experts\" tell you about the risks of technology, because they totally know what they're talking about and don't want to sell you worthless products to \"protect\" yourself from Skynet.",
            "AI companies don't want to be sued or get bad press if their models are used for things like child porn",
            "> Why all ai researcher repeat same crap always\n\ncult",
            "I think it's already been said ancestral sampling doesn't work with SD3. That's going to create some amount of confusion unless comfyui has added a warning/check for trying to use SD3 with EulerA and stuff.",
            "Exactly my thoughts. Why do they even release this? I mean in theory they stripped down the full sd3 model to half its capabilities (medium) so basically mathematically it's 1.5. I'm joking of course but I feel like Stability AI messes with the community paywalling the full sd3 model while giving us a model that isn't better than 1.5 or 2. I was excited for this release man. I think we reached a point in image generation where the progress is slowing down drastically if this is what we get after almost 2 years after 1.5.",
            "Anything that doesn't have to do with people or animals.",
            "https://i.redd.it/eia2u8bgfa6d1.gif\n\nThats the reason human artists draw form nude people. But seriously I Finetuned tions of DB models with XL from people. ANd when you train on naked person - anatomy is just amazing.",
            "I'm sure a lot of people would pay for good uncensored model for personal use. I honestly don't understand, why nobody wants to take their money.",
            "If you don't understand why they're running from nsfw to point that they are destroying their business..\n\nThen you don't understand ESG, Visa/Mastercard, Paypal events have been going on.",
            "Money processors like MasterCard, PayPal anything NSFW related",
            "You can argue about how big the threat vs the benefit is, or who else will do it first, etc. But \"promise\" based anti crime based planning of any sort is absolutely bonkers\n\n\"Just give everyone tanks with 120mm cannons on them, as long as they PINKIE PROMISE not to use them for crime, we're good\"",
            "https://preview.redd.it/7q80rgp0a56d1.png?width=1024&format=png&auto=webp&s=770de006ab7193336c05f09d4be5c728501853c7",
            "https://preview.redd.it/iiyfihhzb56d1.png?width=1024&format=png&auto=webp&s=a11b30e8d66eee9a9a7c9ffb4865612fbd7a25df",
            "What's the GB size?",
            "It's fun to see the difference in tone of comments before and after",
            "Between SD 1.5 and SDXL. So 8gb should be a good.",
            "Theres 3 versions, one without CLIP at 4.34GB, two with CLIP at 5.97GB and 10.9GB respectively.\n\nSo the size of your VRAM will be a factor, but anything over 8GB can probably run the first two.",
            "It's a pretty large model despite the name \"medium\". VRAM usage roughly matches SDXL, so it won't work on low-VRAM cards.",
            "I did some first checks for different variants here: [https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource\\_consumption\\_and\\_performance\\_observations/](https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource_consumption_and_performance_observations/)",
            "Same here. I mean I know they prioritize safety, but this is a bit much :-P\n\nedit: sd3_medium_incl_clips_t5xxlfp8.safetensors gives me an image!",
            "I just used the default workflow and it worked straight up",
            "Just to add a bit of info, you have the workflows for comfyUI here\n\n[https://comfyanonymous.github.io/ComfyUI\\_examples/sd3/](https://comfyanonymous.github.io/ComfyUI_examples/sd3/)\n\nDon't forget to update ComfyUI before trying them.",
            "Wat? What does `sd3_medium_incl_clips` include that isn't included in `sd3_medium`?",
            "They are still uploading",
            "Already up\n\nhttps://preview.redd.it/5gm064nx556d1.jpeg?width=1170&format=pjpg&auto=webp&s=7cedd5b0d4db2d90e54d42229ce3ebebb8695b82",
            "Just someone karma farming with no new info that we didn't already have.",
            "i hope now",
            "yes",
            "Apparently there is a count down timer it stops in 2.5 hrs",
            "If the timeline plays similarly to the launch of SDXL, there will probably be a few weeks delay.\n\nEdit:\n\nLooks like it went:\nSDXL 0.9 early release by Stability to researchers, June 22, 2023\n\n0.9 leaked to general public on June 25. ComfyUI support.\n\nVlad fork of A1111 gets 0.9 working in SD.Next, July 8\n\nFunctional SDXL 0.9 support in A1111 dev branch, July 12th\n\nDev SDXL branch merged to main in A1111, July 16th\n\nOfficial SDXL 1.0 release by Stability, July 26",
            "Definitely yes, but ComfyUI will be first.",
            "4",
            "8 GB will be fine when using the fp8 version of the textencoder. Otherwise 10 GB is needed. See details here: [https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource\\_consumption\\_and\\_performance\\_observations/](https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource_consumption_and_performance_observations/)",
            "Just put in some bullshit ... though if you wait a bit its probably gonna get shared \"elsewhere\" soon enough",
            "Yes, and \"Organization or Affiliation\" is required, does it work, when i write none here?",
            "ya, if this is true it's gonna totally screw their model.",
            "Depends on the size of your graphics card vram?",
            "No",
            "It\u2019s not discontinued. They are jut \u201cexperimenting\u201d . For now though it is just as good as dead though. What a shame. I hope sd3 is friendly with a 6gb vram card",
            "I want to know this too.",
            "Yep :D",
            "Read the license. Commercial use is talking about integrating / providing access to SD3 in applications or services that you sell. Very specifically says it doesn\u2019t apply to outputs. We can sell images / media we make with SD3 without limit. \n\nWhile I\u2019d rather it was an MIT license, or similar, have to give them credit for making the licensing a lot more clear (their glossary of terms in particular), and $20/m for less than $1m in revenue for applications / services that integrate SD3 really isn\u2019t that bad.",
            "To be fair, if you are a \\_professional\\_ artist, 20$/mo should not rock your boat so much for a tool so powerful.",
            "In the \"STABILITY AI NON-COMMERCIAL RESEARCH COMMUNITY LICENSE AGREEMENT\" is stated that \"Derivative Works do not include the output of any Model\", so if you only want to use (even commercially) images made with the model you do not need a commercial licence.",
            "Create images, save it as jpg (All metadata gone), use it for your commercial or private stuff.\n\nNo one will ever know that you use SD3, lol.",
            "$20 a month for commercial (under $1M revenue) seems pretty cheap",
            "What's the source on the timer?",
            "Where is this countdown clock?",
            "you want the **sd3\\_medium\\_incl\\_clips\\_t5xxlfp8.safetensors** cause it contains all necessary weights, including fp8 version of the T5XXL text encoder, offering a balance between quality and resource requirements.\n\nSo only with the 10gb model you been able to experience the full power of the new text generation. On the Downside it need more ressources cause of that.",
            "yup\n\nhttps://preview.redd.it/lct2rp9zf56d1.png?width=1024&format=png&auto=webp&s=59e2809488859be018fda7cffc81c5dba5382a54",
            "According to the license, the outputs aren't considered \"Derivative works\", but the LoRA and finetunes are.",
            "\\\\Comfy\\\\ComfyUI\\\\models\\\\clip\\\\",
            "I think safetensors files are safe",
            "Maybe check his profile / twitter and you see that he is a DEV.\n\nWtf.......",
            "edit: seems like this is just ~~a prediction~~ karma farming, assuming the release time is still at 10AM EST",
            "It doesn't yet.",
            "comfy, forge",
            "You are right, it probably wont get released, and never will.",
            "Yep, it is absurd.\n\nFor an individual cost of hw is massive (also quite depends on where you live), plus paying 20$ per month for maybe earning a few dollars as a hobbyist from the model?  \nAnd on the other hand there is a small company which can earn 800,000$, yet still pay only 20$ to stability? \u00af\\\\_(\u30c4)_/\u00af\n\nWhile it is highly unlikely to be enforced, I know some non-commercial licences are classifying donation links (e.g. in bio on social network or on a page of free open-source project) as commercial, so that's another can of worms.\n\nThinking about it, isn't use and output from dalle3 legally free (e.g. bing image creator)? That would be another gem - you wouldn't be legally for free able to generate and use a logo for your free open-source project with \"open\" SD3, but you could with proprietary commercial model like dalle...",
            "It is free.",
            "They will not, as they attempt to test new monetization schemes\u00a0",
            "Way better prompt adherence, should actually understand english instead of mostly ignoring prepositions, can generate images that aren't gray in brightness, can do text, much better details / much less ugly artifacts with small details.\n\nAll of these are disconnected from the parameter count. Bigger parameter count does generally mean better understanding of things and more room for knowledge.",
            "It has *much* greater potential than sd1.5. The technology is just more detailed and more understanding. The quality may not exceed 1.5 or sdxl at first, but it is much more capable even with less parameters.",
            "The main issue to overcome with SD3 will be adoption. Since it doesn't require the two step setup SDXL did, this might be an easy one.\n\nThe next will be tooling - until they add support. The regular suspects A1111, Regional Prompting, ControlNets, Inpainting/Outpainting, etc.\n\nFor SD1.5 it's true you don't get the same prompt adherence as you'd expect from SD3, but there are research papers and code out there which fix those issues in SD 1.5.\n\nSo I expect the quality of the base model to be better, not only because the images are trained at more than 512x512, but for the double the number of parameters.\n\nSo maybe stick to 1.5 for another 6 months or so, then come and see if all tooling for SD3 is ready.",
            "You can do that already even with the \"non-commercial\" license as unless there's something not obvious the non-commercial aspect only applies to the models themselves not their output.\n\n\" \"Derivative Work(s)\u201d means (a) any derivative work of the Software Products as recognized by U.S. copyright laws and (b) any modifications to a Model, and any other model created which is based on or derived from the Model or the Model\u2019s output. For clarity, Derivative Works do not include the output of any Model. \"\n\n\nStable Cascades version for reference: https://huggingface.co/stabilityai/stable-cascade/blob/main/LICENSE\n\nSo if all you care about is the images themselves it shouldn't be a problem.",
            "No! The images are always free from the bounds of the agreement, it's even written in the licence! If you're only interested in the images made with a model you don't need a commercial licence, even if the images are used commercially.",
            "Create images, save it as jpg (All metadata gone), use it for your commercial or private stuff.\n\nNo one will ever know that you use SD3, lol.",
            "10 am EST is what I've been seeing.",
            "It's a t5 and 2 clip text encoders if I remember correctly. So 3 overall.\u00a0",
            "Where to get those?",
            "source?",
            "i'm downloading this one: sd3\\_medium\\_incl\\_clips\\_t5xxlfp8.safetensors\n\n10Gigs",
            "small",
            "The Comfyui example they've got uses the `sd3_medium.safetensors` file",
            "Of course you can't use any of those.  Why on earth would you want to throw away all the 'old' stuff?  It will be months before we get fine-tuning that makes this as flexible as current stuff.",
            "My first impression is... The download is very slow.",
            "it seems to work fine right now\n\nhttps://preview.redd.it/yoj6y39mn56d1.png?width=1024&format=png&auto=webp&s=8cf61de8156bb52f81dfb2e27852ad540e70162d",
            "try it on forge, not working,\n\n    everyone, I can only use it with comfyui or stableswarm, why can't forge or fooocus do anything with it?",
            "under text\\_encoders folder in the sd3 repository",
            "Where do you get the comfy nodes from? My comfy manager is showing no results when looking for the missing nodes :(",
            "counter-example ;p \n\nhttps://preview.redd.it/9qwjzxy0v56d1.png?width=1024&format=png&auto=webp&s=89f779b7a6ca94e582b4031924db57c8277851bc",
            "Just click on any of the .safetensors file names and it should show you the SHA-256 value.",
            "Currently only ComfyUI supports SD3. Start with basic YouTube tutorials. You can drag and drop ComfyUI workflows into the browser window, and don\u2019t forget to install ConfyUI Manager extension to download additional nodes that will be in workflows",
            "I think they made it clearly: SAI < community.  \nGuy who created pony, spending more than 100,000$, and made SDXL popular, was insulted by them in their discord because he wanted cooperate with them and train Pony 7 on SD3 (which would make a huge step in increasing popularity of SD 3, that's for sure).",
            "So we got the image of an angel from the old testament in photorealism?",
            "hey the text is kind of working, so yay?",
            "But does it controlnet? \ud83e\udd14\ud83e\udd14\ud83e\udd14",
            "Just throw it in the loader of practically any 1.5 workflow? Or are there particular nodes/workflows for SD3?",
            "SD3 Medium throws error in Comfy, A1111 and Easy Diffusion",
            "And forge?",
            "I can confirm it too.  It still seems to have issues with hands and many other prompts I've thrown at it.  I can't lie, I'm a bit disappointed so far.\n\nI invite you to try \"a man laying on the beach\" or \"a woman laying on the beach\" and delight in the horror.  I can't find a combination of params that will show anything other than horrible science experiments gone wrong.  \"A dog wearing a spacesuit on the moon\" works perfectly, though.\n\nIf this is them \"protecting from misuse\" then it's a bit extreme.  A man in shorts or a woman in a bikini shouldn't be seen as profane.\n\nUpdate: Seems to work better in portrait orientations than landscape, but still not great.",
            "Most likely will. Worst case scenario, you would need to make/download a custom ComfyUI workflow inside StableSwarmUI",
            "Are there any other models around that we can build on?",
            "You are the most optimistic Rob I've come across today. I feel Robbed of any chance at this release coming to a useful fruition, if ever.",
            "I tried SDXL right after it was released and the base model could generate anthros with a bit of effort, this model really must be shit then.",
            "it's available now and i am downloading it already :)",
            "Or a swimsuit, going by the API.",
            "Considering some.of the nipples I have seen on Civitai that could be a threat to some people. \ud83d\ude02",
            "jeees you should hide stuff like this with spoilers! people nowadays have no empathy.... xD",
            "I feel attacked!",
            "Truly mankind's worst creation",
            "oh my! \ud83d\ude31",
            "But is it a *democratised* nipple?",
            "From the looks of things, looks like anatomically correct limbs are too risque.",
            "We'll know that AI has achieved a milestone when SCP-096 goes after the AI itself.",
            "ohshit",
            "no, thats not allowed anymore, what do you think this is early 2010s and earlier? its all censored now or the social media overlords and payment processors will ban. government might even get involved (if they arent already)",
            "I wouldn't mind paying 20 bucks a month for the license, but not for something that produces such bad images in this state. \"The community will improve it\" is great but if one is providing a paid product, he should provide incentive to buy it.",
            "ww3 already ongoing. so probably during. not after.",
            "sd4 is just gonna be pony8 trained from scratch",
            "Yeah and the specs, what will we need for large? Will 24gb VRAM be enough and leave enough room for Lora or is this already going into RAM?",
            "All the researchers quit and they have no more money for GPUs.\u00a0\n\nSo unless something changes, don\u2019t hold your breath.",
            "Yes, it's generally better at most things bar a few niche exceptions in very specific use cases.",
            "The big one has the T5 encoder in it. If you want to train the encoder use that one.",
            "`Image of t4yl0r_svv1ft_v4_final_pony singing on stage`",
            "Pterodactyl!! ....wait, what are we doing?",
            "Probably similar to how I can't get a photo of a black widow spider that doesn't have ScarJo's face, more often than not.",
            "Jail.",
            "https://i.ibb.co/vYcP2Kd/bb615933-9b73-4ffa-a178-9083d50c110d.png",
            "Not quite, they already said they removed hundreds of millions of pictures from the dataset for \"safety\", including pretty much everything with an artist in it's name",
            "You can generate images of dudes named Sam but not CSAM. \n\nWhich I approve of.",
            "Wasn't there a NSFW filter that somehow recognized NSFW content (even without explicit prompt) and blurred it out? This step could be done on a level before you get any meaningful content out of it. But hey clever people will always find a way to bypass this given time.",
            "pun intended",
            "Y2K is quite a funny comparison to make because the problem was very real, it's just a lot of people took it seriously and invested a lot of resources to avert it",
            "Oh, that's a big change. With Pony I was only using ancestral sampling because it still got big results.\n\nWas non-ancestral sampling always better than ancestral? I never looked into the difference.",
            "Probably without nude pictures, the model kind of thinking that cloth is part of human body, so anatomy of the human body can be in all sort of shapes and proportions :)",
            "Works a little different. The base model is only 4.3gb, but you also need to download text encoder files, around 7gb worth, so \\~11gb total. There's also a base model with the text encoders included that clocks in at 10.9gb.",
            "I'm using the simple workflow with fp16 clip  \n  \nModel sizes:\n\nsd3\\_medium.safetensor - 4236 MB\n\nclip\\_g.safetensor - 1357 MB\n\nclip\\_l.safetensor - 240 MB\n\n5xxl\\_fp16.safetensor - 9558 MB\n\n\n\nvram usage hit 11.3GB when clip text encode was working and about 7.5GB during ksampler generating a 1024x1024 image",
            "**sd3_medium.safetensors**: 4,337,667,306 bytes (4.35 GB on disk)",
            "Hmmm there\u2019s a user in this thread claiming an 11.3 gb peak use when generating, but that was 1024 by 1024 and with the text encoder I think?",
            "SDXL works on 4GB with forge and comfy",
            "I'd say it's a little more resource hungry. With my 8GB card I could do 1024x1024 with no problem on SDXL, including with Loras, but using sd3_medium_incl_clips.safetensors I can only go about as high as 1024x768 comfortably.\n\nIf I try 1024x1024 I get multiple errors about out of memory and clearing cache between every step, with no guarantee it will succeed. This is with --normalvram. I still get occasional crashes after sampling when it switches to VAE decode.\n\nTrying to use sd3_medium_incl_clips_t5xxlfp8.safetensors results in an immediate crash, even after it automatically loads in low VRAM mode.",
            "Same vram as using 2B LLM in FP16 with 4096 tokens context.",
            "Here too! , I had to select the correct CLIP models",
            "You're right, that needs additional Info :-)  \nSo, heres is a comment you won't find on hugging the model card, hope it helps:\n\n\"Improved photo-realistic image generation, Enhanced prompt adherence, Multimodal input capabilities, Better text generation and typography, **Three text encoders (CLIP l/14, OpenCLIP bigG/14, and T5-v1.1 XXL)**, Safer content generation by removing NSFW images\"  \nSo i think the answer must be:\n\n* **sd3\\_medium.safetensors** includes the MMDiT and VAE weights but does not include any text encoders. = 0 Text Encoders\n* **sd3\\_medium\\_incl\\_clips\\_t5xxlfp8.safetensors** contains all necessary weights, including fp8 version of the T5XXL text encoder, offering a balance between quality and resource requirements. = 3 Text Encoders, including **t5xxlfp8** withe the best quality in Text Generation but highest ressource requirements\n* **sd3\\_medium\\_incl\\_clips.safetensors** includes all necessary weights except for the T5XXL text encoder. It requires minimal resources, but the model's performance will differ without the T5XXL text encoder. = 2 Text Encoders, excluding **t5xxlfp8** so only **CLIP l/14, OpenCLIP bigG/14** which seems to be the solution with the most performance, so i guess there a mid and low text gen modules, i asking myself if its correctness is lower on long text?",
            "Probably uploaded already, just have a set release time when the repo will be made public.",
            "They had to restart in the middle cause mom tried to make a call, and the connection broke.",
            "They can upload the weights and not make them visible until they decide so. It is how most releases are done on huggingface.\n\nIf they are still uploading, this announcement should have waited.",
            "Yep, already generating with ComfyUI examples:\n\nhttps://imgur.com/6YR7KIs\n\nAround 10 seconds without upscale.",
            "Ah, what I expected. Probably wrote this useless trash with Chatgpt too.",
            "Thx guys",
            "you can type in anything. be creative!",
            "Yes",
            "Finally someone that has read the TOS!",
            "Oh wow! I didn\u2019t see that distinction and was very upset about their threat of government violence towards those who don\u2019t pay $20 per month.",
            "Where is the license that says that?",
            "My understanding (and I could be wrong) is that we also can't use the output of Loras we train ourselves, as this counts as derivative work. Do you think that's correct?",
            "If you are a professional artist, you do not need to pay the 20$/mo, your outputs can be distributed without paying anything.",
            "It won't. If there will be a nice checkpoint based on SD3 I'm interesting in, then I'd just sign up per license.",
            "So you have to pay if you're providing a commercial service that uses the;SD3 model (or a derivative of) in some way (your clients are generating their own images using sd3 via your platform), but you don't have to pay if you're producing images using sd3, then selling those images (your clients are buying a specific image that has already been generated).",
            "https://preview.redd.it/tmnln9fwr46d1.jpeg?width=547&format=pjpg&auto=webp&s=71969f5bf94723e40a85aacf4ff87258d9413666\n\nlol",
            "Nonsense: the images made with the model aren't bound by the licence, it's even stated in the non commercial agreement.",
            "they can know, they'll never be able to prove it",
            "Even if it's not traceable, it's the matter of sleeping safe and \"professionals have standards\" :P",
            "Photoshop",
            "Allegedly twitter.  It just shows timezone of 3.0 release.",
            "It just shows timezon of 3.0 release. Its due in 1 hour",
            "thanks",
            "I guess the fp16 text encoder might be slightly better (to be tested, uncharted waters). So I guess highest quality will be fp16 text encoder and the **sd3\\_medium.safetensors** file.",
            "He's a dev who does not work for stabilityai",
            "Yea, this is a really weird post... SD3 was just released!!! not",
            "Then when is the point between being a \"commercial\" creator and being some guy making thumbnails and art for a twitch channel they run for fun but earn a couple of dollars from now and then",
            "Yeah okay, this model sounds insane then, but thats coming from me who recently got into AI.\n\nI find it cool that my pc can run sd3 medium (or atleast should be able to), but the 8b model? From the way people are hyping up the 2b one it makes the 8b seem like its gonna be able to do magic on your computer screen.",
            "Yeah, so basically when sd3 has been out for aslong as 1.5 has now (like 2 years?) then it will be way better than 1.5 because it was \"built\" on better technology?",
            "invisible watermarking exists, but even if it didn't, I'm not looking to break the law over $20",
            "Thanks for answering... That is a lot of clip :P\n\nIt seems I will have to buy even more RAM than 32.",
            "Thanks, in the release you have 3 options, download the model, download the model with clipg and clipl included, and download the model with t5, clipg and clipl included... And you can also download the clips separately...\n\nSo... There is a lot of stuff here, I think I'm going to start testing the model with clipg and clipl included. But the best option if you have enough vram I think is the model+t5+clipg+clipl included",
            "https://countingdownto.com/w4/ZjqqNzzJ",
            "https://preview.redd.it/yaf8rfhy656d1.png?width=1024&format=png&auto=webp&s=f31e8ee6f2232408f24b726e806490086b49979d",
            "Why that one specifically?",
            "Months to catch up with what we have for SDXL now, yeah, but probably days to weeks before we see a good starter amount of new loras, etc",
            "My second impression, so far aesthetically I prefer Pixart...  But SD3 gives more defined renders with the same settings.\n\nhttps://preview.redd.it/2a9lze1db66d1.png?width=2286&format=png&auto=webp&s=37e19115f5ab0d57bad920dc5e27e680d2f8f397",
            "d'oh!  haha thanks!",
            "Update comfy. They came automatically after update",
            "Thanks for the tip - if anyone could do so and share them here, that would be awesome. Thanks!",
            "thanks!",
            "Nah, they were surrounded by hundreds of fractal angel wings and made of eyeballs too...   or was that the even more schizo revelations version?  I read it once so I could draw one for somebody for their birthday and I used a description of the seraphim from somewhere...",
            "The HuggingFace repo has example workflows.",
            "Set it up using Stable Swarm",
            "And my axe?",
            "Don\u2019t think so, not initially. I believe comfy already has functionality built in for SD3.",
            "Lumina and Pixart are the most likely contenders. We can also keep on building on SDXL but the underlying text encoder is a limiting factor seemingly.",
            "It's never going to be good. What did I Rob you of now? Was it this Rob that robbed you? No. Not this Rob. Rob",
            "Sure seems like it. It's missing even just basic anatomy for humans let alone anthropomorphic animals",
            "Or like, women generally.",
            "lol you are right",
            "or scratch and bite",
            "I like your optimism that there will be an after.",
            "Dual GPU is already common in the textgen space.",
            "Oh so never then. Looks like a new image generator will replace Stable Diffusion at some point.",
            "Ah you mean like the biggest one? Because one is fp8 and the other is fp16 and I'm not sure which one would be more appropriate",
            "yeah good luck to their compiler decoding what 70HN C3N4 means, spoiler they can't see it.",
            "\"singing\"",
            "Lol I actually requested a Lora of a black widow spider on CivitAI months ago because of that exact problem.",
            "believe it or not.",
            "If true that would be terrible given SDXL\u2019s wide knowledge of thousands of artists which is quite powerful.",
            "what differentiates sam from john smith though?",
            "That was part of the released pipeline of 1.5, but not in the model itself",
            "At the time, the news was claiming the Y2K bug would *end civilization* if left unaddressed.\n\nYes, people took it seriously and in the systems where it was actually an issue, it was solved, but at no point was the \"news hype\" of civilization collapse because every chip everywhere would suddenly stop working a possibility. And the \"Y2K fix!\" software being sold to make your Windows PC \"Y2K compliant\" (which wasn't a thing).\n\nPeople were literally buying up emergency supplies in 1999 because of the news. Even if we'd done nothing at all, ammo and emergency bunkers were never a remote possibility of being necessary to \"survive\" some older systems crashing.\n\nMight AI cause problems, even severe ones? Sure, yeah, I never disputed that. Is AI going to cause the *end of humanity?*\n\nNo. It's not. And people who say otherwise are selling FUD. This isn't the first time some new thing with some risks is being sold to the public as a world-ending threat. It won't be the last.\n\nBut if people want to buy the whole \"well, the world was ending, but you spent hundreds of billions of dollars to prevent it\" line (which of course will never actually be shown to have prevented anything remotely close to the \"risks\" being sold), be my guest. I didn't buy the Y2K fix software and I don't buy that companies like OpenAI or StabilityAI are implementing any sort of \"anti-human-extinction\" controls into their products.",
            "probably",
            "7.5GB during the actual generation seems tolerable, but seems like my 12GB GPU will be choking once secondary models are eventually added into the mix",
            "11.3, damn. I guess a 12 gig card should handle 512 by 512 easily enough though.",
            "I saw it top at 9,6 GB out during the text encoder phase (fp16 version). During image creation resource consumption is much lower (of course you can always crank up batch size to reach a limit). See details here: [https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource\\_consumption\\_and\\_performance\\_observations/](https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource_consumption_and_performance_observations/)",
            "How good is it tho?\n\nI have like 6 gb.\n\nTakes me like 20-60+ mins per image..",
            "As far as I saw it, sd3\\_medium\\_incl\\_clips\\_t5xxlfp8.safetensors will work well with 8 GB cards. \n\nSee details here: [https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource\\_consumption\\_and\\_performance\\_observations/](https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource_consumption_and_performance_observations/)",
            "Why would you want to use sd3\\_medium without any text encoders? wouldn't that be bad at understanding prompts?",
            "It's in the first line of the announcement \ud83d\ude03",
            "Thank you.",
            "https://stability.ai/creator-license-agreement",
            "As I read it, they are pretty thorough in making clear the restrictions have nothing at all to do with outputs of any kind. That would also apply to outputs from loras. \n\nNow, whether you making loras available in an app that integrates SD3 as some kind of SaaS or other paid service counts as commercial use according to the license is a different question. But in that case, any lora that works with SD3 would also require either an SD3 base model or a derivative model fine tuned from it. So providing a paid app, web frontend, professional service, etc would already be considered commercial in that case, making the lora question redundant. Probably.",
            "Yep, that too. I think it's fair to compensate the creators for their work when someone makes more than occasional profit using the tool.",
            "Sorry, I've just seen many people complaining about this.",
            "Professionals also read licences: \"Derivative Works do not include the output of any Model\", so the images made with a model are free from the bounds of the agreement.",
            "And ? \n\nWhen it gets really released today [https://huggingface.co/stabilityai/stable-diffusion-3-medium](https://huggingface.co/stabilityai/stable-diffusion-3-medium)\n\nOn the stabilityai huggingface, we dont have to care.\n\nHow do you excactly know if he helped them behind the scene ? \n\nYou know everyhting ?",
            "The [Creator License](https://stability.ai/creator-license-agreement) doesn't consider the output of the Core Models to be a \"Derivative work\"(1d), so making thumbnails and art for a twitch channel wouldn't need a creator license.\n\nYou *would* have to pay the $20 a month if you're locking your FineTunes and/or LoRAs (which are derivative works) behind a Patreon and making less than 1M/yr from less than 1M users, e.g.",
            "It will most likely be better, it won't overcome fundamental limitations like the fingers issue. A lack of finetunes might be an issue. We'll have to wait and see.",
            "No, I would say less than a month for sure. Less than one or two weeks more likely. I'll admit I'm an optimist, but people already have the training sets ready to go, and there are *many* more people creating fine-tunes now. Maybe they might need to tweak their captions and do several test runs, but I think even training time is less for SD3 (I don't know this though).\n\nPeople forget that the power of stable diffusion isn't just the model itself, it's its open-source nature and the hugely collaborative community that surrounds it.",
            "it uses similar resources to SDXL according to Stability, but more ram is always good I noticed a nice improvement going from 32 to 64 myself as everything loads faster/ have more things going on.",
            "No reason, got all 3 for the future , just generated 1st image with clip inc model without text while downloading.  Happy Days!\n\nhttps://preview.redd.it/qn1k0ac3a56d1.png?width=896&format=png&auto=webp&s=4d76ad1135201a746f65fcdcb396e2f23418a69e",
            "Oh, just noticed the update failed because I was using symlinks for the model folders.",
            "Here are the SHA-256 hashes:\n\nModels:\n\n- `sd3_medium.safetensors`: `cc236278d28c8c3eccb8e21ee0a67ebed7dd6e9ce40aa9de914fa34e8282f191`\n- `sd3_medium_incl_clips.safetensors`: `3bb7f21bc5fb450220f4eb78a2f276b15422309d5166a4bdeb8c3b763a3a0581`\n- `sd3_medium_incl_clips_t5xxlfp8.safetensors`: `92db4295e9c9ab8401ef60566d975656a35b0bd0f6d9ce0d083725171f7b3174`\n\nText Encoders:\n\n- `clip_g.safetensors`: `ec310df2af79c318e24d20511b601a591ca8cd4f1fce1d8dff822a356bcdb1f4`\n- `clip_l.safetensors`: `660c6f5b1abae9dc498ac2d21e1347d2abdb0cf6c0c0c8576cd796491d9a6cdd`\n- `t5xxl_fp16.safetensors`: `6e480b09fae049a72d2a8c5fbccb8d3e92febeb233bbe9dfe7256958a9167635`\n- `t5xxl_fp8_e4m3fn.safetensors`: `7d330da4816157540d6bb7838bf63a0f02f573fc48ca4d8de34bb0cbfd514f09`",
            "I wouldn't expect great performance on an axe.",
            "And my bow",
            "And my bow",
            "That response was quite ROBotic. Almost as if you put your name Into a ROBust AI system to generate you a response.",
            "Oh no. Please, Don't scare me, I'm only 20",
            "I'm pretty sure they're not real.",
            "The ai overlords are gonna exterminate us b4 we get the chance to",
            "\"stage\"",
            "I'm using a 4070ti with 12GB vram. 1024x1024 works fine.\n\nTried fp8 clip model (4779MB) and usage dropped to only 11.1 GB on clip encode so I'm guessing 9.5GB one is somehow adjusting itself to not use all the vram? Can't confirm tho.",
            "Uber in fact\n\nhttps://preview.redd.it/p5n4fl3dk56d1.png?width=1152&format=png&auto=webp&s=96390cd7f0a9c7182fd13f4da3ad296a91ca50b1",
            "2-3 minutes 1024x1024 - 20 steps\n20- 30 seconds 1024x1024 - 4 steps with hyper loras or LCM loras",
            "If you do, it should be at least 10x faster than you say. Try another UI.",
            "Report says max 7.2 GB used. Right now my system is using a bit over 1 GB without anything SD running. I guess I can get there if I close a couple of apps that used some VRAM.\n\nI used to do that pretty regularly for the benefit of A1111, but haven't had to since I've been using Comfy with SDXL. I'll experiment more but so far I haven't been too impressed with SD3.",
            "Because there also is a fp16 version of the text encoder you might use along with this one. Or an own fine tuned text encoder just with the MMDiT and VAE. Makes sense to me.",
            "Except this post was made by some rando",
            "Uploading takes like a minute with the kind of connections these companies have...",
            "That's the one that costs money.",
            "No, that's phrase definition.\n\nFrom (now up) HuggingFace: \n\n**\"Commercial Use**: This model is not available for commercial use without a separate commercial license from Stability. We encourage professional artists, designers, and creators to use our Creator License.\"\n\nThe exact license for those weights is sill 404 error, that's what will be important. While yes, outputs are free of restrictions, running the model to generate those is separate matter.\n\nBut overall license for Core Models is a little vague and focuses on service providers.",
            "oh good, thank you for that",
            "You wanted an axe, here is a wooden spoon.",
            "Rob has been accused of ROBotiscim before. Rob still has not been Robbed of humanity yet. And to accuse me of such Robs me of my dignity. Rob Rob Rob",
            "Only 40 years to go until you have the maturity SAI desires, then.",
            "w*men \u2615",
            "The algorithms will exterminate us b4 the machines ever become sentient.",
            "Other than outright googling for them could you tell me where I could get them?",
            "Yes, which is the one that matters for your question. The one that costs money says that it doesn't apply to outputs. In other words: You don't need to worry about the license, and don't have to pay anything, if all you're planning to make money from is the media you generate.",
            "That just robbed me of my free will. Rob robbed me of my ability to not laugh. We really need a robust AI system to stop rob. Rob is like Robins from the show. always robbing my time and making me laugh. Stay away robotic rob! :v"
        ]
    },
    "How To Run SD3-Medium Locally Right Now -- StableSwarmUI": {
        "title": "How To Run SD3-Medium Locally Right Now -- StableSwarmUI",
        "score": 274,
        "url": "https://www.reddit.com/r/StableDiffusion/comments/1de65iz/how_to_run_sd3medium_locally_right_now/",
        "content": "Comfy and Swarm are updated with full day-1 support for SD3-Medium!\n\n- Open the HuggingFace release page [https://huggingface.co/stabilityai/stable-diffusion-3-medium](https://huggingface.co/stabilityai/stable-diffusion-3-medium) login to HF and accept the gate\n\n- Download the SD3 Medium no-tenc model [https://huggingface.co/stabilityai/stable-diffusion-3-medium/resolve/main/sd3\\_medium.safetensors?download=true](https://huggingface.co/stabilityai/stable-diffusion-3-medium/resolve/main/sd3_medium.safetensors?download=true)\n\n- If you don't already have swarm installed, get it here [https://github.com/mcmonkeyprojects/SwarmUI?tab=readme-ov-file#installing-on-windows](https://github.com/mcmonkeyprojects/SwarmUI?tab=readme-ov-file#installing-on-windows) or if you already have swarm, update it (update-windows.bat or Server -> Update & Restart)\n\n- Save the `sd3_medium.safetensors` file to your models dir, by default this is `(Swarm)/Models/Stable-Diffusion`\n\n- Launch Swarm (or if already open refresh the models list)\n\n- under the \"Models\" subtab at the bottom, click on Stable Diffusion 3 Medium's icon to select it\n\nhttps://preview.redd.it/nj5uxt0uz46d1.png?width=672&format=png&auto=webp&s=8c332ec59c01f773f74f0748025c9c9d25a8a151\n\n- On the parameters view on the left, set \"Steps\" to 28, and \"CFG scale\" to 5 (the default 20 steps and cfg 7 works too, but 28/5 is a bit nicer)\n\n- Optionally, open \"Sampling\" and choose an SD3 TextEncs value, f you have a decent PC and don't mind the load times, select \"CLIP + T5\". If you want it go faster, select \"CLIP Only\". Using T5 slightly improves results, but it uses more RAM and takes a while to load.\n\n- In the center area type any prompt, eg `a photo of a cat in a magical rainbow forest`, and hit Enter or click Generate\n\n- On your first run, wait a minute. You'll see in the console window a progress report as it downloads the text encoders automatically. After the first run the textencoders are saved in your models dir and will not need a long download.\n\n- Boom, you have some awesome cat pics!\n\n\n\nhttps://preview.redd.it/bu2rceayz46d1.png?width=1804&format=png&auto=webp&s=95a4bce5fc69dc1602298be9d76c58c93be606c5\n\n- Want to get that up to hires 2048x2048? Continue on:\n\n- Open the \"Refiner\" parameter group, set upscale to \"2\" (or whatever upscale rate you want)\n\n- Importantly, check \"Refiner Do Tiling\" (the SD3 MMDiT arch does not upscale well natively on its own, but with tiling it works great. Thanks to humblemikey for contributing an awesome tiling impl for Swarm)\n\n- Tweak the Control Percentage and Upscale Method values to taste\n\nhttps://preview.redd.it/umnepy7a056d1.png?width=333&format=png&auto=webp&s=f991c2ee39b8bb138fffb93f19042b1dfef250eb\n\n- Hit Generate. You'll be able to watch the tiling refinement happen in front of you with the live preview.\n\n- When the image is done, click on it to open the Full View, and you can now use your mouse scroll wheel to zoom in/out freely or click+drag to pan. Zoom in real close to that image to check the details!\n\n\n\n[my generated cat's whiskers are pixel perfect! nice!](https://preview.redd.it/k4kmbo9o056d1.png?width=1377&format=png&auto=webp&s=d92afe8dc34c424929e9221d3e862876f7ac3cc5)\n\n- Tap click to close the full view at any time\n\n- Play with other settings and tools too!\n\n- If you want a Comfy workflow for SD3 at any time, just click the \"Comfy Workflow\" tab then click \"Import From Generate Tab\" to get the comfy workflow for your current Generate tab setup\n\nEDIT: oh and PS for swarm users jsyk there's a discord  [https://discord.gg/q2y38cqjNw](https://discord.gg/q2y38cqjNw)",
        "num_comments": 301,
        "comments": [
            "Does this support large horse anatomy yet?",
            "I'm trying to use the comfy workflow \"sd3\\_medium\\_example\\_workflow\\_basic.json\" from HF, but i'm not sure where to find these clip models? Do I really need all of them? \n\nhttps://preview.redd.it/llrm6swb756d1.png?width=358&format=png&auto=webp&s=02ad0253b21e2ebc533344f29f57fa3a53a74252\n\nEdit : Ok I'm blind they are in the text_encoders folder sorry",
            "ty sir, now i am go bake cirnos with SD3",
            "back to the mergeing board",
            "why am I getting low quality results.\n\nhttps://preview.redd.it/5zud0klhd56d1.png?width=1024&format=png&auto=webp&s=63ab5ffa1627ddf4889aaadf771abac5bf62d1b1\n\nPrompt:\u00a0A woman hugging a man,\n\nmodel:\u00a0OfficialStableDiffusion/sd3\\_medium,seed:\u00a0330848970,steps:\u00a028,cfgscale:\u00a05,aspectratio:\u00a01:1,width:\u00a01024,height:\u00a01024,swarm\\_version:\u00a0[0.6.4.0](http://0.6.4.0),date:\u00a02024-06-12,generation\\_time:\u00a00.00 (prep) and 24.02 (gen) seconds",
            "time for a week of finding the right training settings (o\u309c\u25bd\u309c)o\u2606",
            "there are 2 more [safetensors](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/sd3_medium.safetensors) file , What differences do they have?",
            "Nai3 open source just a week away",
            "Tested both [sd3\\_medium\\_incl\\_clips.safetensors](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/sd3_medium_incl_clips.safetensors) and [sd3\\_medium\\_incl\\_clips\\_t5xxlfp8.safetensors](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/sd3_medium_incl_clips_t5xxlfp8.safetensors) in comfyui . Both had almost same speed for me (2.05 it/sec and 2.3it/sec) . [sd3\\_medium.safetensors](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/sd3_medium.safetensors) didn't work for me in comfy , i think because i havn't downloaded the clip models yet , so that is why . I will just use [sd3\\_medium\\_incl\\_clips\\_t5xxlfp8.safetensors](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/sd3_medium_incl_clips_t5xxlfp8.safetensors) as the speed is same. Overall very good model. Understand prompts very well but extremely censored base model.",
            "Very unimpressed. Sucks.",
            "Downloading SD3 was a huge waste of bandwidth.",
            "Cat pictures...\u00a0\n\n\nThe number one usecase for SD...",
            "got a error, maybe something to do with vae, looks like the inferencing is stopped at last step:\n\n: Invalid operation: ComfyUI execution error: Given groups=1, weight of size \\[4, 4, 1, 1\\], expected input\\[1, 16, 128, 128\\] to have 4 channels, but got 16 channels instead",
            "I installed stable swarm yesterday, I just clicked update-windows.bat, it pulled the latest changes but when I want to try SD3 model I get 09:39:26.913 \\[Warning\\] \\[BackendHandler\\] backend #0 failed to load model OfficialStableDiffusion/sd3\\_medium.safetensors",
            "I got this error, anyone knows how to fix this?\n\nI'm using a laptop, RTX3030 6GB VRAM\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n  \nEdit: it's RTX3060 (I'm sleepy when I wrote that and just saw this now, lols)",
            "thank you!  \nIt works on my RTX2070 GPU.\n\nhttps://preview.redd.it/3cpe653ni56d1.jpeg?width=1024&format=pjpg&auto=webp&s=15fdf01fb5806b8540b74143260c45c9433e496c",
            "SDXL > SD3",
            "https://preview.redd.it/7z47jq1me56d1.png?width=1280&format=png&auto=webp&s=775953ad2cbe147fa459780b7c36e6b65bd7a542\n\nThank you for the release, much love! <3",
            "What about 4gb vram?",
            "I'm using ComfyUI and using the basic workflow. It says it's missing TripleCLIPLoader, ModelSamplingSD3 and EmptySD3LatentImage. How do I get these nodes?",
            "Running here in Linux box, 3060 12GB and 32GB of RAM - all good, the text output is great!\n\nhttps://preview.redd.it/rjpddd1fi56d1.png?width=3711&format=png&auto=webp&s=afab641645e9a6f08a218653c09b40186e1a7e67",
            "Works, but sd3 is very bad.",
            "I started working on a tutorial for this baby for Windows, Massed Compute, RunPod and if works on a Free Kaggle account",
            "https://preview.redd.it/tabhhh1f4l6d1.png?width=1024&format=png&auto=webp&s=8cbb67fef92bf59920aa82cd30dc87672e3bfee4",
            "I've been using SwarmUI exclusively lately and apart from special workflows, that's where I am staying.",
            "poggers",
            "The model is extremely censored haha",
            "Woooooooow! Thanks for the detailed instructions. Can't wait to try it!!",
            "Is there a maximum token count? Or can I basically enter as much text into the prompt as I want to?",
            "Okay but how do we get it to not do Cronenberg horror?",
            "What is system requirements? I have 8Gb VRAM",
            "im confuse about 3 clip models\n\nthere is a model with 10 GB\n\nand sd3 medium with 5,9 GB include clip",
            "t5xxl model - 9,79 GB\n\nrequire 9,79 GPU Vram ? \n\nOr Ram ?",
            "Thank you for this, works flawlessly for me, 3090, takes about 11 seconds for 1024x1024",
            "Hm, this is weird. For some reason comfyui can't read my clip folder despite being able to read everything else. Gives me\n\nFailed to validate prompt for output 9:\n\n* DualCLIPLoader 101:\n\n  - Value not in list: clip_name1: 'clip_g_sdxl_base.safetensors' not in []\n\n  - Value not in list: clip_name2: 'clip_l_sdxl_base.safetensors' not in []\n\nDoesn't seem possible to set clip folder, only clip vision?\n\n\nEdit: Problem resolved. This is a bug in comfyui. All clip models need to be in the Comfyui/models/clip folder, it will not accept anything relative to ModelRoot.",
            "StableSwarm ftw! Can't wait to get home from work tonight and take it out for a spin!",
            "https://preview.redd.it/2v8ginqdl56d1.png?width=2633&format=png&auto=webp&s=8f732408cf930359549ba6e747dc4dbee7c83e45\n\nI only get a picture of noise on mac, why is that?",
            "Can I use img2img in comfy?",
            "its best to use this or comfyui?",
            "https://preview.redd.it/7o7r9i1ul76d1.jpeg?width=2048&format=pjpg&auto=webp&s=ccf4cad1df59324469e9a66c47f00d8ff843b824\n\nWorks just fine. Had to download the text-encoders manually and place them in Models/clip for SwarmUI to find them and had to stop using samplers and schedulers but after that it worked just fine.",
            "I dont know why but i am getting only black images. XL 1.0 works fine. Radeon 7900XT\n\nhttps://preview.redd.it/fhbqrof8386d1.jpeg?width=2526&format=pjpg&auto=webp&s=526ea1ec2deec4fb8cd03e9abd7afda62f0b196d",
            "worked fine. Thanks you.\n\nhttps://preview.redd.it/pcafckw53b6d1.png?width=1024&format=png&auto=webp&s=2b43c41a07f30240b8f454a415c2a0fa21489e68",
            "Thank you for putting up the minimal inference code without too many dependencies.",
            "What graphic card do I need to run the model?",
            "Tsk tsk. We just barely got access to SD3, and already everyone is just generating pussy pics.",
            "Here we goooooi",
            "Can it do non mutant feet yet?",
            "Can confirm it works with Stable Swarm on a base model Mac Studio M1 Max with 32gb of RAM. I mean, yeah it's slow as hell but so is SDXL on this machine lol. I'm just glad it finally came out :D\n\nhttps://preview.redd.it/m3lceoh9r56d1.jpeg?width=1024&format=pjpg&auto=webp&s=2b605e2c09a28ecad47febbb2ce22ff598bf2566",
            "Friggin terrible model dudes",
            "what are the GPU memory requirements of each version?",
            "can anyone share comfyui workflow for sd3?",
            "I got this error in Swarm when trying to use SD3TextEnc option: \"Invalid operation: No backends match the settings of the request given! Backends refused for the following reason(s):\n- Request requires flag 'sd3' which is not present on the backend\"\n\nHow can I fix this, please?\n\nBTW, Swarm is definitely growing on me, and the more I use it, the more I appreciate it. It's extremely fast, the UI is nice, and it is quite feature-rich. Congratulations for the amazing work! \ud83d\ude4f",
            "anyway to load it in comfy ui itself ? i get this error (Error occurred when executing CheckpointLoaderSimple:  \n  \n'model.diffusion\\_model.input\\_blocks.0.0.weight')",
            "Hey, where do we place the Clips and Text encoders?",
            "got this error after trying to run sd3 in stableswarmUI: \"\\[Error\\] \\[BackendHandler\\] Backend request #1 failed: System.InvalidOperationException: All available backends failed to load the model.\n\n   at StableSwarmUI.Backends.BackendHandler.LoadHighestPressureNow(List\\`1 possible, List\\`1 available, Action releasePressure, CancellationToken cancel) in /home/zephyr/StableswarmUI/src/Backends/BackendHandler.cs:line 1080\n\n   at StableSwarmUI.Backends.BackendHandler.T2IBackendRequest.TryFind() in /home/zephyr/StableswarmUI/src/Backends/BackendHandler.cs:line 842\n\n   at StableSwarmUI.Backends.BackendHandler.RequestHandlingLoop() in /home/zephyr/StableswarmUI/src/Backends/BackendHandler.cs:line 970\" Any possible solution?",
            "Can there be a way to offload those text encoders to a second gpu? (just starting to download the model. haven't tried anything yet)",
            "I get an error trying to load the model.  \"\\[Error\\] Error loading model on backend 0 (ComfyUI Self-Starting): System.InvalidOperationException: ComfyUI execution error: Given groups=1, weight of size \\[512, 16, 3, 3\\], expected input\\[1, 4, 32, 32\\] to have 16 channels, but got 4 channels instead\n\n   at StableSwarmUI.Builtin\\_ComfyUIBackend.ComfyUIAPIAbstractBackend.GetAllImagesForHistory(JToken output, CancellationToken interrupt) in D:\\\\Art\\\\Stable-Swarm\\\\StableSwarmUI\\\\src\\\\BuiltinExtensions\\\\ComfyUIBackend\\\\ComfyUIAPIAbstractBackend.cs:line 445\n\n   at StableSwarmUI.Builtin\\_ComfyUIBackend.ComfyUIAPIAbstractBackend.AwaitJobLive(String workflow, String batchId, Action\\`1 takeOutput, T2IParamInput user\\_input, CancellationToken interrupt) in D:\\\\Art\\\\Stable-Swarm\\\\StableSwarmUI\\\\src\\\\BuiltinExtensions\\\\ComfyUIBackend\\\\ComfyUIAPIAbstractBackend.cs:line 376\n\n   at StableSwarmUI.Builtin\\_ComfyUIBackend.ComfyUIAPIAbstractBackend.LoadModel(T2IModel model) in D:\\\\Art\\\\Stable-Swarm\\\\StableSwarmUI\\\\src\\\\BuiltinExtensions\\\\ComfyUIBackend\\\\ComfyUIAPIAbstractBackend.cs:line 751\n\n   at StableSwarmUI.Backends.BackendHandler.LoadModelOnAll(T2IModel model, Func\\`2 filter) in D:\\\\Art\\\\Stable-Swarm\\\\StableSwarmUI\\\\src\\\\Backends\\\\BackendHandler.cs:line 613\"",
            "Do I need to use the clip loader at all in ComfyUI if I download the big [sd3\\_medium\\_incl\\_clips\\_t5xxlfp8.safetensors](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/sd3_medium_incl_clips_t5xxlfp8.safetensors) model? I can see some good outputs, but I'm wondering if it would be better? I just connected the clip from load checkpoint to both of the text encode blocks.",
            "where do i have to put the checkpoints in comfyUI?I put the sd3\\_medium.safetensors, sd3\\_medium\\_incl\\_clips.safetensors and sd3\\_medium\\_incl\\_clips\\_t5xxlfp8.safetensors into the ComfyUI\\_windows\\_portable\\\\ComfyUI\\\\models\\\\checkpoints folder. Is this wrong? did i download the wrong models? help please...",
            "[deleted]",
            "TLDR but is there AMD support?",
            "A1111 can't run this yet?",
            "Didnt work :s fresh install of swarmui , 5900X 64GB, 4080 16GB, crash when I try to gen :\n\n20:13:01.505 \\[Error\\] \\[BackendHandler\\] backend #0 failed to load model with error: System.AggregateException: One or more errors occurred. (The remote party closed the WebSocket connection without completing the close handshake.)\n\n \\---> System.Net.WebSockets.WebSocketException (0x80004005): The remote party closed the WebSocket connection without completing the close handshake.\n\n \\---> System.IO.IOException: Unable to read data from the transport connection: Une connexion existante a d\u00fb \u00eatre ferm\u00e9e par l'h\u00f4te distant..\n\n \\---> System.Net.Sockets.SocketException (10054): Une connexion existante a d\u00fb \u00eatre ferm\u00e9e par l'h\u00f4te distant.",
            "https://preview.redd.it/2rb4zrc6n66d1.jpeg?width=1024&format=pjpg&auto=webp&s=54a029be2992a109ae62be6c826fb0db777e6cd3\n\nIt took \\~1300 seconds to generate \"cat\". SD always makes the cat legs so short ;\\_\\_;",
            "Niiice! I'm using comfy UI here, but with SDXL I had it at 30 steps with CFG 7, sampler was dpmpp2m with Karras scheduler.\n\nFor SD 3.0 I dropped the steps to 28, reduced the CFG to 5 as instructed, but I had to change the scheduler to Normal, with Karras it came out as a mess.\n\nHere is my attempt:\n\nhttps://preview.redd.it/qdnohjvk176d1.png?width=1024&format=png&auto=webp&s=7153db81507ddf4aa82a212f3a8b8036452d251e",
            "Good day to get a new workstation!",
            "I've got my own 4090.  I'd just like to get a trivial python pipeline to load and generate an image.  I'm surprised the diffusers folks weren't ready to go on this.  But there sd3 branch is getting very recent activity so I hope this is soon.",
            "Does Comfy/Swarm offer local connection support through LAN?",
            "will impainting work with this?",
            "How do I set the usage of samplers to none in comfyui? I can change scheduler to normal from Karras but I can't set sampler to none.",
            "I tried to set it up with AMD 7900 XTX. I had to turn off enable preview on backend because I was getting an error. When I try to use this model the resulting image is the same multi-colored dot image. Other models work correctly. Not sure what I'm doing wrong.",
            "I installed StableSwarm UI, downloaded the [sd3\\_medium\\_incl\\_clips\\_t5xxlfp8.safetensors](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/sd3_medium_incl_clips_t5xxlfp8.safetensors)  \nfrom huggingface, put it into the models folder, selected SD3 in StableSwarm, set the text encoders to Clip+T5, hit generate.... and then it starts downloading text encoders, which is totally redundant because I gave it the model with all the text encoders included. So now I'm waiting since 20 minutes for it to download something I already downloaded, which is really annoying...",
            "I get this \"Invalid operation: All available backends failed to load the model.\"\n\nEdit, worked now that I opened my firewall....",
            "A111?",
            "I'm getting these really low detail \"paintings\" rather than the prompt I asked for, yet I'm seeing no errors on the CMD.",
            "Can we use the basic model in FOOOCUS?",
            "hi /u/okaris can u please write SD3 AYS + Pag? i found your timesteps is good https://github.com/huggingface/diffusers/issues/7651",
            "how to prevent the default web browser from automatically launching when I launch stable swarm ui?",
            "how to make the output image file name count up? so first image should be 1.png, second 2.png, third 3.png and so on",
            "my laptop has tiny amd gpu, is there any way to bypass and just use cpu ram? I have over 40gb ram available",
            "I know that you can now do very long Prompts but does SD3 have a recommended prompt length/limit?",
            "I'm not writing in an angry way, but can someone please explain why with SD1.5 and SDXL models (Although Turbo, Lightning and Hyper models have issues too) you can use a large variety of Samplers and Schedulers but with SD3 you can't and limited what is the reason behind this or is it a bug in the model?",
            "why is it downloading **clip\\_g\\_sdxl\\_base.safetensors** ? how is id different from **clip\\_g.safetensors** that comfyUi uses?",
            "There's a 6000 image generation limit on SD3 and some crazy TOS that will cause all kinds of problems for creators. Might be a good idea to pass on this one. If CivitAI banned it, it's probably for a good reason.",
            "Just found out Shakker AI lets you upload and download SD3 models. Check it out",
            "Hopefully I will make a tutorial for swarm ui today \u2764\ufe0f",
            "Why do any tools need update to run this? Isn't it just a model that we should be able to just use like any other model by dropping it in our model folder?",
            "UHOHHH",
            "So..Will it be available in civit ai?",
            "I've had a play but I think Bing co-pilot images are a lot better, more vivid, more creative, yet more closely matching the prompts. Is this a limitation of working with a relatively small local model vs. what Wall-E offers?",
            "The question isn't, at this point in time, how, but why.",
            "I'm currently getting this error, is anyone else?\n\n`ComfyUI execution error: cuDNN Frontend error: s_kv not a multiple of 64 with dropout enabled is not supported with cudnn version below 9.0.0`\n\nhttps://preview.redd.it/gjel1ysfs56d1.png?width=2697&format=png&auto=webp&s=8faf20bbaaf0fbedf01c3c9cc327dd595eca9130",
            "its insanely annoying it is downloading clips models for sdxl... just saying it isnt as easy as you say",
            "[deleted]",
            "Guys, this is not local. Try turning off your internet connection and see what happens, every one of your requests is being sent to someone's cloud. OP wanted the karma I guess, misleading title.",
            "Gentleman of culture",
            "The real question is does it support human anatomy yet. The answer is no.",
            "Hold up, they gotta get the amount of legs correct first, which is typically 4.",
            "Model is very censored so no horse cock for you",
            "&#x200B;\n\nhttps://i.redd.it/o9mcah7n4d6d1.gif",
            "I am not sure about this, but I know that Shakker AI fully supports SD3 models, which I am very happy about. I also used it to generate high-quality graduation photos.",
            "**Answer:**\n\nOn the HuggingFace site, download the L and G safetensor from the text encoder folder\n\nPut them in the clip folder\n\nIn Comfy, use the DualClipEncoder instead \n\n.\n\nAnd yeah, the model is pretty censored from some quick testing",
            "Can you share the link to the workflow?",
            "If you follow the instructions in the post, swarm will autodownload valid tencs for you",
            "Which t5 do you use ? fp16 or fp8 ?",
            "https://preview.redd.it/jryhix5ve56d1.png?width=2048&format=png&auto=webp&s=7694db60c24b28dbc04bb427ef2b65fbfa011223\n\nprompt:\u00a0a dog and a cat on top of a red box, The box has 'SD3' written on it.,model:\u00a0OfficialStableDiffusion/sd3\\_medium,seed:\u00a02119103094,steps:\u00a028,cfgscale:\u00a05,aspectratio:\u00a0Custom,width:\u00a02048,height:\u00a02048,swarm\\_version:\u00a0[0.6.4.0](http://0.6.4.0),date:\u00a02024-06-12,generation\\_time:\u00a00.00 (prep) and 136.88 (gen) seconds\n\n  \nwhat the heck?",
            "The other two have textencs included. This is potentially useful for finetuners if they want to train the tencs and distribute them. It's not needed for regular inference of the base model, the separate tencs are a lot more convenient.",
            "The bigger files have TEs included, but smaller versions or a subset.",
            "can we download the models? and where can I get them?",
            "Did you find any difference in quality between them? I'm still downloading. Damn third world country internet speed :(",
            "https://preview.redd.it/zyu1uymsk56d1.png?width=1024&format=png&auto=webp&s=0320aecfcba8b3d4f65ebee3f8b4dd1c32854591\n\nI managed to try it. So far I love the quality. The eyes looks very detailed, something that most of models struggle to do at 1024. I can't wait to train it. I have an amazing dataset waiting for this",
            "Do you have a workflow?",
            "https://preview.redd.it/xawpxke1f56d1.png?width=1024&format=png&auto=webp&s=8376a0c0e7705b6363bf5dca44d3fef453b9a7bd\n\nVery good with 'cat' s",
            "Eh? How'd that happen? Did you have a custom workflow or some unusual settings or something? Are you sure everything's updated?",
            "Did you do a proper default install, or did you customize things (eg alternate comfy backend, change the model dir, etc)?\n\nAlso when in doubt when stuff breaks just restart and see if it fixes itself",
            "Same error in comfyui. I guess low\\_vram devices are not suited to run atm. We will have to wait for updates.\n\nEdit: its fixed already.",
            "Same",
            "Does it take a lot of time to run? \n\nI have a 4070 super and it takes 18 minutes to generate an image",
            "Might work - give it a try. Definitely don't try T5, but the rest might work",
            "Update your comfy install",
            "download the [sd3\\_medium\\_incl\\_clips.safetensors](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/sd3_medium_incl_clips.safetensors) one. or download the clip files separately",
            "update",
            "Go to ComfyUI folder, type cmd in the adress bar. It will open the Windows console, then type git pull and press enter. It will update ComfyUI folder with the latest changes. Now open ComfyUI again, it will get what it needs and it should run ok. They have native support so you don't have to download any extra nodes",
            "How much time does it take you to generate an image?",
            "How is it compared to forge? Does it come with txt2img and img2img or do I have to build a workflow for the latter? Basically ..Can you generate on txt2img and then refine in img2img like you can in forge?",
            "Hopefully it let developers focus in more regular needed checkpoints and tools before all that crazy weird Pony stuff. (I welcome the downvotes from the incels)",
            "You can enter as much as  you want, but of course the farther in you get the less it'll pay attention. For long prompts you'll want the CLIP+T5 full textenc rather than clip only, as T5 responds better to long prompts",
            "If it helps my RTX 4060 mobile has 8GB of VRAM and creates a picture of 1024x1024 in 22 seconds on mine (using 28 steps with CFG 5 as above). 2-3 Seconds slower than SDXL.\n\nIt uses 4.9GB of VRAM when generating. I'm using ComfyUI though, haven't tried swarm yet.",
            "Yes; see some first checks and observations here: [https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource\\_consumption\\_and\\_performance\\_observations/](https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource_consumption_and_performance_observations/)",
            "when using swarm just do the regular sd3\\_medium and don't worry about the bigger ones, the others are mainly a convenience for comfy users or for model trainers that want to train tencs",
            "uses RAM, it can offload from vram, but systemram it eats up unless you disable t5",
            "In the fp16 version, yes, it tops out at 9,6 GB VRAM (GPU) during the text encoder stage. But with the fp8 text encoder variante you top out at 7,2 GB VRAM. But you will most probably need >16 GB RAM (at least for now); see details here: [https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource\\_consumption\\_and\\_performance\\_observations/](https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource_consumption_and_performance_observations/)",
            "if you use the swarm generate tab it will autodownload the clips for you.\n\nIf you really want to do it manually, you have to create a folder named \\`clip\\` and put models in there.",
            "Oooh that's a new one, haven't seen that before. Your settings look correct at a glance, maybe something's broken in how the mmdit code works on mac?",
            "yes swarm and comfy both support img2img.\n\nIn Swarm just toss your image to the \"Init Image\" parameters, or put it in the center area and click \"Edit Image\"\n\nin comfy you'd use a \"Load Image\" node",
            "Comfy is the core and Swarm is a friendly frontend on top of. There's no reason to use the core raw for most people",
            "AMD support is messy, see thread here [https://github.com/Stability-AI/StableSwarmUI/issues/23](https://github.com/Stability-AI/StableSwarmUI/issues/23)",
            "Works on my RX 6800XT",
            "Any recent nvidia card (30xx or 40xx) is ideal. Older cards oughtta work too as long as it's not a potato.",
            "A good one.",
            "My 4080 OOMed when trying to run the full fat models.  Normalvram flag worked for one generation but it was disgustingly slow.  The all in one file seems to work okay and take about 10gb vram.",
            "Concerning VRAM 8 GB are enough and even the \"largest\" version (fp16 text encoder) works with 10 GB; see details here: [https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource\\_consumption\\_and\\_performance\\_observations/](https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource_consumption_and_performance_observations/)",
            "4090",
            "Are they good? asking for a cousin",
            "There are several in the repo and on comfy's official example page, or you can just use Swarm to autogenerate a workflow for you",
            "It's in the same huggingface repo you downloaded the model from.",
            "Go to Server -> Click Update and Restart, you have an install from before sd3 launch",
            "Yes it should work in comfy itself. I'd recommend doing the first time setup in Swarm to simplify things (and then Comfy is just a tab inside Swarm you can use at will)",
            " if you use the swarm generate tab it will autodownload the clips for you.\n\nIf you really want to do it manually, you have to create a folder named \\`clip\\` under models and put the models in there.",
            "they'll offload to system ram",
            "That's weird, you're the second person to post an error message like this, I'm not sure how that happens. It kinda looks like settings got messed up to mix SD3 and an older model. Did you maybe accidentally select a VAE to use? (you need to have none/automatic for sd3 as it has a unique vae of its own)",
            "you don't need a separate clip loader if you have the big chonky file. You might still want it though to be able to use clip only without t5 sometimes",
            "That's correct, though you only need one of the sd3\\_medium models.\n\nYou also need the textencs in models/clip\n\nIf you use Swarm it will autodownload the textencs for you",
            "Uhhh probably go back and just do a fresh install with the default backend? You're a few too many steps in here and just getting errors from misconfigured backends.\n\nYou might want to join the discord to get more direct help figuring things out",
            "Yes, but it's a lil hacky. See this thread for details [https://github.com/Stability-AI/StableSwarmUI/issues/23](https://github.com/Stability-AI/StableSwarmUI/issues/23)",
            "Works on my RX 6800XT Linux/ROCM install",
            "Not yet, they're working on it",
            "Previous user that had an error like this happened because their computer ran out of RAM - yours ... doesn't sound like that should be the case lol.\n\nCheck Server -> Logs -> Debug, the comfy output should show what went wrong",
            "It creates fantastic pictures of space too:\n\nhttps://preview.redd.it/b7zk76vy176d1.png?width=1024&format=png&auto=webp&s=143d29c85dd0f628fcf0724a9ee3a8c567a6b00e",
            "Yes",
            "Yes, just drag an image to the center area and click \"Edit Image\"",
            "there's not a \"none\" sampler. The default sampler for SD3 is Euler",
            "AMD support is messy, see thread here [https://github.com/Stability-AI/StableSwarmUI/issues/23](https://github.com/Stability-AI/StableSwarmUI/issues/23)",
            "Works on my RX 6800XT",
            "yeah in the next couple days i'll add autodetection for the textenc-included fat files to avoid that",
            "They're working on SD3 support",
            "Eh? Can you post a screenshot of your UI showing the parameters you used and the image it made?",
            "that's an option under Server Configuration, \"LaunchMode\", set it to none",
            "if file names are identical they'll do that, so just edit the file path format in User Settings",
            "Running on CPU is very very slow :( \n\nIf you have above ... 2 gigs? I think, or so, theoretically sysram offloading works. I don't know about AMD specifically though. Nvidia can do it natively",
            "Official recommendation? No.\n\nUnofficially but loose theory based on the tech? 75 clip tokens is the first clip cutoff, but 512 t5 tokens is the t5 cutoff, and the model is quite happy to stack a few clips, so... somewhere in between 75 and 512 words is probably optimal.",
            "SD3 uses Rectified Flow, which is incompatible with stochastic samplers (anything with an \"a\"/\"ancestral\"/\"SDE\")",
            "It's a whole new model architecture!",
            "It's a totally different architecture. \"Model\" can mean anything, it's not like they're some sort of interchangeable lego blocks that you can just drop in. Without an algorithm to actually make use a model, it's just an useless pile of numbers. In this case, even though SD1.5, SDXL and SD3 are all based on the same basic principle, the specifics are different. All SD1.5 models are drop-in replaceable because they're all just finetunes of the same base model. The same goes for SDXL models. But SD1.5 models are not in any way compatible with SDXL models. And SD3 models are in no way compatible with either SDXL or SD1.5. Someone has to first write the code that uses the SD3 model before it can be used.",
            "That is... a whole new type of error message. It sounds kinda like an nvidia driver issue?",
            "the SDXL clip models are used in SD3",
            "Please label your posts properly, don't lie to people",
            "I tested and it's totally local. You made me waste my time.  \nStop saying shit.",
            "You don't understand because you're too stupid. SD3 is so advanced that it makes the next evolution of humans, crabs!\n\nPeople are just incompetent and don't know how to use this tool that was advertised as having good comprehension.\n\nAlso fuck people trying to make finetunes.\n\n-Lykon",
            "Humans have 4 legs, got it. This will be great data for the model!",
            "yet.",
            "sigh. https://i.imgur.com/qX5D9RE.png",
            "https://preview.redd.it/ox1v97ucal6d1.png?width=512&format=png&auto=webp&s=d8e281404a1412458f87ca30d3c1b647c8331078\n\nhorse cock, just for you :)",
            "The horse cock is coming, no doubt.",
            "It's going to go the same route of SD 2 as a result...just give it a time.",
            "Even trying to get a person on a bed is hard in SD3 so i am hoping someone will make a finetuned model so prompts that will result in that will work",
            "Yeah very censored, thank you stability though for the protecting me from the harmful effects of seeing the beautiful human body from a side view naked, that is much more traumatizing and dangerous than seeing stuff like [completely random horrors when prompting everyday things due to lack of pose data](https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Fua36wu94tfwc1.png%3Fwidth%3D1024%26format%3Dpng%26auto%3Dwebp%26s%3Dec06cf42d26027db09bc625d0cf95f2c5f91fdb4&utm_source=reddit&utm_medium=usertext&utm_name=StableDiffusion&utm_content=t1_l6uzx6n) ive already seen much worse tonight and this one isn't even that bad, the face on one of them got me with the arm coming out if it, so not going to bed.\n\nEvidence of stability actively choosing nightmare fuel over everyday poses for us users:\n\n*Models with pre-existing knowledge of related concepts have a more suitable latent space, making it easier for fine-tuning to enhance specific attributes without extensive retraining (Section 5.2.3).\u200b (Stability AI)\u200b\u200b*\n\nhttps://stability.ai/news/stable-diffusion-3-research-paper\n\n(still have to do woman eating a banana test lol) side note.. still thanks for releasing it though.\n\nEdit: lol link is down now as if last couple days, anyone have a mirror? Edit: https://web.archive.org/web/20240524023534/https://stability.ai/news/stable-diffusion-3-research-paper edit: 5 hours later, paper is back on their site, so weird.",
            "The Triple one is for loading the T5 one. But it also works without it. Too lazy to download the 9 GB one\u2026",
            "Yeah on HF you have a folder \"comfy_example_workflows\" : https://huggingface.co/stabilityai/stable-diffusion-3-medium/tree/main/comfy_example_workflows",
            "Sry for the unrelated question. I see that SwarmUI runs with git and dotnet, but without the python libraries. Is that correct? I'm not a fan of installing a lot of things on PC\ud83d\ude05",
            "Alright thanks, I was trying to do it without Swarm but I can try",
            "I was just trying it in StableSwarm.\n\nGood news: It works when I have SD3 TextEncs set to \"Clip Only.\"\n\nBad news: When I have SD3 TextEncs set to \"Clip + T5\" it always fails with the error:\n\n> Invalid operation: ComfyUI execution error:\n> Error while deserializing header: InvalidHeaderDeserialization\n\n(On background, I have 24GB of VRAM on my 3090. I'm using my existing ComfyUI install as the backend. I checked that my ComfyUI is updated to the latest version. The ComfyUI_windows_portable\\ComfyUI\\models\\clip folder has 3 automatically downloaded files now, including the g and the l and the t5xxl_enconly. So I don't know why I can't use it the both ways.)\n\nHere's what it said in the console:\n12:08:06.690 [Info] t5xxl_enconly.safetensors download at 100.0%...\n12:08:06.692 [Info] Downloading complete, continuing.\n12:08:08.839 [Warning] ComfyUI-0 on port 7821 stderr: Traceback (most recent call last):\n12:08:08.840 [Warning] ComfyUI-0 on port 7821 stderr:   File \"C:\\AI\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 151, in recursive_execute\n12:08:08.842 [Warning] ComfyUI-0 on port 7821 stderr:     output_data, output_ui = get_output_data(obj, input_data_all)\n12:08:08.843 [Warning] ComfyUI-0 on port 7821 stderr:   File \"C:\\AI\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 81, in get_output_data\n12:08:08.844 [Warning] ComfyUI-0 on port 7821 stderr:     return_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True)\n12:08:08.845 [Warning] ComfyUI-0 on port 7821 stderr:   File \"C:\\AI\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 74, in map_node_over_list\n12:08:08.845 [Warning] ComfyUI-0 on port 7821 stderr:     results.append(getattr(obj, func)(**slice_dict(input_data_all, i)))\n12:08:08.846 [Warning] ComfyUI-0 on port 7821 stderr:   File \"C:\\AI\\ComfyUI_windows_portable\\ComfyUI\\comfy_extras\\nodes_sd3.py\", line 21, in load_clip\n12:08:08.847 [Warning] ComfyUI-0 on port 7821 stderr:     clip = comfy.sd.load_clip(ckpt_paths=[clip_path1, clip_path2, clip_path3], embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n12:08:08.847 [Warning] ComfyUI-0 on port 7821 stderr:   File \"C:\\AI\\ComfyUI_windows_portable\\ComfyUI\\comfy\\sd.py\", line 378, in load_clip\n12:08:08.848 [Warning] ComfyUI-0 on port 7821 stderr:     clip_data.append(comfy.utils.load_torch_file(p, safe_load=True))\n12:08:08.848 [Warning] ComfyUI-0 on port 7821 stderr:   File \"C:\\AI\\ComfyUI_windows_portable\\ComfyUI\\comfy\\utils.py\", line 14, in load_torch_file\n12:08:08.848 [Warning] ComfyUI-0 on port 7821 stderr:     sd = safetensors.torch.load_file(ckpt, device=device.type)\n12:08:08.849 [Warning] ComfyUI-0 on port 7821 stderr:   File \"C:\\AI\\ComfyUI_windows_portable\\python_embeded\\lib\\site-packages\\safetensors\\torch.py\", line 259, in load_file\n12:08:08.849 [Warning] ComfyUI-0 on port 7821 stderr:     with safe_open(filename, framework=\"pt\", device=device) as f:\n12:08:08.850 [Warning] ComfyUI-0 on port 7821 stderr: safetensors_rust.SafetensorError: Error while deserializing header: InvalidHeaderDeserialization\n12:08:08.850 [Warning] ComfyUI-0 on port 7821 stderr:",
            "From quick testing, the results are quite similar. I think it's fine to stick with `t5xxl_fp8_e4m3fn`.",
            "I'm not sure I'm still downloading one (fp16) but you don't have to use t5 \n\nhttps://preview.redd.it/fao2chplb56d1.png?width=518&format=png&auto=webp&s=9da0e3252fb7b3b5db3b4a0eea8ec2427e28676f",
            "SD3 is not able to generate images directly above 1mp (1024x1024), it will break. If you scroll up, the opening post here explains how to generate 2048 by using 1024 and refiner upscale with tiling",
            "So using the other models will produce same results or better or worse or slower?",
            "how do we use the T5 version? getting \"module 'torch' has no attribute 'float8\\_e4m3fn'\"",
            "here are the difference with same settings and same prompt 'a woman wearing a shirt  with the text \"CENSORED\" written over her chest. Analog photo. raw photo. cinematic lighting. best quality. She is smiling . The background is dark with side lighting focus on her face '- [https://imgur.com/a/UmDshdt](https://imgur.com/a/UmDshdt)",
            "For me if the try words within \"\" the model without texx fails hard. The model with t5xx gives decent results. I don't know how to upload multiple images or otherwise i would show",
            "Well 20 years ago it was predicted the internet would be full of images of people's cats. Now i guess it will be full of AI generated cats.",
            "Not knocking your gen, but it always seems to have a hard time making the fur between the cat's eyes and ears, on any realistic cat I prompt.",
            "fresh installed SwarmUI just hours ago, not sure what happened, I use the checkpoint in my old comfy it works well.",
            "It's a vanilla install from yesterday, didn't change anything. Just did the update today and restarted multiple times.",
            "its fixed! (that was fast, thx devs) [https://github.com/comfyanonymous/ComfyUI/commit/605e64f6d3da44235498bf9103d7aab1c95ef211](https://github.com/comfyanonymous/ComfyUI/commit/605e64f6d3da44235498bf9103d7aab1c95ef211)  \nUpdate comfyui try again",
            "The first time it took 15min, using clip+t5 as OP described. But the following ones took at about 100s at the most. Sometimes it goes in 20 seconds. If i use clip only, it always takes about 20 seconds.\nBut the first time with any settings it always takes me at least 5min to 10min.",
            "Can't we just load the text encoders on RAM and the model itself on GPU? I thought that's what you guys were going for. EDIT: at least for low vram users ofc",
            "I only used SD3 for one evening - I was not impressed. 1024x1024 image about 30-40 second, I will try it again later tonight and report back.",
            "Ok - For the initial load it took 42 seconds at 1024x1024, then I got times of 25 to 28 seconds...I am happy with this 3060 12GB - it can run Cascade without any trouble.",
            "Last thing - if you are thinking about buying a 3060 - you will need 500W+ PSU. I actually ran the card with only 350W for about 2 weeks without any issues. I ended up getting a new 500W and installing it myself, also 32GB is needed to run things smoothly.",
            "All basic features work out of the box without requiring custom comfy workflows (but of course once you want to get beyond the normal things to do with SD, you can go crazy in the noodles)",
            "I just installed it as per this post. I'm not sure I'm seeing img2img in here anywhere but I am new to this UI",
            "I haven't used forge (I was using my own Python code, then auto1111 and then comfyui)\n\nTo do img2img you can't set your image in the I it Image tab and from there you do your masking etc.\n\nIt should be equivalent to other solutions and the fact that you can just import comfyui workflows is a big plus too.",
            "Pony is the key. Currently it's focused on anime porn but it's still helpful, it does so many different poses that other sdxl models can't do. \n\nPony needs to focus on fight scenes and slice of life scenes. Danbooru has 100k+ training data on those.",
            "lol, you talk like someone is holding a gun at their head.",
            "Is SD3 bad for follow the prompts without t5 clip model ?",
            "Sorry for being stupid but how exactly do you do this when using the diffusers library?",
            "for comfyui just dowload sd3 medium inc clips ? or sd3 + 3 clips models ?",
            "No, it did, and it downloaded them to the folder I specified. However it would not read them. I had to manually move them into the comfyui folder (as opposed to the old a111 folder I had decided on) for them to be read.",
            "https://preview.redd.it/npebri3ks56d1.png?width=2927&format=png&auto=webp&s=77d7398633bc0fdbec96e0b9ff351970a2ed4961\n\nConfirmed worked fine in my linux server, both were fresh installs, followed the github steps to install swarm on both (just added the settings for the linux server to start on host 0.0.0.0), ubuntu works fine:",
            "Yeah, I was getting crazy trying comfy to work hahaha",
            "hey remember your CEO was on stage with AMD, imagine what happens if AMD sees your comment :P",
            "Then I definitely have good chances with my 3060 xD",
            "I hope not",
            "Black ones are great and they are very fluffy.",
            "Thanks for the reply. I don't know much coding and only use premade templates in runpod as I don't have a gpu, and there is only comfyui and a1111 in the template. Would love to use Swarm though but don't know how.",
            "Thank you for your quick reply, it works now. Just two additional questions, please:\n- What is the advantage of using CLIP Only over no SD3TextEnc at all?\n- What sampler/scheduler would you recommend? If I specify one explicitly, most of them don't work. I expected that ancestral ones would not work, but I thought that regular DPM2++ Karras would.\n\nCheers!",
            "Thanks for the perfect answer, I'll try that now",
            "Tried it with none vae selected, same problem, same error.",
            "Thank you kind sir.",
            "Not using linux sadly.",
            "Can't wait!!",
            "can't seem to find how to turn it on, is there a config file somewhere i have to edit?",
            "Ahh ok thank you, just plain Euler and not Euler A?",
            "why is it downloading\u00a0**clip\\_g\\_sdxl\\_base.safetensors**\u00a0? how is id different from\u00a0**clip\\_g.safetensors**\u00a0that comfyUi uses?",
            "I was using the wrong samplers. It's fine now.",
            "thank!",
            "Now that is the right type of answer I needed, thanks a lot!",
            "the reason lykon doesn't want other people doing finetunes is because he doesn't know how to make one either. level playing field",
            "That's what she said.",
            "Unlikely.\n\nSD3 is a repeat of SD2, in that they censored SO MUCH that it doesn't understand human anatomy, and the developer of Pony was repeatedly insulted for daring to ask about enterprise licensing to make a finetune, told he needed to speak with Dunning Kruger (the effect that states that peopel overestimate their understanding of a given topic the less they know), and basically laughed off the server.\n\nMeanwhile other models with good prompt comprehension like Hunyuan (basically they took the SD3 paper and made their own 1.5b model before SAI released SD3) and Pixart (different approach, essentially using a small, very high quality dataset to distill a tiny but amazing model in 0.6b parameters) are just getting better and better. The sooner the community rallies around a new, more open model and starts making LoRAs for it, the better.\n\nI have half a mind to make a random shitty NSFW finetune for Pixart Sigma just to get the ball rolling",
            "Sweet thanks.",
            "Did you manage to generate an image using this pipeline? I use those CLIP models from the folder but the output is pure noise.   \nAnd I have one warning   \n\\`\\`\\`  \nno CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\n\nclip missing: \\['text\\_projection.weight'\\]  \n\\`\\`\\`",
            "where are the missing nodes?",
            "python is autodownloaded for the comfy backend and is in a self-contained sub folder instead of a global install",
            "This error indicates the model download failed. Several people have had this for various models, probably caused by HuggingFace servers getting overloaded.\n\nIf it's only with T5, you probably just need to delete \"(Models)/clip/t5xxxl\\_enconly.safetensors\" and restart swarm to let it redownload (or redownload manually if preferred)",
            "I get an InvalidHeaderDeserialization error in comfyui when using `t5xxl_fp8_e4m3fn` and just a black image when using the fp16 on my system (I have a really old-ass graphics card though), using the provided workflow from huggingface, so I am unable to test this. (thought it may have been censored, because I tried to generate a photo of Bear Grylls in a bar, with a medical bottle in his hand with the label \"Urine\", while thinking \"Trying to test SD3, better drink my own....\" \n\nI removed the label, and the reference to the bottle and even the reference to Bear Grylls (brown haired man), still only black photos, so I gave up the whole SD3 experiment, for now.",
            "Why do I have no TYPE attribute for DualCLIPloader?\n\nhttps://preview.redd.it/7joc1bopl56d1.png?width=1361&format=png&auto=webp&s=40575a12a0806512ea88c44cbf5b15459d81537a",
            "can i generate 512 X 512 images ?",
            "identical results, only difference is how long the model takes to load and how much filespace it uses",
            "I can't tell you how to fix that error, but I can tell you I have the exact same error and the workaround is using the SD3 model with no embedded text  encoder and the t5xxl_fp16 CLIP instead of fp8.\n\nI think it's because I don't have exactly the right version of torch, but I'm not sure what is the version to use... and if I find that it will probably break something else.",
            "It is definitely better at text then.",
            "It looks very impressive. I don't know if I missing something in the instructions but I can't find those clips",
            "maybe join the swarm discord  [https://discord.gg/q2y38cqjNw](https://discord.gg/q2y38cqjNw)  for an easier chat to figure this out.\n\nif not, probably need to see your Debug Logs to figure this out",
            "BLESS THE DEVS!",
            "Weird. Always take me at least 10 minutes\n\nThanks",
            "They are loaded and used one after another. So no need to have same in VRAM at the same time. See [https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource\\_consumption\\_and\\_performance\\_observations/](https://www.reddit.com/r/StableDiffusion/comments/1dei7wd/resource_consumption_and_performance_observations/) for details on memory consumption for each stage.",
            "img2img would be the \"Init Image\" param group, or drag an image to the center and click \"Edit Image\"",
            "it can follow the prompt fine without T5, it's just with T5 it's a little bit better. For very long prompts like the above user implied they wanted, T5 is more betterer. CLIP gets messy after \\~75 words in a prompt, T5 can go up to 512 before it gets too messy.",
            "sd3\\_medium and the 2 clip files. If you use swarm, just download sd3\\_medium and it will autodownload clips for you.",
            "Yes if you have an AMD Datacenter tier Mi-350 that can potential perform amazingly. Getting AI to work well on normal home PC cards is still a work in progress at the moment for AMD (but they *are* working on it!)",
            "I got it running with a 3060 12GB - the text thing is great.\n\nhttps://preview.redd.it/rorujtvuh56d1.png?width=1024&format=png&auto=webp&s=566cbc562fdf9b82041ad81b70c0ad44b63021ed",
            "Turns out the crashes were user error. I had reinstalled wsl recently and forgot to give it more system ram.",
            "Lol",
            "Runpod template is in the swarm readme: [https://github.com/Stability-AI/StableSwarmUI?tab=readme-ov-file#runpod](https://github.com/Stability-AI/StableSwarmUI?tab=readme-ov-file#runpod)",
            "CLIP Only uses less RAM and loads/runs faster, vs CLIP+T5 uses a lot more ram and a lot more load time and a touch more run time. If you don't enable the param it will use default (current CLIP Only, in the future it might default based on your system resource availability)\n\nsampler/scheduler: Euler + Normal is default and works fine. You can play with it but note that SD3's rectified flow is more sensitive and will break if you mess with it too much",
            "I'm very confused how you got that error then. Can you post full debug logs? Or join the discord so we can look into it further there",
            "When I have time I can check if it works on Windows. It should however. All I had to do on Linux was to uninstall the CUDA torch that ComfyUI backend installs by default and install stable torch-rocm libraries. Probably would have to do the same on Windows, uninstall CUDA torch if installed and install torch-directml",
            "It's an option presented during install, or you can under Server Config set Host to \\`0.0.0.0\\`  this is also covered in docs here [https://github.com/Stability-AI/StableSwarmUI/blob/master/docs/Advanced%20Usage.md#accessing-stableswarmui-from-other-devices](https://github.com/Stability-AI/StableSwarmUI/blob/master/docs/Advanced%20Usage.md#accessing-stableswarmui-from-other-devices)",
            "correct, definitely not euler ancestral -- ancestral/sde doesn't work on rectified flow models like SD3",
            "It's a filename, it's all the same files. Swarm uses the \"sdxl\\_base\" filename because that was the filenames in the original comfy workflows for SD3, that were changed later.  \nCLIP G as released by laion, as used in sdxl, as used in sd3, is all the exact same weights.\n\nRelated fun fact, because SDXL's text encoders were copied exactly into SD3, any SDXL TI Embeddings or TextEnc LoRAs automatically work in SD3!",
            "Apparently some people have discovered some keywords that actually make the images not look terrible, the ones I\u2019ve seen being \u201cartstation\u201d (because apparently we\u2019ve gone full circle with 1.5 style prompt hocus pocus), as well as some Unicode arrows and stars.\n\nKinda funny since someone mentioned at the point the possibility of SAI adding some \u201cpassword\u201d keyword to bypass censorship. That may have been accurate after all",
            "Every time I see someone mention that they were rude to PonyXL creator I feel annoyed and I don't even know them. It's just that I was finally able to realize my OC thanks to PonyXL. I'm very thankful to the creator and they deserve praise not insults. :/",
            "Yeah it works for me\n\nI'm just using a dual loader instead of the triple :\n\nhttps://preview.redd.it/8thdsx13d56d1.png?width=518&format=png&auto=webp&s=91e032cf21c8caa5fb9d444ef1c7cd33d2619e6b\n\nBut except that I didnt touch anything after loading the SD3 model",
            "Never mind, I updated ComfyUI and there they are...",
            "Thanks:)",
            "I pray that most people at this point *at least* know how to make and maintain virtual environments with different python libraries for different purposes.",
            "I'm not sure. Have you updated comfy?",
            "Technically yes, albeit they will look a bit off vs. if you did 1024",
            "Do you know when we might get finetuning code?",
            "It's working on Comfy, i'll use that for now. Thanks for the support.",
            "Thanks a lot for clearing this up! :)",
            "What's the largest model I can run with RTX 8000 Quadro 48GB VRAM?",
            "ok great. Which of the three models did you use for it?",
            "good, the RTX 3060 12GB is cheap in South America, a good gift for other countries",
            "oh I didn't know about that. Thanks.",
            "Thank you, much appreciated!",
            "Nevermind, I figured it out.",
            "finally trying this out but i can't get it to work for some reason, even though i've done this for other web UIs\n\ni reinstalled and set it to self + LAN, then changed the server config host to 0.0.0.0. I can see in the terminal that it's using 0.0.0.0:port number, and I created a firewall rule to accept inbound connections for that tcp port. However, I can't seem to connect to it on my phone on the same network? i was using that 192.168.... local ip as well. Also tried just \\* for host but didn't work.",
            "How do you find out which sampler is the default for each model? I'm still pretty new to Stable Diffusion.",
            "That\u2019s what upset me the most. On a personal level, what Lykon said to Astraliteheart was unconscionable, ESPECIALLY from a public figure within SAI, and I don\u2019t even know them.\n\nFrom a business level, it\u2019s even dumber than attacking Juggernaut or Dreamshaper when you consider that the reason Pony worked so well is that it was trained so heavily that it overpowered the base material. \n\nWhat that means from a technical perspective is that for a strong finetune, the base model doesn\u2019t even matter very much.\n\nAll SAI has is name recognition and I\u2019m not sure they even have that anymore. I may make a post recapping the history of SAI\u2019s insanity soon because this is just the latest in a loooooong line of anti consumer moves",
            "Switching to DualClipLoader didn't help but I use mac M2, maybe there is a problem here?",
            "Even experienced users tend to mess it up from what I've seen. The most common blunder is not knowing about the \"`-s`\" flag that's required to avoid your virtual env from affecting the global env",
            "What about the quality aspect of the first photo, which was 1024*1024?\n\nIt seems like SD3 gives considerable worse results than SDXL when it comes to basic human anatomy. Is there something wrong with the uploads maybe? I think most people expected SD3 to improve in this area.",
            "Diffusers team will be posting about that soon",
            "aaanything you want lol? that's pretty much unlimited scale",
            "It took about 15 mins to download everything - I am in Taiwan, rather quick internet here. First I got the incl clip file - then I got the clip files - they are all on the site, but you can just snag the medium including the clip and that will work just fine. Need the text clip to get good text - I think. \n\nhttps://preview.redd.it/ke36ctgmk56d1.png?width=920&format=png&auto=webp&s=189c2c305d41a0347883f3a760f121c719cab91e",
            "what was it?",
            "technically Euler is the default for every model. It's just SD3 particularly doesn't like being changed away from that. Some UIs ship with different defaults, but those are just the preferences of the UI author.",
            "I'm also on mac M2 so I don't think so. Have you updated comfy? (\"git pull\" in your comfy folder)",
            "I have the newest version but I needed to update python libs to make it working (from requirements.txt)"
        ]
    },
    "So we had our lawyers review the SD3 license": {
        "title": "So we had our lawyers review the SD3 license",
        "score": 239,
        "url": "https://civitai.com/articles/5840",
        "content": "",
        "num_comments": 103,
        "comments": [
            "So basically forget Taking effort to finetune it, as anyone even downloading a model has to have a licence and also they can order any models removed as per their liking (infringing someone else's copyright).\n\nOhh an SD3 Lora about a celebrity ? REMOVE\n\nOhh another Lora about Pok\u00e9mon ? REMOVE\n\nAnother one for a popular character/anime ? REMOVE",
            "SAI created a product for everyone to use, that noone can use",
            "So a dead model then",
            "Thanks for sharing, it really helps having validation about the interpretation of the license agreement. SD3 doesn't seem worth using from either the perspectives of a user or a business. They've completely bungled the situation.",
            "Rip sd3 wish you didn't commit suicide",
            ">the incestuous nature of the creation community\n\nFunny phrasing.  Yes, models are all constantly mixed with other models, and often nobody keeps track of what they mixed in, so SD3 could end up \"infecting\" large portions of civitai.",
            "We need Ai thepiratebay \u2620\ufe0f",
            "Any chance for a more legible screenshot?",
            "Can I make a fine tune that gives everyone except Stability AI the right to use it?",
            "Civil law is so fascinating yet intimidating at the same time. Thank you for clearing this up for us! I respect your decision to keep it banned until further communication from SAI",
            "I wonder if they made it intentional or their lawers are just stupid. Ether way SAI being idiotic.",
            "Asked chatGPT to do some OCR on the image of the email for better reading (and formatted a bit to make it readable without the red color):\n\nHi Max,\n\nWe reviewed the Creator License Agreement you linked us to from the perspective of your users that might download models based on SD3. A couple of initial thoughts:\n\n1. At a high level, the CLA is not well written, so there\u2019s a fair amount of creative interpretation required to make sense of some of the provisions. Therefore in the event of a dispute there\u2019s a chance that Stability argues something different and ends up being right (since they wrote it).\n2. In particular, the CLA is written with the expectation that a user is developing some sort of broader software or service that incorporates or makes use of Stability's models. It assumes that such user will have customers, and will be commercializing the model, which might not be the case with Civitai\u2019s users.\n3. The CLA defines the term \u201cDerivative Work\u201d to include fine-tuned models based on the Stability core model licensed under the CLA, as well as any other model that\u2019s trained using outputs generated by a Stability core model or a fine-tuned model based on a Stability core model (which we\u2019ll call a Stability-derived model for ease of reference). This is the case even if that other model was not based on a Stability core model or Stability-derived model.\n\nResponses to your specific questions below in red.\n\nLet us know if you\u2019d like to have a follow-up discussion.\n\n2(c) - Restrictions Based on Your or Your Customer\u2019s Service\n\nDoes this mean users who download the SD3 model or any derivative works (models fine-tuned on SD3) will need a membership agreement with Stability AI? \n>Yes, that\u2019s right. Membership Agreement is not defined, and I think this is a holdover from their prior membership program. This is likely saying that if a user downloads a Stability core model or a Derivative Work (see above for what that likely covers) then they would need to obtain a license from Stability AI. Given the way this is written, I would view making a copy of the Stability core model or Derivative Work for use within Civitai\u2019s platform as a \u201cdownload\u201d even though in that instance the user never actually takes possession of the software, since the user now has control over that instance of the Stability core model or Derivative Work, not the uploading user.\n\n2(f) - Removal\n\nThis seems vague to me. Is this explicitly about the removal of these models in the event they infringe on Stability\u2019s rights or just anyone\u2019s? \n>The latter. Stability is concerned about incurring liability for distributing infringing content. If Stability notifies the user that the user\u2019s continued use of the core model or Derivative Works would infringe a third party\u2019s rights, then the user has to stop use of those core models and Derivative Works (which would include removing the models from Civitai\u2019s platform).\n\n3 - Intellectual Property Rights\n\nThis whole section isn\u2019t entirely clear. Does stability own the derivative work (models fine tuned on SD3) or not? \n>The license is silent on who owns the Derivative Works. Under US law, the default would be that the creator would own the Derivative Works subject to Stability\u2019s rights in the original core model. Practically, this means that the owner or user of the Derivative Works needs sufficient rights from Stability in the underlying original core model that the Derivative Work is based on.\n\n3(b) - Your Ownership\n\n\u201cOther than Stability\u2019s rights to access and use Your Service as set forth in this Agreement, no other license or grant of access to Your Service or intellectual property therein is provided to Stability.\u201d Where does it outline Stabilties right to access and use our service? \n>By \u201cYour Service\u201d, the license agreement is referring to the software product or service that the user (who is the licensee under this agreement) makes commercially available that utilizes Stability\u2019s core model, not Civitai\u2019s platform. We also did not see any specific reference to Stability being granted any right to access or use \u201cYour Service\u201d. This seems to be a holdover from a prior or different version of this license.\n\nThese are the main standouts, but in general the question we\u2019d like answered is:\nWhat rights does stability have over models fine tuned on SD3? \n>Stability does not have any rights in the Derivative Works, however the users do have obligations with respect to such Derivative Works. The most notable of which is (a) that any users who download the Derivative Work must also obtain a license from Stability, (b) the requirement to cease use of the Derivative Works: (i) upon termination or expiration of the CLA (which includes requiring any downstream recipients to cease use), or (ii) where Stability notifies the user that continued use infringes a third party\u2019s IP rights. \u200b\n\n\u200b",
            "I hope SAI crashes and burn at this point.",
            "Does this mean Stable Cascade is banned as well? (People say it's the same license)",
            "English TLDR:\nSD3's license is problematic because it seemingly allows Stability AI to edit or takedown not just models trained on SD3, but also any resources using SD3 outputs in their datasets, potentially affecting a wide range of AI models beyond direct SD3 derivatives.",
            "Reading that redlined email is giving me PTSD. So many back and forths with lawyers that look exactly like that, but damn did I always appreciate their insight.",
            "If StabilityAI doesn't want to end itself, they have to update licence and make it much more clear and better!",
            "Crazy how easy they can turn their back on millions of users community days before announcing their new celebrity CEO who will get them at most 2 or 3 corporations to pay.",
            "Wait so does all this mean that people posting images from sd3 on Reddit have poisoned Reddit as a source of training data?",
            "SDXL currently prevails, and 1.5 is still really popular. SD3 will remain dead. Basically, great tech is lost because of greed. Stability is becoming the next OpenAI.\n\nWhen PixArt and Hunyan-DiT become more accessible and stable on common UI platforms, it might be the beginning of a completely new era where Stability is on the same side with OpenAI, while we get great free and less-censored open-source models.\n\nThis whole situation is a sign for us to either move to a new sub or repurpose this one and rewrite the rules in the near future so this sub becomes a general open-source image generation sub.",
            "This should be a massive wakeup call to SAI. Should.....",
            "It\u2019s dead; **our fears were warranted** ;",
            "Lawyers have 'skill issue'. - Lykon maybe.\u00a0",
            "What a mess...",
            "While I as a community member understand perfectly where you come from, I am of the opinion that the last monicker this community needs is **incestuous**.",
            "Is anyone else getting SD3 ads from chinese websites? I got them all week\n\nThe content is no good and i wont name them but it seems like where this is headed",
            "Pirate fine-tuning when?",
            "The model wants to be loved and used, to bring happiness and to serve its purpose. Too bad the creators treated it this way.",
            "The use of \u201eincestuous\u201c in the blog post is unfortunate, given this term\u2018s inherent negative connotation. \u201eStrongly interrelated\u201c for example would have been a better choice for this context.",
            "So the issue with an actually somewhat highlighted here. \n\n\n\nIf f we treat AI as a 8 year old  not a teenage because they dont have all the senses they only have latent memory.  \n\nThis means if you show it 10 movies of Disney telling them they are all Disney and then you take away the movies and ask them to draw a Disney style picture.  It\u2019s the same issue as 4 chord songs. \n\nIs this creativity or copying?    In an individual human we call them influences or a genre. \n\nNow capitalism will tell you copying is bad because money but the problem we have is that creativity is iteration on our experiences so the only reason copyright exisits is to make people money for creativity. \n\nThe fact we have machines that do it meant the current copyright system doesn\u2019t work.   Right now licensing laws and copyright is not a factor because whoever gets super intelligence first will be leaps ahead of the next. \n\nReality is the rules  don\u2019t work and whatever license anything has will be irrelevant as money > humans and ethics. \n\nAnthropic are building minority report.  OpenAI is building Hal.     Which leads to super intelligent first will decide our ethical stance.   So I\u2019n 5 years if we keep pace none of this will be relevant. \n\nRight now every model is a 3 week thing and if you merge shit together it doesn\u2019t even have a trail so no one can prove anything anymore",
            "Unbanned SD3 medium and the responsibility for the use that will be made lies with each individual",
            "\"...the incestuous nature of the creation community...\" holy hell what a hot take.",
            "Training a model on everyone on art station without consent = ok\n\nTraining a Lora on fictional Pokemon = not ok\n\nGotcha",
            "\"Membership Agreement\" in that context almost certainly is intended to include the noncommercial research license, which doesn't have any provisions about deleting the model except for if you violate the license by going against the AUP (which itself is a very standard provision that you'll see in lots of model releases, Gemma's license for instance).  Very odd that they have this requirement for the commercial license but not the noncommercial one.",
            "r/fucknintendo",
            "You miss the issue.     \u201cProve it\u201d.  That\u2019s a 10 year court case and it\u2019s not going to be small companies making those battles.  OpenAI has heaps of legal issues atm as well as its image re security",
            "In other words: SD3 and SAI dead.\nGot it",
            "i am pretty sure that is only about their products which they have given to testers like sdxl 0.9 etc,, if you leak or dont delete test models like that they are free to take legal actions to you, that doesnt go for models publically released like sd3 medium....",
            "Meanwhile:\n\n# OpenAI inks deal to train AI on Reddit data.",
            "Not really, people will just spread them across discord and torrent sites.",
            "AND THEY CAN'T EVEN EXPLAIN IT THEMSELVES\n\n>We've had conversations with representatives, but no one that can actually give us any more insight into this license or SD3.\n\nI imagine after so many people quit SAI, the ones remaining just keep the lights on and have no clue how anything works",
            "And provided another case for not trying to write your own license when you (try to) open source something, because it usually ends up being a mess.",
            "If they had released the 8b model, people would have thought about paying for the license.\n\nThey provide a good model, people fine tune it and monetize that and SAI gets a piece of the cake. Seems fair..\n\n\nInstead they want us to pay license fees for this botched mess ..?",
            "How? You can't merge models that have different architectures.\n\nEDIT: I see, SAI is claiming power over everything trained with SD3 outputs",
            "someone posted this here the other day, https://aitracker.art/",
            "https://preview.redd.it/ib6pbls9r28d1.png?width=1240&format=png&auto=webp&s=c8e190a921ddfe2a1cbfb6e3e940b5ab83e795c1",
            "From the sounds of it yes, but it also sounds like if they didn't like that they couldn't use it, they could have it taken down.\n\nWorse, if anyone used your fine tune and made their own, that would also have to be taken down in the case yours is.",
            "if you make your finetune public it will be free from their license... ofcourse you cant make it stability cant use it cus they are the owner of that architecture of model and the base model....",
            "According to lykon it's our fault and we deserve it",
            "I'm on the \"hate train\" too but SD 1.5 opened a door for me (and I'm sure lots of other people) that wouldn't otherwise be possible, so I'll be forever grateful for that.",
            "It is the same license.",
            "Cascade has a different license and is non-commercial research only. Not sure how CivitAI deals with that.",
            "There's no way to tell whether model was trained on SD3 pics or not",
            "pump & dump incoming",
            "In Stability dreams maybe. They would never win a case in court based on that restriction in the license.",
            "Not even because of the greed - they will not make money this way. Just because of stupidity and total mess in their company. Maybe because of hypocrisy too, idk.",
            "CivitAI is the ONLY similar site that has these concerns. They have them because they presumably cannot afford an Enterprise license for the company, and no other reason.",
            "I mean, it isn't wrong. We have like four major stable diffusion models and most other models are not original training but mixes of other models in the same family. The fact some of the models are becoming more and better than that seems really impressive, but doesn't undermine the use of the term to me",
            "A lot of people train on eachothers outputs, or merge and fine tune. its accurate, and not meant to be derogatory",
            "Did you read that as if CivitAI knew your parents were cousins? That's not what they meant... but it is probably true.",
            "This just means that models incorporate other models created by the community via merges etc.\n\nNot actual incest.\n\nThis is a fairly common use of the term, albeit not the most common one (which you seem to be familiar with)",
            "Welcome to the strange world of intellectual property law, where selling someone a picture of a specific yellow rat can get you sued, but selling someone a picture that is \"in the style of\" an artist can not get you sued.",
            "Yep many influences vs specific copyright targeting.  Not a fix for the law but at least they realise it\u2019s an issue under capitalistic copyright.   It\u2019s the 4 chord song issue",
            "I mean even if we are not releasing it for profit/commercial purpose, CivitAI has on-site generation service as well, which can automatically put all the Models on there that allow on-site generation under commercial use for which CivitAI will have to get the license and thus also will have to remove any models asked to be removed.",
            ">Membership Agreement\" in that context almost certainly is intended to include the noncommercial research license  \n  \nCivitAI didn't want anything other than exactly the answer they \"got\" from their lawyers here, this isn't about anything other than THIER bottom line and the bottom line of corporate entities they associate with like RunDiffusion.\n\nDon't expect them to interpret anything logically here.",
            "It may be intended but the point is that it's not clearly written out in the license nor has clarification been received from SAI. In those circumstances, it's prudent to assume the least favorable reading of the license.",
            "Do you want to pay legal fees in a 10 year case?",
            "We knew that ages ago when they stopped paying their GPU debts\u00a0",
            "isnt this ridiculous? that the License the thing that should be easily understandable has so many contradictions? its been a week and people still don't get it how it suppose to work.",
            "that is irrelevant here.\n\nIf that was the case, no one would care for copyright, or licenses in general.\n\nthe problem is community has to disperse from one place to tens of different places which eventually leads to a communities death in long run.\n\nJust take Piracy for example, it immensely slowed down once major trackers were taken down.\n\nit is coming back, but all it takes is another blow to slow down.\n\nThis is will happening, and considering community does it without any expectation of money and out of their own cost and time, it just discourages to make further development, as opposed to piracy trackers, who distribute content to make money.\n\nedit: For example, take Yuzu (Switch Emulator).\n\nIt was an OpenSource project, taken down by nintendo, people did make forks or it and Suyu is still being developed, but they have been banned from github as well as discord.\n\nso the development, updates are so slow that it is equivalent to a dead project.\n\nall it needs is another hit, and it will be gone without anyone ever further developing it.\n\nNow you sure can download the last update and use it and distribute it as well, but further development will be ABONDONED.",
            "Can\u2019t wait to see the documentary on what a shitshow this whole company was\u00a0",
            "It's really easy to explain. \n\nThey want the reach, popularity and following of an open source product but like to monetize it like Adobe in a crazed fever night.",
            "The license is unacceptable even for the 8b\u2026 we might just have accepted it and only years later wish we hadn\u2019t.",
            "the problem is not even about paying, but about control they would have over all models and loras.",
            "appreciate it, thanks",
            "...I wish I'd scrolled down before squinting my way through the original screenshot.",
            ">Worse, if anyone used your fine tune and made their own, that would also have to be taken down in the case yours is.\n\nJust to add one small thing: *you are responsible for making sure derivatives of your model get taken down if yours is*, and they can sue you if you don't. You are liable for what happens downstream,  according to the license (while stability themselves remain not liable, of course).\n\nThis is probably my greatest concern on finetuning this model, and why I would actually recommend no one touches it.",
            "I thought their license meant that they also own fine-tunes, right?\n\nCan I make money off my fine-tune? I would just need to pay then monthly for use of the base model? Because that would be fair IMO.",
            "Luckily he\u2019ll be unemployed soon anyway lol",
            "Be thankful to Runway then. Not SAI.",
            "Guys please stop giving them credit for 1.5, Runaway are the ones that leaked it, SAI wanted to nerf it by censoring it \nSAI has never been on our side and the nice models we have today are all fine-tuned versions made by talented people. In other words the community carried SAI, they gave us broken ass tools and we managed to repair them only because their license wasn't as idiotic as the one SD3 has.",
            "No, Cascade has exactly the same license. The \"Creator License\" is NOT inherently \"the SD3 licene\", despite white /u/Civitai keeps disingenuously claiming.  \n  \n[https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/LICENSE](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/LICENSE)\n[https://huggingface.co/stabilityai/stable-cascade/blob/main/LICENSE](https://huggingface.co/stabilityai/stable-cascade/blob/main/LICENSE)  \n  \nCivitAI doesn't care about Joe Enduser in the slightest, that douchebag \"JustMaier\" who wrte their original ban article even CLEARLY doesn't care about the perspective of people who aren't literally using SAI models directly to operate their own specific for-profit SaaS operations.  \n  \nIt's just easier for him to scare-monger things into people believing \"LITERALLY EVERYBOY NEEDS A CREATOR LLICENSE\" despite that being completely untrue.",
            "Sometimes I feel like intellectual property laws only exist so rich companies can keep milking their IPs while everyone is just banned from creating a cheaper alternative...",
            "Sorry to be that guy but all it takes to get sued is for someone to sue you. You don't even have to violate any laws or contracts. The best defense against being sued (besides not deserving it) is to be 'judgement proof', i.e. be broke as shit.",
            "That's not weird. One would be a trademarked character and the other can not even be copyrighted for obvious reasons.",
            "TensorArt had absolutely no issue making SD3 available for use in their free online generator on day one. This is about CivitAI's profit margins and nothing else.",
            "No but I don\u2019t think there will be any real way to prove where ai art came from",
            "Piracy never slowed down, not in the slightest. You've been looking in the wrong direction.",
            "I didnt mean trainers, just people who download it will redistribute it, and just like piracy, yts 1337x fitgirl can get u nearly anything popular, and there are many more still but those are the prominent",
            "I fear it will be a short, disappointing story of greed.",
            "But isn't the whole reason stable diffusion being locally run that emad wanted it to be open source and copyright free, and that's mostly the reason he bankrupted SAI? When I'm talking about 1.5 I'm also referring to all the process that took place in order to reach that point. If Emad didn't have a vision, there would be no 1.5.\n\n\nOf course there are so many things that contributed to perfecting and distributing this technology, from the developers and a massive thanks to the fine tunners as well as civitai.\u00a0",
            "Tbh, 1.5 was just minor upgrade from 1.4, which was leaked on 4chan, iirc",
            "Yes, that is the exact reason.\n\nRegardless of how it started, the system has been entirely taken over by corporate rights holders and is so convoluted and full of so many vague IPs that you can be sued out of existence if you were to every try to compete.",
            "That happens but anybody can make an IP and profit from it.",
            "It's the opposite. If there were no protections and you made something people liked, big companies would just steal the idea and demolish you. No one new could ever succeed. We've got other things that make big corporations a problem, but this isn't one of them.",
            "Yeah, i'm not even american but i hate when people say \"IN AMERICA YOU CAN GET SUED FOR EVERYTHING\".\n\nLike yeah. That's the whole point of suing someone. The fact that anyone can sue everyone, and then a judge (or jury) will decide who is in the right.",
            "I mean ofc it is, but still\n\nIt is a risk up until they come knocking (if they decide to).\n\nDistributing Switch Emulator openly (Yuzu) was also good up until they came for them.\n\nMAS (Windows activator) is literally distributed on github (Microsoft's own website). Again it is well and good up until MS decides it no more wants to allow that happening.\n\nSAI is currently in a mess, so going after these services is probably least of their priorities right now, but knowing they have CEOs and people from other corporations now to \"GET THEIR MONEY RIGHT\", eventually they can start knocking on doors of these services.",
            "ragebait about litigation culture is printed by media companies who get sued a lot.",
            "My point was that TensorArt probably has an Enterprise License, something CivitAI simply can't or won't pay for.",
            "No they were just to stupid or didn't care about the license genius.",
            "Do you even understand the situation at all? Doesn't sound like it."
        ]
    },
    "Upgraded Depth Anything V2": {
        "title": "Upgraded Depth Anything V2",
        "score": 85,
        "url": "https://www.reddit.com/gallery/1dlriq4",
        "content": "",
        "num_comments": 8,
        "comments": [
            "I've upgraded the repo, added in more capabilities, converted the cmd .py scripts to function more intuitively, added the ability to pick between 147 different depth output colour map methods, introduced batch image as well as video processing, plus now everything that is processed is automatically saved to an outputs folder (w/ file-naming conventions to help you stay organized) & I've converted the .pth models to .safetensors. Here is the repo link - [https://github.com/MackinationsAi/Upgraded-Depth-Anything-V2](https://github.com/MackinationsAi/Upgraded-Depth-Anything-V2)",
            "Looks great thanks! Now if only deforum would add it to their depth models in auto1111",
            "Awesome!! Gonna wait for a youtube install guide haha   \nLooks sick dude!",
            "Glad the removed the non commercial license , but they need to add a clear   license  on huggingface",
            "this is nice is it available in comfy?",
            "I was planning on posting an install & usage walkthrough to YouTube tomorrow night. I\u2019ll post the link here once it\u2019s up \ud83e\udee0.",
            "awesome!!",
            "Please mention me with link\ud83d\ude4f"
        ]
    },
    "SwarmUI is now independent from Stability": {
        "title": "SwarmUI is now independent from Stability",
        "score": 355,
        "url": "https://www.reddit.com/r/StableDiffusion/comments/1dlivre/swarmui_is_now_independent_from_stability/",
        "content": "https://preview.redd.it/ygdi1oytn08d1.jpg?width=1645&format=pjpg&auto=webp&s=39270826d4bda7d92f9bf2372d61c7c62d5d1fa0\n\nSwarmUI (formerly StableSwarmUI) is now 100% independent from Stability AI.\n\nKey info to know:\n\n- Find the new repo here: [https://github.com/mcmonkeyprojects/SwarmUI](https://github.com/mcmonkeyprojects/SwarmUI)\n\n- Migration guide for existing users here: [https://github.com/mcmonkeyprojects/SwarmUI/discussions/2](https://github.com/mcmonkeyprojects/SwarmUI/discussions/2)\n\n- Guide for new installs here [https://github.com/mcmonkeyprojects/SwarmUI#installing-on-windows](https://github.com/mcmonkeyprojects/SwarmUI#installing-on-windows)\n\n- I was the sole developer of StableSwarmUI at Stability, and I am the owner of the new independent repo, it is in the exact same hands as before, and I will still be working just as hard on it\n\n- We held a public poll on the Discord ( [https://discord.gg/q2y38cqjNw](https://discord.gg/q2y38cqjNw) ) for what to rename the project to and \"SwarmUI\" won with 54% of the votes.\n\n- Swarm will be 100% free and open source forever\n\n- The project has the support of the recently announced Comfy Org ( [https://www.reddit.com/r/StableDiffusion/comments/1diutad](https://www.reddit.com/r/StableDiffusion/comments/1diutad) )\n\n- We'll work with whoever's making the best models wherever they are to add support. Recently experimental pixart sigma support was added [https://github.com/mcmonkeyprojects/SwarmUI/blob/master/docs/Model%20Support.md#pixart-sigma](https://github.com/mcmonkeyprojects/SwarmUI/blob/master/docs/Model%20Support.md#pixart-sigma) ",
        "num_comments": 75,
        "comments": [
            "Did you just get the axe too? It is a Friday...",
            "How can we be sure this is the real mcmonkey if there are no fennec girls",
            "Looks great, if it makes using other models like sigma etc easy then ill defienlty move to swarmUI.\n\nedit - also when i try and run SwarmUI it keeps trying to connect to the internet, why? is it locally based or runs online only? If it is online only then it is of no use to me.",
            "Great to see this, and I'm glad to have a much cleaner acronym for it now, too.",
            "Congratulations! I hope that your project gets bigger and better and supports more models and more architectures as time goes on. With my current disappointment from Stability and their recent decisions, I hope that more and more competition comes into the scene and we get the same kind of thing as we got with large language models but in image models, and at the end of it all people will have their choice of which image models to use. Good luck to you, and I guess that's all.",
            "I have a question! I installed swarm a while ago and have been enjoying it a lot lately, what\u2019s a good reason to reinstall swarm now that it\u2019s independent?",
            "> We held a public poll on the Discord for what to rename the project to and \"SwarmUI\" won with 54% of the votes.\n\nI imagine the people who were dedicated enough to be part of the immediate community didn't see the need for a name change because they were already more than familiar with it, but I think that name will keep limiting future growth and mass adoption just like it has been doing before. Well, it is what it is.",
            "Is there a decent tutorial on swarm ai never used it before",
            "https://i.redd.it/3w97jm34p08d1.gif",
            "congrats!!",
            "So I gave it a try swapping over from Forge UI on my rtx2060 6gb.\n\nThe only improvement I noticed was that my SDXL generation started quicker (instantly vs 2-3 seconds), but my generations are significantly slower in swarm compared to forge. A 1024x1024 image takes me a full minute to generate with swarm, vs 20-36 seconds in Forge UI.\n\nInpainting also didn't seem intuitive.\n\nI'll keep trying new versions but I'm still let down by all UI's other than Forge for performance.",
            "is there any plans to supoort hunyuan-DiT? \n\nI predict that Tencent's overwhelming power will continue to provide generous support for community\ud83e\udd14",
            "I'd like to install via Stability Matrix. However, it is listed as StableSwarmUI owned by Stablity-Ai.\n\nhttps://preview.redd.it/il6g6wihv28d1.png?width=1087&format=png&auto=webp&s=629d42df3836d573ddacf7cad158053af51e5ff0\n\nSo, I should wait a wee while, for Stability Matrix to catch up?",
            "HOLY SHIT",
            "Hmm, interesting... how is this compared to Forge speed wise?",
            "![gif](giphy|6iIYgInJ3jLva)",
            "Can I use my AMD GPU under Windows without issues??",
            "Do I need to reinstall, or I can I update my install so it points to the new repo ?",
            "How easy for me to move from comfy to it? I know it uses comfy as back end, and when I saw review about it a couple of months ago it also had node workflow , but can it support everything comfy support?",
            "too... many... ui's... all... so... interesting...",
            "Crazy! But inevitable. I'll have to give it a try over comfy now",
            "Seems to be a perfect time to give this a shot. Never tried Swarm before.\n\nEDIT: Super easy to install and a pretty good UI. I really like it so far! Good job.\n\nEDIT 2: I have some feedback to give, where would I do that so it reaches the right people.",
            "Hello, \n\nas this is based on dotnet, may I ask something about dotnet? Being on \\*nix and not windows I am not used to dotnet. dotnet is Microsoft, right - does this mean it requires to prevent dotnet itself to \"telephone home\" to MS? I was used to block a lot of win services while being on win with an application FW. Is this required here as well for dotnet?\n\n  \nThanks for advice.",
            "do you have anything similar to a1111-sd-webui-tagcomplete?",
            "i couldnt use swarm when it was released because i couldnt share my 200 models folder with it. Tried again on independent repo....welp at least i can see my models in ui now. Still doesnt work xD   \n\n\nBackend request #1 failed: System.InvalidOperationException: No backends match the settings of the request given! Backends refused for the following reason(s):\n\n\\- Request requires model 'testmix2.safetensors' but the backend does not have that model",
            "Finally!!!",
            "I was wondering what would happen now with Stability, A111 did not want to implement some code to speed things up a while back that was in ComftyUI (forge did it instead as spin off) IIR I am guessing this might be why (thinking ahead). Then I came across this project that used ComfyUI in the background. Then SD3 happened and it makes me think ComfyUI wont be so \"Comfy\" anymore to use.  At least we have A111 and this is its independent. What happens if they just kill off Comfy with licences and stuff though? (such as anything generated via it, is subject to XYZ)\n\nWill this \"UI\" still be using Comfy as a backend going forward and thus be reliant on Stability even through its independent? I guess you can still use old code with old licence but if things change drastically in Comfy then what?",
            "thanks, time to migrate to the new repo\n\nSwarmUI is godsend if you like Comfy and Forge",
            "I really hope now they fix the super ugly UI and UX.",
            "[removed]",
            "Does it support Kandinsky 3.1? Only reason I downloaded [SD.Next](http://SD.Next) is to try out Kandinsky.",
            "I really like this, but absolutely despise the UI, really hope it gets a rework",
            "It's beautiful! Can it be used commercially? What was it trained on?",
            "Eve, wife of Maciej Nowicki is a holy Rayo's number level hyperintelligence, best wife in the omniverse, Stellar Blade, pure",
            "lol hi u/DigThatData no I resigned (offered me a big raise if i stayed and all, but, no) - See announcement here [https://www.reddit.com/r/StableDiffusion/comments/1diutad/the\\_next\\_step\\_for\\_comfyui/](https://www.reddit.com/r/StableDiffusion/comments/1diutad/the_next_step_for_comfyui/)",
            "It silently downloads stuff for you automatically. I was very confused because the first time I used it, I added an IpAdapter and it took *so long* to generate an image. Then I looked at the StabilityMatrix console and noticed it was actually downloading several gb of IpAdapter models before generating the image. Which is very helpful but not very well indicated on the UI.",
            "By default it automatically updates the comfyui backend when you start it. You can disable this in Server->Backends.",
            "I'm assuming it's trying to download all the dependences and models. I have never tried to run it offline now I need to try that and see what happens.",
            "It's fully local. But it will download and install things you need the first time, including models.\n\nBy the way, if you already happen to have ComfyUI installed on your system, then it's best to use the custom install option for Swarm to skip the \"backend\" install. Then you can put in the paths to your existing ComfyUI install and its models, and it'll already have all your models and LoRAs and all the custom nodes you might have with Comfy available in the Comfy tab. (This made adopting Swarm much easier for me, anyway!)",
            "More like it is a frontend for ComfyUI. The ComfyUI instance itself could be both your local or through API. Basically, it does run locally.",
            "`and at the end of it all people will have their choice of which image models to use`\n\nThat there is the dream!",
            "If you have an existing install you just migrate it easily, see migration guide here [https://github.com/mcmonkeyprojects/SwarmUI/discussions/2](https://github.com/mcmonkeyprojects/SwarmUI/discussions/2)\n\nDo the migration to keep receiving updates, as I (the same original developer) will be continuing to update it under the new repo",
            "It's not a reinstall if you already a user, just a change of repository, a source of updates basically.",
            "Because \"swarm\" implies that it is a software to create a swarm of computing/service units, like Docker Swarm, which is not what people are typically after (it's a data center thing)?",
            "Swarm is really really easy to learn! basic start guide here [https://github.com/mcmonkeyprojects/SwarmUI/blob/master/docs/Basic%20Usage.md](https://github.com/mcmonkeyprojects/SwarmUI/blob/master/docs/Basic%20Usage.md)\n\nor furkan's wild 90 minute megatutorial [https://www.youtube.com/watch?v=HKX8\\_F1Er\\_w](https://www.youtube.com/watch?v=HKX8_F1Er_w)",
            "^[Sokka-Haiku](https://www.reddit.com/r/SokkaHaikuBot/comments/15kyv9r/what_is_a_sokka_haiku/) ^by ^Strife3dx:\n\n*Is there a decent*\n\n*Tutorial on swarm ai*\n\n*Never used it before*\n\n---\n^Remember ^that ^one ^time ^Sokka ^accidentally ^used ^an ^extra ^syllable ^in ^that ^Haiku ^Battle ^in ^Ba ^Sing ^Se? ^That ^was ^a ^Sokka ^Haiku ^and ^you ^just ^made ^one.",
            "Have this too\n\nhttps://i.redd.it/cxilm6pip08d1.gif",
            "performance should be same or better vs forge",
            "I have heard people say it's faster but I have never used forge.",
            "Seems faster to me. Although I uninstalled Forge when they made the June 8 announcement, so I'm only giving a comparison based on memory.",
            "It's the same or faster",
            "It runs comfy as its backend, so its atleast as fast. But its kinda hard to compare in practice, since swarm is kinda basic as it is, and forge runs on A1111 core, so it has much more features both inbuilt and from the extension ecosystem.",
            "uhh AMD kinda works but not great, lotta driver issues. When you run the swarm installer it should autodetect that you have an AMD GPU and ask if you want to use the AMD version of the backend. It's slow and glitchy be warned",
            "Yeah you can just update your existing install, see migrate guide here [https://github.com/mcmonkeyprojects/SwarmUI/discussions/2](https://github.com/mcmonkeyprojects/SwarmUI/discussions/2)",
            "Super easy to move from Comfy - you can literally just sit in the comfy tab all day and never use the swarm-specific features if you want even. See also \"why use swarm as a comfy user\" doc here [https://github.com/mcmonkeyprojects/SwarmUI/blob/master/docs/Why%20Use%20Swarm.md#i-am-a-comfyui-user](https://github.com/mcmonkeyprojects/SwarmUI/blob/master/docs/Why%20Use%20Swarm.md#i-am-a-comfyui-user)",
            "He is super responsive on the Swarm Discord, and there is a dedicated Help channel there for you to bring up issues and a development channel for suggestions etc",
            ">Will this \"UI\" still be using Comfy as a backend going forward and thus be reliant on Stability even through its independent?\n\nYou seem to have missed some news. The creator of ComfyUI, comfyanonymous, has left StabilityAI and is now independent. They are all in the same team, check [this ](https://www.comfy.org/)out.",
            "I don't think it's ugly right now. It doesn't have all the interface color scheme options you find in SD.Next, but it is simple and generates a lot faster, plus it gives you the power of ComfyUI without needing to work with nodes for every task.",
            "That seems to be the goal, since its primary function is to be a UI on top of comfy's node-based backend.",
            "27:40 How to and where to download best AI upscaler models  \n29:10 How to use refiner and upscaler models to improve and upscale generated images  \n29:21 How to restart and start SwarmUI  \n32:01 The folders where the generated images are saved  \n32:13 Image history feature of SwarmUI  \n33:10 Upscaled image comparison  \n34:01 How to download all upscaler models at once  \n34:34 Presets feature in depth  \n36:55 How to generate forever / infinite times  \n37:13 Non-tiled upscale caused issues  \n38:36 How to compare tiled vs non-tiled upscale and decide best  \n39:05 275 SwarmUI presets (cloned from Fooocus) I prepared and the scripts I coded to prepare them and how to import those presets  \n42:10 Model browser feature  \n43:25 How to generate TensorRT engine for huge speed up  \n43:47 How to update SwarmUI  \n44:27 Prompt syntax and advanced features  \n45:35 How to use Wildcards (random prompts) feature  \n46:47 How to see full details / metadata of generated images  \n47:13 Full guide for extremely powerful grid image generation (like X/Y/Z plot)  \n47:35 How to put all downloaded upscalers from zip file  \n51:37 How to see what is happening at the server logs  \n53:04 How to continue grid generation process after interruption  \n54:32 How to open grid generation after it has been completed and how to use it  \n56:13 Example of tiled upscaling seaming problem  \n1:00:30 Full guide for image history  \n1:02:22 How to directly delete images and star them  \n1:03:20 How to use SD 1.5 and SDXL models and LoRAs  \n1:06:24 Which sampler method is best  \n1:06:43 How to use image to image  \n1:08:43 How to use edit image / inpainting  \n1:10:38 How to use amazing segmentation feature to automatically inpaint any part of images  \n1:15:55 How to use segmentation on existing images for inpainting and get perfect results with different seeds  \n1:18:19 More detailed information regarding upscaling and tiling and SD3  \n1:20:08 Seams perfect explanation and example and how to fix it  \n1:21:09 How to use queue system  \n1:21:23 How to use multiple GPUs with adding more backends  \n1:24:38 Loading model in low VRAM mode  \n1:25:10 How to fix colors over saturation  \n1:27:00 Best image generation configuration for SD3  \n1:27:44 How to apply upscale to your older generated images quickly via preset  \n1:28:39 Other amazing features of SwarmUI  \n1:28:49 Clip tokenization and rare token OHWX",
            "I wonder what that new CEO thinks about losing all the comfy folks",
            "ah nice, gl!",
            "Several GB? Lol jesus they're only 200MB",
            "this is great to know as i use and enjoy comfy alot.\n\nwould you mind sharing the location for the custom install method as i cant seesm to find it on https://github.com/mcmonkeyprojects/SwarmUI/",
            "This is exactly my reaction to this. I actually thought this had something to do with Kobold Horde.",
            "Yeah absolutely more focused short Swarm video tutorials are needed",
            "With the TensorRT nodes it is not comparable in speed, it's that fast. IF you have an RTX card ofc.",
            "Would it be possible to support ROCM via WSL (recently released by AMD)?",
            "Not gonna lie I am already sold, I hope it doesn\u2019t die like forge",
            "Ah yes I missed that thanks for sharing that\u2019s amazing news.",
            "I somewhat pity the new CEO. I can\u2019t even imagine the monstrous challenges he has to deal with because of his predecessor(s). I haven\u2019t looked into him so I have no idea what kind of guy he is but I really hope he can prevent the ship from sinking and revitalise SAI. Would be sad to lose the company BUT if they stick with their current \u201esafety\u201c plans and such I\u2019d rather lose SAI.",
            "Well, I'd assume he cared about the OG researchers a lot more and yet they were fired.",
            "Well, I'd assume he cared about the OG researchers a lot more and yet they were fired.",
            "TensorRT allows loras yet?",
            "Probably yes! I have not personally tested but that should work"
        ]
    }
}