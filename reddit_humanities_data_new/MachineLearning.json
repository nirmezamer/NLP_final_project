{
    "[D] Simple Questions Thread": {
        "title": "[D] Simple Questions Thread",
        "score": 13,
        "url": "https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/",
        "content": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!",
        "num_comments": 50,
        "comments": [
            "Hi everyone, I have a question about decoders. For LMs, the text generation stops when some special token, e.g., <EOS>, is generated. How does the generation stop for transformer decoders that don't generate discrete tokens via softmax? One of the approaches I know is to set a predefined length, but is there a more dynamic way of doing so? Thanks!",
            "wouldn't [TOVA](https://arxiv.org/pdf/2401.06104) give better performance in [Based](https://arxiv.org/pdf/2402.18668) than sliding window, especially for long contexts? if i understood correctly other efficient alternatives to attention struggle to recall details like names or prompt format and Based is supposed to fix this, and TOVA would help paying attention not to the recent tokens but to the most important",
            "is it possible for an llm to adjust its own weights on the fly based on my replies? afaik there are several RL techniques but can they adjust weights *on the fly* based *only on replies*, like trying to act more like when i praise it and less like when i scold it? do it work with rnn-like models like mamba, rwkv and based or it will probably ruin the current state?",
            "Hello,\n\nI am a college student and wrote a paper with some colleagues about ML, using different image recognition models to solve a specific problems. The results were not as good as expected (we think we know the reason why) but we think we have a nice work finished. We would like to get feedback from other people and have it posted somewhere so that we can reference it.\n\nDo you think it's a good idea for us to publish our paper in arXiv? What are other alternatives?\n\nWe also don't really have in mind publishing it to any journal because we doubt our work is worth it to be in any journal. What are your opinions on that?\n\nThanks!",
            "I'm working on my first computer vision project, which involves annotating charts for their underlying data-table. I'd like to fine-tune an existing model I've found, but all resources for doing so primarily share code, without logic or details about dataset generation, required dataset size, best practices for dealing with common failure cases, learning rate (this is covered a bit though), epochs, etc. What are good resources for learning about all of these very specific decisions, or any other good in depth nitty-gritty resources for similar topics in deep learning in general?",
            "How come they use a negative sign in front of the cost function's derivative at the end but a positive sign in the beginning? It's from CS229 at Stanford.\n\n[https://ibb.co/PN7X1fw](https://ibb.co/PN7X1fw)",
            "I'm looking for deep learning, or machine learning more generally, or artificial intelligence more generally, courses or lectures or books, that have a lot of theoretical and practical mathematics but also practical coding! Text form works, I prefer video form, but ideally if it has both text and video form!\n\nI love Stanford CS229: Machine Learning and other Stanford courses but that has basically just the theory mathematics part. https://www.youtube.com/playlist?list=PLoROMvodv4rNyWOpJg_Yh4NSqI4Z4vOYy\n\nI love Karpathy's neural networks zero to hero but that's mostly coding and not much mathematics and it's mostly deep learning and not rest of machine learning. https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ\n\nAndrew Ng\u2019s Machine Learning courses seem to have a lot of code but not really much theory and mathematics. \n\nDive into deep learning seems to cover a lot with mathematics and code but I wish it was in video form too! https://www.d2l.ai/chapter_preface/index.html\n\nAnd lots of these don't cover neurosymbolic methods or other methods in AI, which I dont really need in one place with all of the others, but it would be great bonus!",
            "Hello all,\n\nHas anyone had an issue with a CNN model learning from the background of the images in the dataset and how to combat that? My entire dataset has very distinctive white rollers in the background and when I visualise the decision making using LIME it tells me the model was almost entirely relying on the rollers in the background. I then preprocessed the the image to make the entire background a black mask with an RGB value of (0, 0, 0), yet the model still uses the background to make decisions, according to LIME! I don't get how a CNN is pulling features out of an entirely black featureless background, and also don't get why the model is almost 100% accurate in its predictions too.\n\nSo, has anyone experienced similar/ know a way forward with such a dataset? Can anyone shed light on how the model is so accurate when LIME says its almost entirely using the black featureless background?\n\nPulling my hair out, so any help or guidance is appreciated! :)",
            "I am a beginner in NLP/ML, but I would like to understand how I could make it possible.\n\nSo basically, there is an existing NLP on Huggingface that does text generation in my language very well [https://github.com/MinSiThu/MyanmarGPT](https://github.com/MinSiThu/MyanmarGPT)  \nBut when asked in other languages like English, it fails to give weird answers unfortunately.\n\nHow can I go about training a model specialized for translation between English-Burmese and Burmese-English based on the existing models?\n\nI can set up and use the GPUs in my university for that.",
            "Hi, so I'm working on a project in which I want to calculate the cosine similarity between a query vector and corresponding document vectors ( around a billion of them ) and then threshold them to get the most relevant documents.  The number of relevant documents isn't bounded so kNN isn't very relevant other than for initial pruning. Here, the speed is of the essence so the scale is a problem. I initially looked into FAISS but is there any other thing that I can look at that would be faster than FAISS? Also, should I instead turn to some other programming language altogether to get the additional boost in performance? Note that finally I'm supposed to deploy it on gcp.",
            "**When authors use the term \"ill conditioned\" for machine learning problems what do they mean?**\n\nI have read some papers about optimization techniques for machine learning and sometimes people just use the term \"ill conditioned\" but don't say what they mean by it. I know conditioning for matrices but those authors talk about \"ill conditioned objectives\" or optimization techniques that \"deal with ill conditioning\". What do they mean by that?",
            "Is anyone here trying to get Austrian Visa for ICML? I'm applying from NYC and can try for group door-step appointment if there are few. Otherwise its inordinately expensive.",
            "Hi all, I was working on a project where i have a lot of application logs and i need to find the anomalies in it using machine learning or deep learning techniques. It is a massive un-labelled log line dataset which is logged when a user performs some action on application or when some background process is run.  \nPlease help me in solving this problem.  \nany resource, code or technique will be really helpful.  \nThanks in advance \ud83d\ude4f",
            "Hello everyone, recently started working with LSTMs for multivariate time series forecasting. The main idea is to train an  LSTM on a large training set of 10 000 time series (of length let's say 700) of houses temperature taking into account outside factors (outside temp, sun irradiation, humidity etc.) as well as house parameters (surface area, type of heating, windows area etc.) as inputs.  \nMy goal is to run the inference for new houses to predict their temperature evolution for the same length of time.  \nAlthough the LSTM is performing fairly well for the majority of the validation/test set, one issue is apparent for all the houses: the prediction for the first couple of timestamps is bad (which is expected since the LSTM doesn't have enough context yet).   \nI was wondering if there is anyway to initialize the LSTM with the first temperature of the output so that it can use it to start it's prediction. I read a bit about the initial hidden state and cell state but couldn't find anything about initializing the LSTM with the first value of the output timeseries for it to start predicting from there.   \n  \nAny help/insight is appreciated.     \nThank you!",
            "other things being equal, what will have better performance: a model like based (an rnn-like model + small sliding window attention), or like mamba2-hybrid (an rnn-like model + full context attention) + TOVA with the same size as based-like's window?",
            "Looking for an advance Machine Learning book.\n\nHi everyone, recently i had finish Bishop Deep Learning book and i found it interested. I want to read more about advance topic in machine learning (especially multimodal field). Can anyone suggest me some books to read ? (like same type Bishop book, i really love it).",
            "Simple/Not Simple - ML solution space - classification, clustering, and regression(???)\n\nSaw a nice MECE ML solution space description and I'm spacing the third. Not different types of training but types of problems that ML solves.\n\nThanks!",
            "Hello Everyone\n\nCan someone tell me if I can use EncoderDecoder Model such as T5 for pretraining using Causal Language Model? I want to pretrain it for Next word prediction",
            "Hello! I want to build a model using machine learning to predict student dropout and I saw that the data points in the dataset should be IID. But I have a dataset wherein the students came from the same household and some of my predictors are age, employment status, if they have student loan, bank account, region they live in and if they have any illness. Now I am not sure if I should consider students from the same household or only pick one student from one household? Does belonging in the same household affect the IID of my data point? What to do?",
            "I am currently developing an app that focuses on the Autism education market, and I believe incorporating AI, particularly NLP and personalized learning models, could significantly enhance its effectiveness. Here's a brief overview of my project and what I'm looking to achieve:\n\n**Project Overview:**\n\n**App Purpose:** Assists Parents and Carers of children with Autism, specifically in the Non-verbal area. By allowing them to help communicate changes, choices, and plans etc.\n\n**Current Features:** The app gathers feedback on children's response to these suggestions and choices etc, through facial emotional recognition, and apparent attention to the images. Plus the parents leave a simple bit of feedback in the way they choose the next action. The App already uses AI to summarize feedback and find trends and make possible suggestions\n\n**Goal:** Implement AI that specializes in Autism to help customize this summary and suggestions for each child.\n\n**AI Implementation Goals:**\n\n**Specialized AI for Autism?:** would building/training our own (even with commercial tools) be more superior and less general than ChatGPT ?.\n\n**Personalized Learning Models**: while I sort of understand that personal LLM's for each user is a big undertaking, I only say that as I don't fully know what's involved. But understand that keeping a \"Vector\" database to help prime an AI response/answer to something, may be a better option, and keep me using the one LLM ? ... a lot of these questions come with a curiosity about security as well, which of any of these, or better options, would be the most secure (if that's even a possibility)\n\n**Future Integration**: Integrate other apps and tools to contribute to the feedback loop\n\n**Seeking Advice:** I am looking to discuss with data scientists or ML/AI experts on:\n\nBest practices for developing AI models specific to Autism education.\n\nHow to design and implement personalized learning LLMs or some similar system, to keep a personalized track, or ask questions of progress and changes in preference etc over time..\n\nEffective ways to integrate AI with the app\u2019s feedback system. Potential challenges and solutions for continuous learning and adaptation of the AI.\n\nIf anyone has experience in this area or could provide insights or resources, I would greatly appreciate it. Your expertise could help make a significant positive impact on the education of children with Autism.\n\nThank you in advance for your help!",
            "How can I make an ai voice model trained on a YouTube channel that posted ASMR videos?\u00a0\n\n\nI want to make an ai voice model trained on an inactive ASMR youtuber so I can make new ASMR videos and song covers with their voice. What programs and steps would I need to take to go about doing this? Would I have to download all of their videos and put them through a program that isolates their vocals like Lalal.ai? What program would help me do that and once I have the vocals how would I use those to make an ai model? Any advice or links would be appreciated.\u00a0",
            "Hey everyone! Im trying to fill the rest of my electives with worthwhile stats courses that will aid me better in Data Science or Machine Learning (once I get my masters in Comp Sci). \n\nWhat would you consider the essential statistics courses for a career in data science? Specifically data engineering/analysis, data scientist roles and machine learning. \n\nThanks!",
            "I have a good knowledge of ML and know the basics, such as the difference between supervised and unsupervised learning. In supervised learning, I know the implementation of models such as logistic regression, linear regression, lasso, SVM, k-nearest neighbors, and decision tree. I am also familiar with linear algebra, including vector addition, vector subtraction, vector multiplication, and other vector operations like dot product, cross product, and projection.\n\nRegarding statistics for ML, I understand categorical and numerical data and other related topics. I also have knowledge of probability.\n\nNow, can someone tell me how to start with deep learning?\nIf possible please attach the resources too\nThanks a lot",
            "I would like to understand some of the challenges ML engineers face with training and deploying models in the cloud. Specifically do these pain points resonate with you. I am looking to create a startup to address some of these, so would really appreciate your inputs on whether these are relevant and important to you. Thanks\n\n1. **High Costs of AI Compute**:\n   * **Pain Point**: Traditional cloud computing for AI workloads is expensive, especially for small to medium-sized enterprises (SMEs) with limited budgets.\n2. **Complexity of Infrastructure Selection**:\n   * **Pain Point**: Selecting the right AI infrastructure is complex and time-consuming, requiring specialized knowledge and expertise that many businesses lack.\n3. **Lack of Transparency in Pricing**:\n   * **Pain Point**: Cloud providers often have complex and opaque pricing structures, making it difficult to understand and compare costs.\n4. **Limited Negotiation Power**:\n   * **Pain Point**: Smaller businesses lack the negotiation power to secure discounts and favorable terms from cloud providers.\n5. **Challenges in Monitoring and Reporting**:\n   * **Pain Point**: Monitoring and reporting AI compute usage, costs, and performance metrics can be challenging and resource-intensive.",
            "For autoregressive generation, regardless of the type of model, you always need to choose a discrete stopping condition. The choice you make is almost arbitrary and is really determined by the nature of the model and the training data.\n\nFor LLMs a special token is a simple and easy solution, because there's no other obvious stopping condition in the data or in the model itself.\n\nFor vision transformers, which generate continuously-valued tokens, you don't need a stopping condition because the number of tokens that need to be generated is determined by the resolution of the image, and really you don't even need autoregressive generation at all.\n\nThere are other kinds of models that do offer natural stopping conditions, though. \"Deep equilibrium models\" (DEQs) are (explicitly) autoregressive models that deliberately implement dynamical systems that reach a fixed point when run for long enough. So there's a natural stopping condition here: you can stop generating new samples in the sequence when the difference between one sample and the next is small enough. DEQs generally avoid this though by using a trick that involves solving for the fixed point explicitly, rather than generating samples autoregressively.\n\nYou could imagine other variations on that theme, e.g. you could create a model that implements dynamical systems that naturally converge to a periodic attractor, which is easy to detect, or a maybe a chaotic attractor, or some other kind of state that has a clear detection criterium.\n\nIn my opinion this all is an indication that we are implementing LLMs incorrectly, or that they are not capable of doing the things that most people want them to do. I think the \"correct\" version of these models would presumably have a natural stopping condition, rather than requiring an artificial cludge like adding <EOS> tokens into the data.",
            "This sounds like Reinforcement learning from human feedback. You will probably need to add some sentiment model to convert your replies to a numerical score. I am not sure what \"on the fly\" means, but you can update LLM weights, and then prompt it with the chat history generated with the old weights and continue the same conversation.",
            "It seems that they distributed the negative sign in the last line. So the output goes from - a \\* (h(x) - y) \\* x\n\nto a \\* (y - h(x)) \\* x. They are equivalent, there is no deeper meaning to this besides preference.",
            "You might be misinterpreting what you're looking at. I'm guessing you're trying to classify a single object against a background (either white rollers or black mask)?\n\nWhat might be happening is that your model is using the shape of the object's silhouette to do the classification. You might be expecting LIME to highlight the object in this case, but it would be equally correct for it to highlight the background, because the hole in the background left by the object is the same shape as the object itself.\n\n\"Model interpretability\" is generally a false idol; there's no algorithm that you can use that is going to consistently and correctly \"explain\" to you how a model is working. If that were possible then you wouldn't need a neural network at all. Every supposed method of model interpretation requires its own interpretation in turn.\n\nThe ultimate test of model correctness is your test/train split. If you're sure you did that correctly then you should believe the results, no matter what any interpretability tool says. Conversely, if you're not sure you did that correctly, then you absolutely should not trust the model, no matter what any interpretability tool says.",
            "This is an interesting problem that I have actually done research on in the past. It is called algorithmic bias in machine learning models. I read over the other comment thread which seems to conclude that LIME could be causing the issues. While this might be the case, it is really common for CNNs to use shortcuts, like white rollers, to make classifications. Your model might have great performance on your data, and yet if you use it out in the wild it could completely collapse. This is because it learned that the key rules to classifying data are based on something that is training/testing data specific. Additionally, while your test data might not be contaminated, the entire dataset could be biased by a lack of variety in backgrounds. This is a very difficult problem to solve, but the best way of counteracting it is including more variability in your data (more backgrounds, etc.) or training via transfer learning (gives the model better robustness to outliers).",
            "It seems that the link you've provided is an example of somebody taking GPT and fine-tuning it on Burmese. It is designed specifically to perform well in Burmese, which makes sense why it would exhibit odd behavior for English related tasks. \n\nIf you are interested in translation, I would try training a machine translation model. They are different from the architecture of GPT. GPT is what's called a decoder only architecture, meaning it's only job is to predict the next token based on prior context. However, in machine translation they use an encoder/decoder architecture. The addition of the encoder before the decoder allows for your inputs (English) to be cast into a Burmese/English language embedding, and then decoded into Burmese.",
            "I think in this context they mean that the solution to the objective might be practically difficult to find, or might have many solutions. Problems of these types usually require regularization terms to make them easier to solve. Take for example a quadratic optimization landscape that is locally linear at the global minimum. Even if you can solve this optimization problem, the problem is \"ill conditioned\" because the solution could lie anywhere on the line that defines the minimum. In particular if you are solving the problem using gradient descent, you could run into numerical instabilities that make the algorithm run forever. In this scenario, dealing with ill conditioning might be adding an L2 regularization term that increases the convexity of the solution, hence making the landscape better conditioned for optimization.",
            "https://en.wikipedia.org/wiki/Condition_number",
            "Not machine learning, but this is a deep learning book by Bishop: [https://www.bishopbook.com](https://www.bishopbook.com)  \nI haven't read it personally, but looking over the topics it looks like it covers quite a bit.\n\nEdit: after taking a quick look, I think you would be interested in section 12.4 - multimodal transformers.",
            "I think this textbook is a good start: [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/) (written by the inventors of DL). Since you have a good background in ML, I don't think you'll have too large of a leap to make.",
            "well, humans don't stop thinking after replying, so a model could generate tokens without stopping, only \"sending\" what it thinks should be sent\n\nthis seems kinda impossible for transformers because each next token either makes the model slower or makes the model totally forget what was before, but for linear models like mamba or rwkv this could make sense",
            "Thank you so much for the reply! I really appreciate the details.",
            "I hadn't heard of DEQs before, reminds me a little bit of score based modeling with SDEs in a way. Do you know if there is still research going on with DEQs, and if so do you know who is working on it?",
            "will updating LLM's weights make the current state of an rnn-like model useless? i mean i want the model to adjust its own weights during the conversation without the need for the model to \"read\" the conversation history again",
            "Hey, thanks for your reply.\n\nYou make some very good points. I have to use interpretation/ explainable methods as the point of my project is to understand what those tools can tell us.\n\nThe task at hand for the classifier is binary and to determind whether an apple is 'defective' or 'not defective' based on bruising, scarring, black spots on the skin etc.\n\nI think it must be LIME messing up because like you say, what's important is it IS correct with a high accuracy, and i've painstakingly ruled out contamination between the training, val test sets.\n\nI've just now managed to implement SHAP which is another explainer tool and it does seem to be highlighting defective areas so I think it has to be a LIME issue, yet i've followed all the documentations and tried ti on different archiectures, so idk.",
            "Agreed. But while it may not be robust to real world data, it still shouldn't be able to use entirely black (0, 0, 0)RGB background as 'important features', right? Especially when that preprocessing has been applied to the entire datatset. The entire datatset has a black background. I'm highly suspect of LIME and wonder if anyone else had had LIME go rogue labelling random background/ areas.",
            "Thank you! Is there any way I can assist you?",
            "Thank you for your reply. I just finish that book :(.",
            "Thanks a lot man!   \nI am confused af, where and how to start but ig this will help",
            "Autoregressive token generation with a perpetually-expanding context is just one way of doing things, and it's probably not the best or most correct one. This is what I mean by current LLMs not being the right answer.\n\nConsider that a computer's CPU runs forever with no problems, because it doesn't perform computation on the entirety of the RAM with every clock cycle. The same can be done with autoregressive transformers, or with any other autoregressive model.",
            "Yeah DEQs and score based models are very closely related in the sense that both are examples of neural differential equations, they just have different properties - DEQs always evolve in time towards a fixed point (by construction), whereas SDEs can do pretty much whatever.\n\nI'm not really up to date on DEQs specifically, but in general you'll probably be interested to read about \"implicit layer neural networks\", of which DEQs are one example. There's a good introduction to them here: https://implicit-layers-tutorial.org/",
            "I don't think few update steps would ruin state for the model with updated weights, but updated model still needs to read the whole convo anyway to do a gradient backpropagation through time for the next update.",
            "It could be that your model is learning spurious correlations from the black background. For example, if the problem is really easy, it could still use dependencies on seemingly random features. I don't have much experience with LIME - I used GradCAM and ScoreCAM which I found to be very helpful.",
            "I would start with chapter 3. It will expose you to many of the methods people use to tackle DL problems. Each section could have its own textbook, so move at your own pace and investigate what you find interesting.",
            "I actually found another textbook today on Deep Learning that was published this year so it is very up to date: [https://www.bishopbook.com](https://www.bishopbook.com)  \nHave only read bits and pieces so far but looks like a great resource."
        ]
    },
    "[D] Academic ML Labs: How many GPUS ?": {
        "title": "[D] Academic ML Labs: How many GPUS ?",
        "score": 33,
        "url": "https://www.reddit.com/r/MachineLearning/comments/1dlsogx/d_academic_ml_labs_how_many_gpus/",
        "content": "Following a [recent post](https://www.reddit.com/r/singularity/comments/1coq6tn/ai_godmother_standfords_nlp_lab_has_only_64_gpus/), I was wondering how other labs are doing in this regard.\n\nDuring my PhD (top-5 program),  compute was a major bottleneck  (it could be significantly shorter if we had more high-capacity GPUs). We currently have \\*no\\* H100.\n\nHow many GPUs does your lab have? Are you getting extra compute credits from Amazon/ NVIDIA through hardware grants?\n\n  \nthanks\n\n",
        "num_comments": 49,
        "comments": [
            "atm, princeton PLI and harvard kempner have the largest clusters, 300 and 400 H100s respectively. stanford nlp has 64 a100s; not sure about other groups at stanford.",
            "Not a ML lab but my research is in CV. Back in 2019 when I started I had access to one 2080 Ti.\n\nAt some point in 2020 bought a laptop with an RTX 2070.\n\nLater, in 2021 got access to a server with a V100 and an RTX 8000. \n\nIn 2022 got access to a 3090.\n\nIn 2023, got access to a group of servers from another lab that had 12x 2080Tis, 5x 3090s, and 8x A100s. \n\nThat same year I got a compute grant to use an A100 for 3 months. \n\nRecently school bought a server with 8x H100s that they let us try for a month. \n\nAsides from that, throughout 2021-2023, we had access to rent GPUs per hour from a local academic provider.\n\nMost of these are shared, except the original 2080 and 3090.",
            "None. No credits either. I managed to get my internship company to help me with some cloud credits since the university wasn't helping.",
            "I'd wager to say the vast majority of ML do not have access to a single H100 xD",
            "Sounds like your lab should embrace Edge AI ^(please ^we ^^need ^^^help)",
            "EU lab here, we have roughly 16 lab-exclusive A100s and access to quite a few more GPUs via a few different additional clusters. For those scale is hard to guess, since they have many users, but it's roughly 120k GPU hours/cluster/year. Anything beyond 80G GPU mem is a bottleneck, though, I think we have access to around 5 H100s in total.",
            "No H100, but 16 A100s and around 84 other GPUs (RTX 3090, TITAN, Quadro RTX, ...). I consider myself lucky because in Europe some universities / research labs offer almost no compute.",
            "Graduated at the end of 2022. I think I had access to close to 30 gpu servers (just for my lab). Each server had 4 GPU cards of varying quality as they were acquired over the years. Unfortunately, I don't remember what the best cards were that we had towards the end. It was still a struggle at times competing with other PhD students in the lab at times, but overall it was a privilege to have so much compute handy.\u00a0",
            "My university offers a cluster with 52 GPU nodes, each having 4 H100 GPUs. The resources are of course shared across all departments and some other institutions can access it too. Nevertheless, even students are granted some hours on the cluster each month. If you need more computing time you need to apply for a dedicated compute project of different scales.\n\nI really like the system and access to it has been a game changer for me.",
            "Students in EU don't even imagine having access to enterprise computational power other than free TPU credits from Google and similar offerings. Except for maybe ETH Zurich, since that university is funded by billionaires from the WWII era",
            "Phd student at Mila here (UdeM, Montreal), we have about 500 GPUs in-house, mostly A100 40Gb and 80Gb",
            "Studies and analysis think tank: for classified applications we have a dual-A100 machine, but for all our unclass work we have an analytic compute service that launches AWS instances. \n\nAll paid for by either USG sponsors or research grants.",
            "Coming from a not-terribly-prestigious lab/school our limit was about 4 80GB A100s. You could get 8 in a pinch but the people in charge would grumble about it. To clarify, more GPUs were available but not necessarily networked in such a way as to make distributed training across all of them practical. i.e. some of them were spread out across several states.",
            "lol we hired gpu on vast ai",
            "I work for a major cloud computing provider developing and fixing software for H100s all day, so this thread is very interesting to read. I didn\u2019t know H100s were that rare.",
            "yes, I heard about that. but again: how many people are they using these gpus? is it only for phds? when did they buy it? interesting to see the details of these deals",
            "also are the A100 40 or 80 GB?",
            "In 2022 got access to a 3090: do you mean a \\*single\\*???",
            "that'sa vicious cycle. especially if your advisor doesn't have connections with the industry, you need to prove yourself to establish yourself. But to do so, you need sufficient compute... how many credits did they offer? was it only for the duration of your internship?",
            "we don't (top 5 in the US).",
            "sorry didn't get the joke :(",
            "What y\u2019all need help with?",
            "also how many does yours have? No H100 is not normal? we have 56 of 48GB",
            "we don't have 80 G GPUs :( are you in the UK?",
            "are you in the UK?",
            "exactly. limited resource adds another layer of competition among the students. you clusters seems similar to ours",
            "are you in the US != Princeton/ Harvard? That's a lot of compute.",
            "i did my undergrad in europe. before landing US, I didn't know what ML is .....",
            "How much does ETH Z\u00fcrich have?",
            "thanks! what's the ratio of 40GB and 80GB? how easy is it to reserve and keep an 8 GPU server with 80 GB for some months?",
            "what do you mean by classified applications? A100 have 40 or 80 GB memory?",
            "you mean limit per student?",
            "is there a special offer for universities?",
            "Yes. It's rough out there.",
            "It's how research is in the third-world. They got around 3.5k, but the catch was that, they would keep about 2.5k and give me 1k (that's enough for me). They used my proposal to get the credits from Amazon through some free credits program.",
            "It's not quite a joke, I'm in the industry, and while huge models are exciting, in any field that I care about, which is life critical, real time inference on device, these models are, not useless, but not a solution either.\n\nThink tiny models, and whatever you thought, it's probably 10-100 too large, now think about a weak hardware, you probably didn't think about a hardware weak enough, think modern Iphone, lol kidding, 5 times weaker, at least!\n\nLuckily, HW is getting cheaper, and more specialized architectures are getting a focus for accelerating the relevant ops, but it's still very far, and the research for what works in huge models doesn't necessary transfer to the edge, for example in vision, CNN are still the golden standard, not only because they are faster than transformers, but because Transformers don't scale down, ending up both slower and worst in accuracy.\n\nWe need scientists to get back on track so I can use your fruit of labor to make money.\n\nFor your question, we have 5x 3080TI in house, training and deploying ~15 models a year, the models are deployed over thousands of thousands of toasters saving lives once in a while.",
            "Nope, RWTH Aachen University in Germany",
            "I studied at ETH. Labs have access to the Euler cluster which is a shared cluster for all of ETH. I'm not sure how the allocation is handled. You can read more about the cluster here: [https://scicomp.ethz.ch/wiki/Euler](https://scicomp.ethz.ch/wiki/Euler)\n\nEuler contains dozens of GPU nodes equipped with different types of GPUs:\n\n* 9 nodes with 8 x\u00a0[Nvidia GTX 1080](https://www.nvidia.com/en-us/geforce/news/geforce-gtx-1080)\u00a0(formerly in\u00a0[Leonhard Open](https://scicomp.ethz.ch/wiki/Leonhard))\u00a0*decommissioned in 2023*\n* 47 nodes with 8 x\u00a0[Nvidia GTX 1080 Ti](https://www.nvidia.com/en-us/geforce/news/nvidia-geforce-gtx-1080-ti)\u00a0(formerly in\u00a0[Leonhard Open](https://scicomp.ethz.ch/wiki/Leonhard))\u00a0*decommissioned in 2023-2024*\n* 4 nodes with 8 x\u00a0[Nvidia Tesla V100](https://docs.nvidia.com/dgx/dgx1-user-guide/introduction-to-dgx1.html)\u00a0(including some formerly in\u00a0[Leonhard Open](https://scicomp.ethz.ch/wiki/Leonhard))\n* 93 nodes with 8 x\u00a0[Nvidia RTX 2080 Ti](https://www.nvidia.com/en-me/geforce/graphics-cards/rtx-2080-ti)\u00a0(including some formerly in\u00a0[Leonhard Open](https://scicomp.ethz.ch/wiki/Leonhard))\n* 16 nodes with 8 x\u00a0[Nvidia Titan RTX](https://www.nvidia.com/en-us/deep-learning-ai/products/titan-rtx)\n* 20 nodes with 8 x\u00a0[Nvidia Quadro RTX 6000](https://www.nvidia.com/en-us/design-visualization/rtx-a6000)\n* 33 nodes with 8 x\u00a0[Nvidia RTX 3090](https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3090-3090ti)\n* 3 nodes with 8 x\u00a0[Nvidia Tesla A100](https://www.nvidia.com/en-us/data-center/a100)\u00a0(40 GB PCIe)\n* 3 nodes with 10 x\u00a0[Nvidia Tesla A100](https://www.nvidia.com/en-us/data-center/a100)\u00a0(80 GB PCIe)\n* 2 nodes with 8 x\u00a0[Nvidia Tesla A100](https://www.nvidia.com/en-us/data-center/a100)\u00a0(80 GB PCIe)\n* 40 nodes with 8 x\u00a0[Nvidia RTX 4090](https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4090)",
            "no, but if the project got published at a international conference/journal, we got a amount of money. So yeh my school support a little bit",
            "wow. could you make any progress? that's suffocating. is your lab US or Europe?",
            "They got around 3.5k: what do you mean they, your advisor?\n\n3.5k: is this compute credits? how much time does this give you?",
            "wow thans for the detailed reply. is it for the full university though? how easy is it to reserve 1 node with eight 80 GB gpus?",
            "does the amount compensate for the full hardware used or only a portion?",
            "I'd say I've made the biggest leaps when compute is not an issue. For example having access to the H100 server currently has allowed me to generate more data in two weeks than I could have gathered in half a year before. Hopefully enough for two papers or more. But it's indeed very restricting. The experiments you can run are very limited.\n\nFor reference, this is in Asia (Taiwan specifically).",
            "The company. $3.5k in AWS cloud credits",
            "No not that easy. You need to be part of a lab with access. I'm not sure how the access by the labs is handled. I was part of one for a project where we had access to quite a few of the smaller GPUs. You schedule a job with what I remember being Slurm (a resource manager for shared clusters; it basically decides which jobs get to run in which order and priority). I think it's rather rare having access to those larger GPU groups. Probably it's also only a few labs which really have projects that require those. My impression was that ETHZ doesn't have thaaaat many labs working on large scale ML models or LLMs in general. Yes, there are two NLP groups, but they're not that obsessed with LLMs as e.g. Stanford NLP.",
            "got it thanks. my PhD lasted 7 years due to that ( before 2022 I had access to only 16 GB  gpus). Great that you gathered experiments for two years :)",
            "I see. Thought you were getting credits directly from the company you were interning (nvidia/ google/ amazon). again $1K isn't it scarce? for an 8-GPU H100 how much hours of compute is it?",
            "Yeah, I guess it wouldn't be much for good quality research. But this is for my Masters, so it doesn't have to be that good. If you use 8 GPU H100, you probably run out of it within a day. I am using an A10G instance. So it doesn't consume much. It costs like 1.3$/hr."
        ]
    },
    "[D] Memory mechanism for Transformers": {
        "title": "[D] Memory mechanism for Transformers",
        "score": 13,
        "url": "https://www.reddit.com/r/MachineLearning/comments/1dlb0wj/d_memory_mechanism_for_transformers/",
        "content": "Hey folks! I am wondering what interesting work has been done to add a short term memory mechanism to transformers? Does someone know what the important work in this area is?",
        "num_comments": 11,
        "comments": [
            "There's like a hundred papers on memory-augmented transformers but none of them are seeing any practical use. \n\nEverybody's using regular old attention or sometimes one of the long-context variants.",
            "Check out Facts as Experts (https://arxiv.org/abs/2007.00849), which augments the transformer with a key-value lookup where the key are the contextual entity mention embeddings. It's bit of a pain to setup and train but may be interesting to you.",
            "Memory Augmented Transformers could be a great resource for exploring this topic.",
            "can't remember what it's called, but saw a cool one that basically added an RNN state for a running memory",
            "!remindme 2 days",
            "Important? Almost nobody on(at least publicly available) does anything beside KV cache.\n\nTheoretically  Memorizing Transformers, RMT.\n\nPractically there was landmark attention(eg https://huggingface.co/eugenepentland/WizardLM-7B-Landmark) but it never gained traction\n\nThere also was some papers about kv cache compression, but in practice most important stuff which is actually done in practice is kv cache quantization which means bigger context which means bigger memory",
            "I bet someone combined transformers with neural turing machines",
            "I should also add that I am interested in memory for transformers for the purpose of reasoning, in particular not interested in methods that try to simply extend context size.",
            "Out those hundreds of papers, what is a sampling that gives good coverage of the different approaches?",
            "They are interconnected and most of such works view themselves as \"larger context\" even though its no longer just bigger number of Qs, Ks, Vs.\n\nYou can also check benchmarks like babilong as its designed for long context and reasoning. Its as if simple reasoning and haystack search had a baby.\n\nThough authors are RMT guys, they only test activation beacon from non standard attention(also finetuned mamba was the best model) and nobody but them tried it. They also refer to longbench paper.\n\nActivation beacon paper does cite other works. \n\nArxiv also has google scholar link in the bottom so you can do research and find other papers who cite papers.",
            "I don't think Tramsformers cannot reason because of a lack of short-term memory.\u00a0\nSelf-attention kinda is the short-term memory. If the Key-Values are persistent you basically have a memory store / database (which is basically what an MLP can be).\nProbably neither self-attention nor linear projection with nonlinear elementwise activation can express or learn by gradient descent the functions to reason consistently"
        ]
    },
    "[P] AgileRL - evolutionary RLOps for state-of-the-art deep reinforcement learning": {
        "title": "[P] AgileRL - evolutionary RLOps for state-of-the-art deep reinforcement learning",
        "score": 13,
        "url": "https://www.reddit.com/r/MachineLearning/comments/1dla20p/p_agilerl_evolutionary_rlops_for_stateoftheart/",
        "content": "Hi, I've posted before about our evolutionary hyperparameter optimization for reinforcement learning achieving SOTA results, but I'd like to share that our open-source framework has now had its v1.0.0 release!  \nPlease check it out! [https://github.com/AgileRL/AgileRL](https://github.com/AgileRL/AgileRL)\n\nThis library is initially focused on reducing the time taken for training models and hyperparameter optimization by pioneering\u00a0evolutionary HPO techniques\u00a0for reinforcement learning. Evolutionary HPO has been shown to drastically reduce overall training times by automatically converging on optimal hyperparameters, without requiring numerous training runs.\n\nWe are constantly adding more algorithms and features. AgileRL already includes state-of-the-art evolvable\u00a0on-policy,\u00a0off-policy,\u00a0offline,\u00a0multi-agent\u00a0and\u00a0contextual multi-armed bandit\u00a0reinforcement learning algorithms with\u00a0distributed training.\n\nI'd love to get your feedback!",
        "num_comments": 1,
        "comments": [
            "Really cool work! What kind of architecture mutations are currently supported? In the [documentation](https://docs.agilerl.com/en/latest/off_policy/index.html#mutation), I see a generic keyword for architecture mutations next to new layer and activation layer mutations. Do you see any scope to include hierarchical architecture mutations like the ones proposed in the deepmind paper [https://arxiv.org/abs/1711.00436](Hierarchical Representations for Efficient Architecture Search)?"
        ]
    },
    "[Project] Training loss is not decreasing  ": {
        "title": "[Project] Training loss is not decreasing  ",
        "score": 0,
        "url": "https://www.reddit.com/r/MachineLearning/comments/1dlpqsd/project_training_loss_is_not_decreasing/",
        "content": "I am using this git repo: [https://github.com/ZhaoJ9014/face.evoLVe](https://github.com/ZhaoJ9014/face.evoLVe) to train Arcface model. **Datasets: MS-Celeb-1M and my own private dataset. Backbone: IR\\_50. Loss: Focal loss. Steps per epoch: 686. Learning rate: 0.001.** But the training loss is not decreasing after 125 epochs. I have used gradient clipping to see if it solves the problem. Experimented with batch size. Have tried LR 0.1, 0.001, 0.00001, 0.3. But no solution. What alternative repo can I use to train Arcface?",
        "num_comments": 9,
        "comments": [
            "You can try to do a lr sweep to see which lr is best/working",
            "How many steps per epoch? What's your learning rate?\u00a0",
            "So far I have tried 0.1, 0.001, 0.00001, 0.3. None of them decreasing the loss.",
            "steps:686(with private dataset), learning rate: 0.001",
            "Try lowering the learning rate to .000001 that seems way too high\u00a0",
            "Okay. I will see what happens.",
            "Even after 6 epochs the loss seems to be constant in range.",
            "I think you actually want to do the opposite. According to the docs and the default configuration the LR automatically decays every 30 epochs by 10e-4. If your LR was too big this would mostly be self-correcting and you\u2019d see it start working in your way to 125. Try running it full default first - start the LR at 0.1",
            "I have run it with the default LR value. The problem remains."
        ]
    }
}